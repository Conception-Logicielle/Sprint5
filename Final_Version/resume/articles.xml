<articles>
    <article>
        <preamble>ACL2004-HEADLINE.txt</preamble>
        <titre>Hybrid Headlines: Combining Topics and Sentence Compression</titre>
        <auteur>David Zajic, Bonnie Dorr, Stacy President</auteur>
        <abstract>This paper presents Topiary, a headlinegeneration system that creates very short, informative
            summaries for news stories by combining sentence compression and unsupervised topic discovery. We will show
            that the combination of linguistically motivated sentence compression with statistically selected topic
            terms performs better than either alone, according to some automatic summary evaluation measures. In
            addition we describe experimental results establishing an appropriate extrinsic task on which to measure the
            effect of summarization on human performance. We demonstrate the usefulness of headlines in comparison to
            full texts in the context of this extrinsic task.
        </abstract>
        <introduction>In this paper we present Topiary, a headlinegeneration system that creates very short, informative
            summaries for news stories by combining sentence compression and unsupervised topic discovery. Hedge Trimmer
            performs sentence compression by removing constituents from a parse tree of the lead sentence according to a
            set of linguistically-motivated heuristics until a length threshold is reached. Unsupervised Topic Discovery
            is a statistical method for deriving a set of topic models from a document corpus, assigning meaningful
            names to the topic models, and associating sets of topics with specific documents. The topics and sentence
            compressions are combined in a manner that preserves the advantages of each approach: the fluency and
            event-oriented information from the lead sentence with the broader coverage of the topic models. The next
            section presents previous work in the area of automatic summarization. Following this we describe Hedge
            Trimmer and Unsupervised Topic Discovery in more detail, and describe the algorithm for combining sentence
            compression with topics. Next we show that Topiary scores higher than either Hedge Trimmer or Unsupervised
            Topic Discovery alone according to certain automatic evaluation tools for summarization. Finally we propose
            event tracking as an extrinsic task using automatic summarization for measuring how human performance is
            affected by automatic summarization, and for correlating human peformance with automatic evaluation tools.
            We describe an experiment that supports event tracking as an appropriate task for this purpose, and show
            results that suggest that a well-written human headline is nearly as useful for event tracking as the full
            text.
        </introduction>
        <corps>manner that preserves the advantages of each approach: the fluency and event-oriented information from
            the lead sentence with the broader coverage of the topic models. The next section presents previous work in
            the area of automatic summarization. Following this we describe Hedge Trimmer and Unsupervised Topic
            Discovery in more detail, and describe the algorithm for combining sentence compression with topics. Next we
            show that Topiary scores higher than either Hedge Trimmer or Unsupervised Topic Discovery alone according to
            certain automatic evaluation tools for summarization. Finally we propose event tracking as an extrinsic task
            using automatic summarization for measuring how human performance is affected by automatic summarization,
            and for correlating human peformance with automatic evaluation tools. We describe an experiment that
            supports event tracking as an appropriate task for this purpose, and show results that suggest that a
            well-written human headline is nearly as useful for event tracking as the full text.
            2

            Previous Work

            Hedge Trimmer is a sentence compression algorithm based on linguistically-motivated heuristics.
            Previous work on sentence compression (Knight and Marcu, 2000) uses a noisy-channel model to find the most
            probable short string that generated the observed full sentence. Other work (Euler, 2002) combines a
            word-list with syntactic in-
            formation to decide which words and phrases to
            cancel. Our approach differs from Knight’s in that we do not use a statistical model, so we do not require
            any prior training on a large corpus of story/headline pairs. Topiary shares with Euler the combination of
            topic lists and sentence compression. However Euler uses the topic lists to guide sentence selection and
            compression towards a query-specific summary, whereas Topiary uses topics to augment the concept coverage of
            a generic summary. Summaries can also consist of lists of words or short phrases indicating that the topic
            or concept they denote is important in the document. Extractive topic summaries consist of keywords or key
            phrases that occur in the document. (Bergler et al., 2003) achieves this by choosing noun phrases that
            represent the most important text entities, as represented by noun phrase coreference chains. (Zhou and
            Hovy, 2003) imposes fluency onto a topic list by finding phrase clusters early in the text that contain
            important topic words found throughout the text. In text categorization documents are assigned to
            pre-defined categories. This is equivalent to assigning topics to a document from a static topic list, so
            the words in the summary need not actually appear in the document. (Lewis, 1992) describes a probabilistic
            feature-based method for assigning Reuters topics to news stories. OnTopic (Schwartz et al., 1997) uses a
            HMM to assign topics from a topic-annotated corpus to a new document.
            3 Algorithm Description
            Topiary produces headlines by combining the output of Hedge Trimmer, a sentence compression algorithm, with
            Unsupervised Topic Detection (UTD). In this section we will give brief descriptions of Hedge Trimmer, recent
            modifications to Hedge Trimmer, and UTD. We will then describe how Hedge Trimmer and UTD are combined. 3.1
            Hedge Trimmer

            Hedge Trimmer (Dorr et al., 2003b) generates
            a headline for a news story by compressing the lead (or main) topic sentence according to a linguistically
            motivated algorithm. For news stories, the first sentence of the document is taken to be the lead sentence.
            The compression consists of
            parsing the sentence using the BBN SIFT parser
            (Miller et al., 1998) and removing low-content syntactic constituents. Some constituents, such as certain
            determiners (the, a) and time expressions are always removed, because they rarely occur in human-generated
            headlines and are low-content in comparison to other constituents. Other constituents are removed one-by-one
            until a length threshold has been reached. These include, among others, relative clauses, verb-phrase
            conjunction, preposed adjuncts and prepositional phrases that do not contain named entities. 1 The threshold
            can be specified either in number of words or number of characters. If the threshold is specified in number
            of characters, Hedge Trimmer will not include partial words. 3.2
            Recent Hedge Trimmer Work

            Recently we have investigated a rendering of the
            summary as “Headlinese” (Mårdh, 1980) in which certain constituents are dropped with no loss of meaning.
            The result of this investigation has been used to enhance Hedge Trimmer, most notably the removal of certain
            instances of have and be. For example, the previous headline generator produced summaries such as Sentence
            (2), whereas the have/be removal produces (3). (1) Input: The senior Olympic official who leveled stunning
            allegations of corruption within the IOC said Sunday he had been “muzzled” by president Juan Antonio
            Samaranch and might be thrown out of the organization. (2) Without participle have/be removal: Senior
            Olympic official said he had been muzzled (3) With participle have/be removal: Senior Olympic official said
            he muzzled by president Juan Antonio Samaranch Have and be are removed if they are part of a past or present
            participle construction. In this example, the removal of had been allows a high-content constituent by
            president Juan Antonio Samaranch to fit into the headline. The removal of forms of to be allows Hedge
            Trimmer to produce headlines that concentrate 1
            More details of the Hedge Trimmer algorithm can be
            found in (Dorr et al., 2003b) and (Dorr et al., 2003a).
            more information in the allowed space. The removal of forms of to be results in sentences that
            are not grammatical in general English, but are typical of Headlinese English. For example, sentences (5),
            (6) and all other examples in this paper were trimmed to fit in 75 characters.
            emergency shortening methods which are only
            to be used when the alternative is truncating the headline after the threshold, possibly cutting the middle
            of a word. These include removal of adverbs and adverbial phrases, adjectives and adjective phrases, and
            nouns that modify other nouns.
            (4) Input: Leading maxi yachts Brindabella, Sayonara and Marchioness were locked in a
            three-way duel down the New South Wales state coast Saturday as the Sydney to Hobart fleet faced
            deteriorating weather.
            3.3

            (5) Without to be removal: Sayonara and Marchioness were locked in three
            (6) With to be removal: Leading maxi yachts Brindabella Sayonara and Marchioness locked in three When have
            and be occur with a modal verb, the modal verb is also removed. Sentence (9) shows an example of this. It
            could be argued that by removing modals such as should and would the meaning is vitally changed. The
            intended use of the headline must be considered. If the headlines are to be used for determining query
            relevance, removal of modals may not hinder the user while making room for additional high-content words may
            help. (7) Input: Organizers of December’s Asian Games have dismissed press reports that a sports complex
            would not be completed on time, saying preparations are well in hand, a local newspaper said Friday. (8)
            Without Modal-Have/Be Removal: Organizers have dismissed press reports saying (9) With Modal-Have/Be
            Removal: Organizers dismissed press reports sports complex not completed saying In addition when it or there
            appears as a subject with a form of be or have, as in extraposition (It was clear that the thief was hungry)
            or existential clauses (There have been a spate of dog maulings), the subject and the verb are removed.
            Finally, for situations in which the length threshold is a hard constraint, we added some
            Unsupervised Topic Discovery

            Unsupervised Topic Discovery (UTD) is used
            when we do not have a corpus annotated with topics. It takes as input a large unannotated corpus in any
            language and automatically creates a set of topic models with meaningful names. The algorithm has several
            stages. First, it analyzes the corpus to find strings of words that occur frequently. (It does this using a
            Minimum Description Length criterion.) These are frequently phrases that are meaningful names of topics.
            Second, it finds the high-content words in each document (using a modified tf.idf measure). These are
            possible topic names for each document. It keeps only those names that occur in at least four different
            documents. These are taken to be an initial set of topic names. In the third stage UTD trains topic models
            corresponding to these topic names. The modified EM procedure of OnTopicTM is used to determine which words
            in the documents often signify these topic names. This produces topic models. Fourth, these topic models are
            used to find the most likely topics for each document. This often adds new topics to documents, even though
            the topic name did not appear in the document. We found, in various experiments, that the topics derived by
            this procedure were usually meaningful and that the topic assignment was about as good as when the topics
            were derived from a corpus that was annotated by people. We have also used this procedure on different
            languages and shown the same behavior. Sentence (10) is a topic list generated for a story about the
            investigation into the bombing of the U.S. Embassy in Nairobi on August 7, 1998. (10) BIN LADEN EMBASSY
            BOMBING POLICE OFFICIALS PRISON HOUSE FIRE KABILA
            3.4

            Combination of Hedge Trimmer and
            Topics: Topiary
            The Hedge Trimmer algorithm is constrained to
            take its headline from a single sentence. It is often the case that there is no single sentence that
            contains all the important information in a story. The information can be spread over two or three
            sentences, with pronouns or ellipsis used to link them. In addition, our algorithms do not always select the
            ideal sentence and trim it perfectly. Topics alone also have drawbacks. UTD rarely generates any topic names
            that are verbs. Thus topic lists are good at indicating the general subject are but rarely give any direct
            indication of what events took place. Topiary is a modification of the enhanced Hedge Trimmer algorithm to
            take a list of topics with relevance scores as additional input. The compression threshold is lowered so
            that there will be room for the highest scoring topic term that isn’t already in the headline. This amount
            of threshold lowering is dynamic, because the trimming of the sentence can remove a previously ineligible
            high-scoring topic term from the headline. After trimming is complete, additional topic terms that do not
            occur in the headline are added to use up any remaining space. This often results in one or more main topics
            about the story and a short sentence that says what happened concerning them. The combination is often more
            concise than a fully fluent sentence and compensates for the fact that the topic and the description of what
            happened to it do not appear in the same sentence in the original story. Sentences (11) and (12) are the
            output of Hedge Trimmer and Topiary for the same story for which the topics in Sentence (10) were generated.
            (11) FBI agents this week began questioning relatives of the victims (12) BIN LADEN EMBASSY BOMBING FBI
            agents this week began questioning relatives Topiary was submitted to the Document Understanding Conference
            Workshop. Figure 1 shows how Topiary peformed in comparison with other DUC2004 participants on task 1, using
            ROUGE. Task 1 was to produce a summary for a single news
            document no more than than 75 characters. The
            different ROUGE variants are sorted by overall performance of the systems. The key observations are that
            there was a wide range of performance among the submitted systems, and that Topiary scored first or second
            among the automatic systems on each ROUGE measure.
            4

            Evaluation

            We used two automatic evaluation systems, BLEU
            (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), to evaluate nine variants of our headline generation
            systems. Both measures make ngram comparisons of the candidate systems to a set of reference summaries. In
            our evaluations four reference summaries for each document were used. The nine variants were run on 489
            stories from the DUC2004 single-document summarization headline generation task. The threshold was 75
            characters, and longer headlines were truncated to 75 characters. We also evaluated a baseline that
            consisted of the first 75 characters of the document. The systems and the average lengths of the headlines
            they produced are shown in Table 1. Trimmer headlines tend to be shorter than the threshold because Trimmer
            removes constituents until the length is below the threshold. Sometimes it must remove a large constituent
            in order to get below the threshold. Topiary is able to make full use of the space by filling in topic
            words. 4.1
            ROUGE

            ROUGE is a recall-based measure for summarizations. This automatic metric counts the number of n-grams in
            the reference summaries that
            occur in the candidate and divides by the number of n-grams in the reference summaries. The size of the
            n-grams used by ROUGE is configurable. ROUGE-n uses 1-grams through n-grams. ROUGE-L is based on longest
            common subsequences, and ROUGE-W-1.2 is based on weighted longest common subsequences with a weighting of
            1.2 on consecutive matches of length greater than 1. The ROUGE scores for the nine systems and the baseline
            are shown in Table 2. Under ROUGE1 the Topiary variants scored significantly higher than the Trimmer
            variants, and both scored signif-
            0.35

            0.3

            0.25

            0.2

            0.15

            0.1

            0.05

            0
            ROUGE-1
            ROUGE-L

            ROUGE-W-1.2

            Automatic Summaries

            ROUGE-2

            ROUGE-3
        </corps>
        <conclusion>Conclusions and Future Work

            We have shown the effectiveness of combining
            sentence compression and topic lists to construct informative summaries. We have compared three
            approaches to automatic headline generation (Topiary, Hedge Trimmer and Unsupervised Topic Discovery) using
            two automatic summarization evaluation tools (BLEU and ROUGE). We have stressed
            the importance of correlating automatic evaluations with human performance of an extrinsic task, and have
            proposed event tracking as an appropriate task for this purpose. We plan to perform a study in which
            Topiary, Hedge Trimmer, Unsupervised Topic Discovery and other summarization methods will be evaluated in
            the context of event tracking. We also plan to extend the tools described in this paper to the domains of
            transcribed broadcast news and crosslanguage headline generation.
            Acknowledgements
            The University of Maryland authors are supported, in part, by BBNT Contract 0201247157, DARPA/ITO Contract
            N66001-97-C-8540, and NSF CISE Research Infrastructure Award EIA0130422.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>Sabine Bergler, René Witte, Michelle Khalife, Zhuoyan Li, and Frank Rudzicz. 2003. Using knowledge-poor
            coreference resolution for text summarization. In Proceedings of the 2003 Document Understanding Conference,
            Draft Papers, pages 85– 92, Edmonton, Candada. Bonnie Dorr, David Zajic, and Richard Schwartz. 2003a.
            Cross-language headline generation for hindi. ACM Transactions on Asian Language Information Processing
            (TALIP), 2:2. Bonnie Dorr, David Zajic, and Richard Schwartz. 2003b. Hedge trimmer: A parse-and-trim
            approach to headline generation. In Proceedings of the HLTNAACL 2003 Text Summarization Workshop, Edmonton,
            Alberta, Canada, pages 1–8. T. Euler. 2002. Tailoring text using topic words: Selection and compression. In
            Proceedings of 13th International Workshop on Database and Expert Systems Applications (DEXA 2002), 2-6
            September 2002, Aix-en-Provence, France, pages 215–222. IEEE Computer Society. Kevin Knight and Daniel
            Marcu. 2000. Statisticsbased summarization – step one: Sentence compression. In The 17th National Conference
            of
            the American Association for Artificial Intelligence
            AAAI2000, Austin, Texas. David Lewis. 1992. An evaluation of phrasal and clustered representations on a text
            categorization task. In Proceedings of the 15th annual international ACM SIGIR conference on Research and
            development in information retrieval, pages 37–50, Copenhagen, Denmark. Chin-Yew Lin and Eduard Hovy. 2003.
            Automatic Evaluation of Summaries Using N-gram CoOccurrences Statistics. In Proceedings of the Conference of
            the North American Chapter of the Association for Computational Linguistics, Edmonton, Alberta. Ingrid
            Mårdh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo. S. Miller, M. Crystal, H.
            Fox, L. Ramshaw, R. Schwartz, R. Stone, and R. Weischedel. 1998. Algorithms that Learn to Extract
            Information; BBN: Description of the SIFT System as Used for MUC-7. In Proceedings of the MUC-7. K.
            Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine
            Translation. In Proceedings of Association of Computational Linguistics, Philadelphia, PA. R. Schwartz, T.
            Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood model for topic classification of
            broadcast news. In Eurospeech-97, Rhodes, Greece. David Zajic, Bonnie Dorr, Richard Schwartz, and Stacy
            President. 2004. Headline evaluation experiment results, umiacs-tr-2004-18. Technical report, University of
            Maryland Institute for Advanced Computing Studies, College Park, Maryland. Liang Zhou and Eduard Hovy. 2003.
            Headline summarization at isi. In Proceedings of the 2003 Document Understanding Conference, Draft Papers,
            pages 174–178, Edmonton, Candada.
        </biblio>
    </article>
    <article>
        <preamble>Boudin-Torres-2006.txt</preamble>
        <titre>A Scalable MMR Approach to Sentence Scoring for Multi-Document Update Summarization</titre>
        <auteur>Florian Boudin \ and Marc El-Bèze \ \ Laboratoire Informatique d’Avignon 339 chemin des Meinajaries,
            BP1228, 84911 Avignon Cedex 9, France. Juan-Manuel Torres-Moreno \,[ [ École Polytechnique de Montréal CP
            6079 Succ. Centre Ville H3C 3A7 Montréal (Québec), Canada. florian.boudin@univ-avignon.fr
            juan-manuel.torres@univ-avignon.fr marc.elbeze@univ-avignon.fr
        </auteur>
        <abstract>redundancy with previously read documents (history) has to be removed from the extract. A natural way
            to go about update summarization would be extracting temporal tags (dates, elapsed times, temporal
            expressions...) (Mani and Wilson, 2000) or to automatically construct the timeline from documents (Swan and
            Allan, 2000). These temporal marks could be used to focus extracts on the most recently written facts.
            However, most recently written facts are not necessarily new facts. Machine Reading (MR) was used by (Hickl
            et al., 2007) to construct knowledge representations from clusters of documents. Sentences containing “new”
            information (i.e. that could not be inferred by any previously considered document) are selected to generate
            summary. However, this highly efficient approach (best system in DUC 2007 update) requires large linguistic
            resources. (Witte et al., 2007) propose a rule-based system based on fuzzy coreference cluster graphs.
            Again, this approach requires to manually write the sentence ranking scheme. Several strategies remaining on
            post-processing redundancy removal techniques have been suggested. Extracts constructed from history were
            used by (Boudin and TorresMoreno, 2007) to minimize history’s redundancy. (Lin et al., 2007) have proposed a
            modified Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) re-ranker during sentence
            selection, constructing the summary by incrementally re-ranking sentences. In this paper, we propose a
            scalable sentence scoring method for update summarization derived from MMR. Motivated by the need for
            relevant novelty, candidate sentences are selected according to a combined criterion of query relevance and
            dissimilarity with previously read sentences. The rest of the paper is organized as follows. Section 2 We
            present S MMR, a scalable sentence scoring method for query-oriented update summarization. Sentences are
            scored thanks to a criterion combining query relevance and dissimilarity with already read documents
            (history). As the amount of data in history increases, non-redundancy is prioritized over query-relevance.
            We show that S MMR achieves promising results on the DUC 2007 update corpus. 1
        </abstract>
        <introduction>Extensive experiments on query-oriented multidocument summarization have been carried out over the
            past few years. Most of the strategies to produce summaries are based on an extraction method, which
            identifies salient textual segments, most often sentences, in documents. Sentences containing the most
            salient concepts are selected, ordered and assembled according to their relevance to produce summaries (also
            called extracts) (Mani and Maybury, 1999). Recently emerged from the Document Understanding Conference (DUC)
            20071 , update summarization attempts to enhance summarization when more information about knowledge
            acquired by the user is available. It asks the following question: has the user already read documents on
            the topic? In the case of a positive answer, producing an extract focusing on only new facts is of interest.
            In this way, an important issue is introduced: c 2008. Licensed under the Creative Commons
            Attribution-Noncommercial-Share Alike 3.0 Unported license
            (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Document Understanding
            Conferences are conducted since 2000 by the National Institute of Standards and Technology (NIST),
            http://www-nlpir.nist.gov 23 Coling 2008: Companion volume – Posters and Demonstrations, pages 23–26
            Manchester, August 2008 introduces our proposed sentence scoring method and Section 3 presents experiments
            and evaluates our approach. sentence s and the query Q:
        </introduction>
        <corps>sentence s and the query Q:
            1 X J We (s, Q) = · max J W(q, m) m∈S 0 |Q|
            2

            where S 0 is the term set of s in which the terms
            m that already have maximized J W(q, m) are removed. The use of J We smooths normalization and misspelling
            errors. Each sentence s is scored using the linear combination:
            q∈Q

            Method

            The underlying idea of our method is that as the
            number of sentences in the history increases, the likelihood to have redundant information within candidate
            sentences also increases. We propose a scalable sentence scoring method derived from MMR that, as the size
            of the history increases, gives more importance to non-redundancy that to query relevance. We define H to
            represent the previously read documents (history), Q to represent the query and s the candidate sentence.
            The following subsections formally define the similarity measures and the scalable MMR scoring method. 2.1
            (1)

            ~
            Sim1 (s, Q) = α · cosine(~s, Q) + (1 − α) · J We (s, Q)
            (2)

            where α = 0.7, optimally tuned on the past DUCs
            data (2005 and 2006). The system produces a list of ranked sentences from which the summary is constructed
            by arranging the high scored sentences until the desired size is reached. 2.2
            A scalable MMR approach

            MMR re-ranking algorithm has been successfully
            used in query-oriented summarization (Ye et al., 2005). It strives to reduce redundancy while maintaining
            query relevance in selected sentences. The summary is constructed incrementally from a list of ranked
            sentences, at each iteration the sentence which maximizes MMR is chosen:
            A query-oriented multi-document
            summarizer
            We have first started by implementing a simple
            summarizer for which the task is to produce queryfocused summaries from clusters of documents. Each document
            is pre-processed: documents are segmented into sentences, sentences are filtered (words which do not carry
            meaning are removed such as functional words or common words) and normalized using a lemmas database (i.e.
            inflected forms “go”, “goes”, “went”, “gone”... are replaced by “go”). An N -dimensional term-space Γ ,
            where N is the number of different terms found in the cluster, is constructed. Sentences are represented in
            Γ by vectors in which each component is the term frequency within the sentence. Sentence scoring can be seen
            as a passage retrieval task in Information Retrieval (IR). Each sentence s is scored by computing a
            combination of two similarity measures between the sentence and the query. The first measure is the well
            known cosine angle (Salton et al., 1975) between the sentence and the query vectorial representations in Γ
            (denoted respectively ~s ~ The second similarity measure is based and Q). on the Jaro-Winkler distance
            (Winkler, 1999). The original Jaro-Winkler measure, denoted J W, uses the number of matching characters and
            transpositions to compute a similarity score between two terms, giving more favourable ratings to terms that
            match from the beginning. We have extended this measure to calculate the similarity between the
            MMR = arg max [ λ · Sim1 (s, Q)
            s∈S
            − (1 − λ) · max Sim2 (s, sj ) ]
            sj ∈E
            (3)

            where S is the set of candidates sentences and E
            is the set of selected sentences. λ represents an interpolation coefficient between sentence’s relevance and
            non-redundancy. Sim2 (s, sj ) is a normalized Longest Common Substring (LCS) measure between sentences s and
            sj . Detecting sentence rehearsals, LCS is well adapted for redundancy removal. We propose an interpretation
            of MMR to tackle the update summarization issue. Since Sim1 and Sim2 are ranged in [0, 1], they can be seen
            as probabilities even though they are not. Just as rewriting (3) as (NR stands for Novelty Relevance): NR =
            arg max [ λ · Sim1 (s, Q) s∈S
            + (1 − λ) · (1 − max Sim2 (s, sh )) ] (4)
            sh ∈H
            We can understand that (4) equates to an OR combination. But as we are looking for a more intuitive AND and
            since the similarities are independent, we have to use the product combination. The
            24
            3. An update summary of documents in C, under the assumption that the reader has already
            read documents in A and B.
            scoring method defined in (2) is modified into a
            double maximization criterion in which the best ranked sentence will be the most relevant to the query AND
            the most different to the sentences in H.
            Within a topic, the document clusters must be processed in chronological order. Our system generates a
            summary for each cluster by arranging the
            high ranked sentences until the limit of 100 words is reached.
            S MMR(s) = Sim1 (s, Q)
             f (H) · 1 − max Sim2 (s, sh ) (5) sh ∈H
            3.2

            Decreasing λ in (3) with the length of the summary was suggested by (Murray et al., 2005) and
            successfully used in the DUC 2005 by (Hachey et al., 2005), thereby emphasizing the relevance at the outset
            but increasingly prioritizing redundancy removal as the process continues. Similarly, we propose to follow
            this assumption in S MMR using a function denoted f that as the amount of data in history increases,
            prioritize nonredundancy (f (H) → 0).
            3

            Most existing automated evaluation methods work
            by comparing the generated summaries to one or more reference summaries (ideally, produced by humans). To
            evaluate the quality of our generated summaries, we choose to use the ROUGE3 (Lin, 2004) evaluation toolkit,
            that has been found to be highly correlated with human judgments. ROUGE N is a n-gram recall measure
            calculated between a candidate summary and a set of reference summaries. In our experiments ROUGE -1, ROUGE
            -2 and ROUGE - SU 4 will be computed.
        </corps>
        <conclusion>Aucune conclusion trouvée.</conclusion>
        <discussion>Discussion and Future Work

            Mani, I. and G. Wilson. 2000. Robust temporal processing of news. In 38th Annual Meeting on Association for
            Computational Linguistics, pages 69–76.
            Association for Computational Linguistics Morristown, NJ, USA.
            In this paper we have described S MMR, a scalable sentence scoring method based on MMR that
            achieves very promising results. An important aspect of our sentence scoring method is that it does not
            requires re-ranking nor linguistic knowledge, which makes it a simple and fast approach to the issue of
            update summarization. It was pointed out at the DUC 2007 workshop that Question Answering and query-oriented
            summarization have been converging on a common task. The value added by summarization lies in the linguistic
            quality. Approaches mixing IR techniques are well suited for query-oriented summarization but they require
            intensive work for making the summary fluent and coherent. Among the others, this is a point that we think
            is worthy of further investigation.
            Murray, G., S. Renals, and J. Carletta. 2005. Extractive
            Summarization of Meeting Recordings. In Ninth European Conference on Speech Communication and Technology.
            ISCA. Salton, G., A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Communications
            of the ACM, 18(11):613–620. Swan, R. and J. Allan. 2000. Automatic generation of overview timelines. In 23rd
            annual international ACM SIGIR conference on Research and development in information retrieval, pages 49–56.
            Acknowledgments

            Winkler, W. E. 1999. The state of record linkage and
            current research problems. In Survey Methods Section, pages 73–79.
            This work was supported by the Agence Nationale
            de la Recherche, France, project RPM2.
            Witte, R., R. Krestel, and S. Bergler. 2007. Generating Update Summaries for DUC 2007. In Document
            Understanding Conference (DUC).
        </discussion>
        <biblio>Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS
            at DUC 2005: Understanding documents via concept links. In Document Understanding Conference (DUC).
            Boudin, F. and J.M. Torres-Moreno. 2007. A Cosine Maximization-Minimization approach for UserOriented
            Multi-Document Update Summarization.
            In Recent Advances in Natural Language Processing (RANLP), pages 81–87. Carbonell, J. and J. Goldstein.
            1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In 21st
            annual international ACM SIGIR conference on Research and development in information retrieval, pages
            335–336. ACM Press New York, NY, USA.
            26
        </biblio>
    </article>
    <article>
        <preamble>compression.txt</preamble>
        <titre>Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks∗</titre>
        <auteur>David Zajic1 , Bonnie J. Dorr1 , Jimmy Lin1 , Richard Schwartz2 1</auteur>
        <abstract>This article examines the application of two single-document sentence compression techniques to the
            problem of multi-document summarization—a “parse-and-trim” approach and a statistical noisy-channel
            approach. We introduce the Multi-Candidate Reduction (MCR) framework for multi-document summarization, in
            which many compressed candidates are generated for each source sentence. These candidates are then selected
            for inclusion in the final summary based on a combination of static and dynamic features. Evaluations
            demonstrate that sentence compression is a valuable component of a larger multi-document summarization
            framework. Keywords: headline generation, summarization, parse-and-trim, Hidden Markov Model PACS:
            Artificial intelligence, 07.05.Mh; Computer science and technology, 89.20.Ff; Spoken languages, processing
            of, 43.71.Sy 1
        </abstract>
        <introduction>This article presents an application of two different single-document sentence compression methods
            to the problem of multi-document summarization. The first, a “parse-and-trim” approach, has been implemented
            in a system called Trimmer and its extended version called Topiary. The second, an HMM-based approach, has
            been implemented in a system called HMM Hedge. These systems share the basic premise that a textual summary
            can be constructed by selecting a subset of words, in order, from the original text, with morphological
            variation allowed for some word classes. Trimmer selects subsequences of words using a
            linguistically-motivated algorithm to trim syntactic constituents from sentences until a desired length has
            been reached. In the context of ∗ Please cite as: David Zajic, Bonnie Dorr, Jimmy Lin, and Richard Schwartz.
            Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks. Information
            Processing and Management, 43(6):1549-1570, 2007. [DOI: 10.1016/j.ipm.2007.01.016] This is the pre-print
            version of a published article. Citations to and quotations from this work should reference that
            publication. If you cite this work, please check that the published form contains precisely the material to
            which you intend to refer. (Received 18 July 2006; revised 3 January 2007; accepted 8 January 2007) This
            document prepared August 31, 2007, and may have minor differences from the published version.
        </introduction>
        <corps>Compression as a Tool for Document Summarization Tasks. Information Processing and Management,
            43(6):1549-1570, 2007. [DOI: 10.1016/j.ipm.2007.01.016] This is the pre-print version of a published
            article. Citations to and quotations from this work should reference that publication. If you cite this
            work, please check that the published form contains precisely the material to which you intend to refer.
            (Received 18 July 2006; revised 3 January 2007; accepted 8 January 2007) This document prepared August 31,
            2007, and may have minor differences from the published version.
            1

            Documents

            Sentence
            Filtering
            Sentences

            Sentence
            Compression Candidates Summary
            Sentence
            Selection
            Task-Specific Features
            (e.g., query)
            Figure 1: The Multi-Candidate Reduction (MCR) framework.
            very short single-document summarization, or headline generation, this approach is applied to the lead
            sentence (the first non-trivial sentence) in order to produce a short headline tens of characters long. HMM
            Hedge (Hidden Markov Model HEaDline GEnerator), on the other hand, is a statistical headline generation
            system that finds the subsequence of words most likely to be a headline for a given story. This approach is
            similar to techniques used in statistical machine translation in that the observed story is treated as the
            garbled version of an unseen headline transmitted through a noisy channel. In their original configurations,
            both Trimmer and HMM Hedge create sentence compressions that mimic the style of Headlinese, a form of
            compressed English associated with newspaper headlines (Mårdh, 1980). The goal of our work is to transition
            from a single-document summarization task with sentencelevel character-based length constraint to a
            multi-document summarization task with summary-level word-based length constraint. In headline generation,
            summaries should fill as much of the available space as possible, without going over the limit. Headlines
            that fall below the length limit miss the opportunity to include additional information. Headlines that
            exceed the length limit must be truncated, resulting in ungrammatical sentences and loss of information.
            When the length constraint is applied to a collection of sentences instead of a single sentence, concerns
            about individual sentence length become less important. A short sentence does not result in wasted space and
            it is acceptable to include a lengthy sentence as long as it includes important, non-redundant information.
            To transition from single-document to multi-document summarization, we examined different combinations of
            possible sentence compressions to construct the best summary. A key innovation of this work—tested for the
            first time in 2005 at the Document Understanding Conference (DUC-2005) (Zajic et al., 2005b) and
            Multilingual Summarization Evaluation (MSE-2005) (Zajic et al., 2005a)—is the use of multiple compressed
            candidates for sentences in the source documents. Sentence compression is used for different purposes in
            single- and multi-document summarization tasks. In the first case, the goal is to fit long sentences into
            the available space while preserving important information. In multidocument summarization, sentence
            compression is used to generate multiple candidates that capture relevant, non-redundant information.
            Sentence selection algorithms can then be applied to determine which compressed candidates provide the best
            combination of topic coverage and brevity. We introduce a framework for extractive multi-document
            summarization called Multi-Candidate Reduction (MCR), whose architecture is shown in Figure 1. Sentence
            compression techniques are employed in the intermediate stage of the processing pipeline to generate
            multiple compressed variants of source sentences. A sentence selector then builds the final summary by
            choosing among these candidates, based on features propagated from the sentence compression method, features
            of the candidates
            2

            themselves, and features of the present summary state. Under this framework, our previously-developed
            systems, Trimmer and HMM Hedge, are employed to generate the compressed candidates. The MCR architecture
            also includes a filtering module that chooses the source sentences from which compressed candidates are
            generated. This work does not examine the filtering process in detail, as we only employ very simple
            approaches, e.g., retain first n sentences from each document. Sentence compression supports extractive
            multi-document summarization by reducing the length of summary candidates while preserving their relevant
            content, thus allowing space for the inclusion of additional material. However, given the interaction of
            relevance and redundancy in a multi-sentence summary, it is unlikely that a single algorithm or scoring
            metric can provide the “best” compression of a sentence. This is the motivation for MCR, which provides
            several alternative candidates for each source sentence. Subsequent processes can then select among many
            alternatives when constructing the final summary. This article is organized as follows: The next section
            relates our approach to other existing summarization systems. In Section 3, we describe Trimmer, a variant
            of Trimmer called Topiary, and HMM Hedge. These systems implement two fundamentally different approaches to
            sentence compression. Section 4 describes how these tools can be applied to a multi-document summarization
            task. Section 5 describes evaluations of our framework. Our systems have also been applied to several
            different types of texts; some of these applications will be briefly covered in Section 6. Finally, we
            propose future work in the area before concluding.
            2

            Related Work

            We have developed a “parse-and-trim” approach to sentence compression, implemented in a system
            called Trimmer (Dorr et al., 2003b). The system generates a headline for a news story by compressing the
            lead (or main) topic sentence according to a linguistically-motivated algorithm that operates on syntactic
            structures. Syntactic approaches to compression have been used in single-document summarization systems such
            as Cut-and-Paste (Jing and McKeown, 2000) and also in multi-document summarization systems such as SC
            (Blair-Goldensohn et al., 2004) and CLASSY (Conroy et al., 2005; Conroy et al., 2006). The SC system
            pre-processes input to remove appositives and relative clauses. CLASSY uses an HMM sentence selection
            approach combined with a conservative sentence compression method based on shallow parsing to detect lexical
            cues to trigger phrase eliminations. The approach taken by Trimmer is most similar to that of Jing and
            McKeown (2000), in which algorithms for removing syntactic constituents from sentences are informed by
            analysis of human summarizers. Trimmer differs from these systems in that multiple compressed candidates of
            each sentence are generated. The potential of multiple alternative compressions has also been explored by
            Vanderwende et al. (2006). Summaries may also contain lists of words or short phrases that denote important
            topics or concepts in the document. In particular, extractive topic summaries consist of keywords or key
            phrases that occur in the document. We have built Topiary, an extension to Trimmer, that constructs
            headlines combining compressed sentences with topic terms. This approach is similar to the work of Euler
            (2002), except that Euler uses topic lists to guide sentence selection and compression toward a
            query-specific summary, whereas Topiary uses topics to augment the concept coverage of a generic summary.
            Our system was demonstrated to be the top-performing single-document summarization system in the DUC-2004
            evaluation (Zajic et al., 2004). In contrast to Topiary, which combines sentence compression with topic
            terms, others have constructed summaries directly from topic terms. For example, Bergler et al. (2003)
            choose noun phrases that represent the most important entities as determined by noun phrase coreference
            chains. Wang et al. (2005) propose a baseline system that constructs headlines from topic descriptors
            identified using term frequency counts; this system was reported to outperform LexTrim, their Topiary-style
            system. Zhou and Hovy (2003) construct fluent summaries from a topic list by finding phrase clusters early
            in 3
            the text that contain important topic words found throughout the text.
            The task of assigning topic terms to documents is related to text categorization, in which documents are
            assigned to pre-defined categories. The categories can be labeled with topic terms, so that the decision to
            put a document in a category is equivalent to assigning that category’s label to the document. Assigning
            topic terms to documents by categorization permits the assignment of terms that do not occur in the
            document. Lewis (1999) describes a probabilistic feature-based method for assigning Reuters topics to news
            stories. Along these lines, OnTopicTM (Schwartz et al., 1997) uses an HMM to assign topics to a document. In
            addition to Trimmer, we have implemented HMM Hedge (Hidden Markov Model HEaDline GEnerator), a statistical
            headline generation system that finds the most likely headline for a given story. This approach is similar
            to techniques used in statistical machine translation in that the observed story is treated as the garbled
            version of an unseen headline transmitted through a noisy channel. The noisychannel approach has been used
            for a wide range of Natural Language Processing (NLP) applications including speech recognition (Bahl et
            al., 1983); machine translation (Brown et al., 1990); spelling correction (Mays et al., 1990); language
            identification (Dunning, 1994); and part-of-speech tagging (Cutting et al., 1992). Of those who have taken a
            noisy-channel approach to sentence compression for text summarization Banko et al. (2000), Knight and Marcu
            (2000), Knight and Marcu (2002), and Turner and Charniak (2005) have used the technique to find the most
            probable short string that generated the observed full sentence. Like other summarization systems based on
            the noisy-channel model, HMM Hedge treats the observed data (the story) as the result of unobserved data
            (headlines) that have been distorted by transmission through a noisy channel. The effect of the noisy
            channel is to add story words between the headline words. Our approach differs from Knight and Marcu (2000),
            Banko et al. (2000), and Turner and Charniak (2005), in that HMM Hedge does not require a corpus of paired
            stories and summaries. HMM Hedge uses distinct language models of news stories and headlines, but does not
            require explicit pairings of stories and summaries. To transition our single-sentence summarization
            techniques to the problem of multi-document summarization, we must consider how to select candidates for
            inclusion in the final summary. A common approach is to rank candidate sentences according to a set of
            features and iteratively build the summary, appropriately re-ranking the candidates at each step to avoid
            redundancy. MEAD (Radev et al., 2004) scores source sentences according to a linear combination of features
            including centroid, position, and first-sentence overlap. These scores are then refined to consider
            cross-sentence dependencies, such as redundancy, chronological order, and source preferences. MCR differs in
            that multiple variants of the same source sentences are available as candidates for inclusion in the final
            summary. Minimization of redundancy is an important element of a multi-document summarization system.
            Carbonell and Goldstein (1998) propose a technique called Maximal Marginal Relevance (MMR) for ranking
            documents returned by an information retrieval system so that the front of the ranked list will contain
            diversity as well as high relevance. Goldstein et al. (2000) extend MMR to multi-document summarization. MCR
            borrows the ranking approach of MMR, but uses a different set of features. Like MEAD, these approaches use
            feature weights that are optimized to maximize an automatic metric on training data. Several researchers
            have shown the importance of summarization in domains other than written news (Muresan et al., 2001; Clarke
            and Lapata, 2006). Within the MCR framework, we discuss the portability of Trimmer and HMM Hedge to a
            variety of different texts: written news, broadcast news transcriptions, email threads, and text in foreign
            language.
            4

            3

            Single-Sentence Compression

            Our general approach to the generation of a summary from a single document is to produce a headline
            by selecting words in order from the text of the story. Consider the following excerpt from a news story and
            corresponding headline: (1)
            (i)

            (ii)

            After months of debate following the Sept. 11 terrorist hijackings, the Transportation
            Department has decided that airline pilots will not be allowed to have guns in the cockpits. Pilots not
            allowed to have guns in cockpits.
            The bold words in (1i) form a fluent and accurate headline, as shown in (1ii).
            This basic approach has been realized in two ways. The first, Trimmer, uses a linguisticallymotivated
            algorithm to remove grammatical constituents from the lead sentence until a length threshold is met. Topiary
            is a variant of Trimmer that combines fluent text from a compressed sentence with topic terms to produce
            headlines. The second, HMM Hedge, employs a noisy-channel model to find the most likely headline for a given
            story. The remainder of this section will present Trimmer, Topiary, and HMM Hedge in more detail.
            3.1

            Trimmer

            Our first approach to sentence compression involves iteratively removing grammatical constituents from
            the parse tree of a sentence using linguistically-motivated rules until a length threshold has been met.
            When applied to the lead sentence, or first non-trivial sentence of a story, our algorithm generates a very
            short summary, or headline. This idea is implemented in our Trimmer system, which can leverage the output of
            any constituency parser that uses the Penn Treebank conventions. At present we use Charniak’s parser
            (Charniak, 2000). The insights that form the basis and justification for the Trimmer rules come from our
            previous study, which compared the relative prevalence of certain constructions in human-written summaries
            and lead sentences in stories. This study used 218 human-written summaries of 73 documents from the TIPSTER
            corpus (Harman and Liberman, 1993) dated January 1, 1989. The 218 summaries and the lead sentences of the 73
            stories were parsed using the BBN SIFT parser (Miller et al., 2000). The parser produced 957 noun phrases
            (NP nodes in the parse trees) and 315 clauses (S nodes in the parse trees) for the 218 summaries. For the 73
            lead sentences, the parser produced 817 noun phrases and 316 clauses. At each level (sentence, clause, and
            noun phrase), different types of linguistic phenomena were counted. • At the sentence level, the numbers of
            preposed adjuncts, conjoined clauses, and conjoined verb phrases were counted. Children of the root S node
            that occur to the left of the first NP are considered to be preposed adjuncts. The bracketed phase in
            “[According to police] the crime rate has gone down” is a prototypical example of a preposed adjunct. • At
            the clause level, temporal expressions, trailing SBAR nodes, and trailing PP nodes were counted. Trailing
            constituents are those not designated as an argument of a verb phrase. • At both the sentence and clause
            levels, conjoined S nodes and conjoined VP nodes were counted. • At the NP level, determiners and relative
            clauses were counted. The counts and prevalence of the phenomena in the human-generated headlines and lead
            sentences are shown in Table 1. The results of this analysis illuminated the opportunities for trimming
            constituents and guided the development of our Trimmer rules, detailed below. 5
            Level
            Sentence
            Clause

            Noun Phrase

            Phenomenon
            preposed adjuncts conjoined S conjoined VP temporal expression trailing PP trailing SBAR relative clause
            determiner
            Summary
            0/218 0% 1/218 0.5% 7/218 3% 5/315 1.5% 165/315 52% 24/315 8% 3/957 0.3% 31/957 3%
            Lead Sentence
            2/73 2.7% 3/73 4% 20/73 27% 77/316 24% 184/316 58% 49/316 16% 29/817 3.5% 205/817 25%
            Table 1: Counts and prevalence of phenomena found in summaries and lead sentences.
            3.1.1
            Trimmer Algorithm

            Trimmer applies syntactic compression rules to a parse tree according the following algorithm:
            1. Remove temporal expressions 2. Select Root S node 3. Remove preposed adjuncts 4. Remove some determiners
            5. Remove conjunctions 6. Remove modal verbs 7. Remove complementizer that 8. Apply the XP over XP rule 9.
            Remove PPs that do not contain named entities 10. Remove all PPs under SBARs 11. Remove SBARSs 12. Backtrack
            to state before Step 9 13. Remove SBARs 14. Remove PPs that do not contain named entities 15. Remove all PPs
            Steps 1 and 4 of the algorithm remove low-content units from the parse tree. Temporal expressions—although
            certainly not content-free—are not usually vital for summarizing the content of an article. Since the goal
            is to provide an informative headline, the identification and elimination of temporal expressions (Step 1)
            allow other more important details to remain in the lengthconstrained headline. The use of BBN’s
            IdentiFinderTM (Bikel et al., 1999) for removal of temporal expressions is described in Section 3.1.2. The
            determiner rule (Step 4) removes leaf nodes that are assigned the part-of-speech tag DT and have the surface
            form the, a, or an. The intuition for this rule is that the information carried by 6
            articles is expendable in summaries, even though this makes the summaries ungrammatical for general
            English. Omitting articles is one of the most salient features of newspaper headlines. Sentences (2) and
            (3), taken from the New York Times website on September 27, 2006, illustrate this phenomenon. The italicized
            articles did not occur in the actual newspaper headlines. (2)
            The Gotti Case Ends With a Mistrial for the Third Time in a Year

            (3)

            A Texas Case Involving Marital Counseling Is the Latest to Test the Line Between Church and
            State
            Step 2 identifies nodes in the parse tree of a sentence that could serve as the root of a compression
            for the sentence. Such nodes will be referred to as Root S nodes. A node in a tree is a Root S node if it is
            labeled S in the parse tree and has children labeled NP and VP, in that order. The human-generated headlines
            we studied always conform to this rule. It has been adopted as a constraint in the Trimmer algorithm that
            the lowest leftmost Root S node is taken to be the root node of the headline. An example of this rule
            application is shown in (4). The boldfaced material in the parse is retained and the italicized material is
            eliminated. (4)
            (i)

            Input: Rebels agreed to talks with government officials, international observers said Tuesday.

            (ii)

            Parse: [S [S [NP Rebels][VP agreed to talks with government officials]], international observers said
            Tuesday.]

            (iii) Output: Rebels agreed to talks with government officials.

            When the parser produces a usable parse tree, this rule selects a valid starting point for compression.
            However, the parser sometimes produces incorrect output, as in the cases below (from DUC-2003): (5)
            (i)

            Parse: [S[SBAR What started as a local controversy][VP has evolved into an
            international scandal.]]
            (ii)

            Parse: [NP[NP Bangladesh][CC and][NP[NP India][VP signed a water sharing
            accord.]]]
            In (5i), an S exists, but it does not conform to the requirements of the Root S rule because it does
            not have as children an NP followed by a VP. The problem is resolved by selecting the lowest leftmost S,
            ignoring the constraints on the children. In (5ii), no S is present in the parse. This problem is resolved
            by selecting the root of the entire parse tree as the root of the headline. These parsing errors occur
            infrequently—only 6% of the sentences in the DUC-2003 evaluation data exhibit these problems, based on
            parses generated by the BBN SIFT parser. The motivation for removing preposed adjuncts (Step 3) is that all
            of the human-generated headlines omit what we refer to as the preamble of the sentence. Preposed adjuncts
            are constituents that precede the first NP (the subject) under the Root S chosen in Step 2; the preamble of
            a sentence consists of its preposed adjuncts. The impact of preposed adjunct removal can be seen in example
            (6). (6)
            (i)

            Input: According to a now finalized blueprint described by U.S. officials and other sources,
            the Bush administration plans to take complete, unilateral control of a post-Saddam Hussein Iraq.
            (ii)

            Parse: [S[PP According to a now finalized blueprint described by U.S. officials and other
            sources], [Det the] Bush administration plans to take complete, unilateral control of[Det a] post-Saddam
            Hussein Iraq.]
            7

            (iii) Output: Bush administration plans to take complete unilateral control of post-Saddam
            Hussein Iraq. The remaining steps of the algorithm remove linguistically peripheral material through
            successive deletions of constituents until the sentence is shorter than a length threshold. Each stage of
            the algorithm corresponds to the application of one of the rules. Trimmer first finds the pool of nodes in
            the parse to which a rule can be applied. The rule is then iteratively applied to the deepest, rightmost
            remaining node in the pool until the length threshold is reached or the pool is exhausted. After a rule has
            been applied at all possible nodes in the parse tree, the algorithm moves to the next step. In the case of a
            conjunction with two children (Step 5), one of the children will be removed. If the conjunction is and , the
            second child is removed. If the conjunction is but, the first child is removed. This rule is illustrated by
            the following examples, where the italicized text is trimmed. (7)
            When Sotheby’s sold a Harry S Truman signature that turned out to be a reproduction, the
            prestigious auction house apologized and bought it back .
            (8)

            President Clinton expressed sympathy after a car-bomb explosion in a Jerusalem market wounded
            24 people but said the attack should not derail the recent land-for-security deal between Israel and the
            Palestinians.
            The modal verb rule (Step 6) applies to verb phrases in which the head is a modal verb and the
            head of the child verb phrase is a form of have or be. In such cases, the modal and auxiliary verbs are
            removed. Sentences (9) and (10) show examples of this rule application. Note that although in Sentence (10)
            the omission of trimmed material changes the meaning, given a tight space constraint, the loss of the
            modality is preferable to the loss of other content information. (9)
            People’s palms and fingerprints may be used to diagnose schizophrenia.

            (10) Agents may have fired potentially flammable tear gas cannisters.
            The complementizer rule (Step 7) removes the word that when it occurs as a complementizer. Sentence (11)
            shows an example in which two complementizers can be removed. (11) Hoffman stressed that study is only
            preliminary and can’t prove that treatment useful. The XP-over-XP rule (Step 8) is a linguistic
            generalization that allows a single rule to cover two different phenomena. XP in the name of the rule is a
            variable that can take two values: NP and VP. In constructions of the form [XP [XP ...] ...], the other
            children of the higher XP are removed. Note that the child XP must be the first child of the parent XP. When
            XP = NP the rule removes relative clauses (as in Sentence (12)) and appositives (as in Sentence (13)). (12)
            Schizophrenia patients whose medication couldn’t stop the imaginary voices in their heads gained some
            relief. (13) A team led by Dr. Linda Brzustowicz, assistant professor of neuroscience at Rutgers
            University’s Center for Molecular and Behavioral Neuroscience in Newark, studied DNA of dozens of members of
            22 families. The rules that remove prepositional phrases and subordinate clauses (Steps 9 through 15) are
            sometimes prone to removing important content. Thus, these rules are applied last, only when there are no
            other types of rules to apply. Moreover, these rules are applied with a backoff option to avoid
            8

            over-trimming the parse tree. First, the PP rule is applied (Steps 9 and 10),1 followed by the SBAR
            rule (Step 11). If the desired sentence length has not been reached, the system reverts to the parse tree as
            it was before any prepositional phrases were removed (Step 12) and applies the SBAR rule (Step 13). If the
            desired length still has not been reached, the PP rule is applied again (Steps 14 and 15). The intuition
            behind this ordering is that, when removing constituents from a parse tree, it is preferable to remove
            smaller fragments before larger ones and prepositional phrases tend to be smaller than subordinate clauses.
            Thus, Trimmer first attempts to achieve the desired length by removing smaller constituents (PPs), but if
            this cannot be accomplished, the system restores the smaller constituents, removes a larger constituent, and
            then resumes the deletion of the smaller constituents. To reduce the risk of removing prepositional phrases
            that contain important information, BBN’s IdentiFinder is used to distinguish PPs containing temporal
            expressions and named entities, as described next. 3.1.2
            Use of BBN’s IdentiFinder in Trimmer

            BBN’s IdentiFinder is used both for the elimination of temporal expressions and for conservative
            deletion of PPs containing named entities. The elimination of temporal expressions (Step 1) is a twostep
            process: (a) use IdentiFinder to mark temporal expressions; and (b) remove [PP ... [NP [X] ...] ...] and [NP
            [X]] where X is tagged as part of a temporal expression. The following examples illustrate the application
            of temporal expression removal rule: (14) (i) (ii)
            Input: The State Department on Friday lifted the ban it had imposed on foreign fliers.
            Parse: [S [NP[Det The] State Department [PP [IN on] [NP [NNP Friday]]] [VP lifted [Det the] ban it had
            imposed on foreign fliers.]] (iii) Output: State Department lifted ban it had imposed on foreign fliers.
            (15) (i)

            Input: An international relief agency announced Wednesday that it is withdrawing from
            North Korea. (ii) Parse: [S [NP [Det An] international relief agency][VP announced [NP [NNP Wednesday]] that
            it is withdrawing from North Korea.]] (iii) Output: International relief agency announced that it is
            withdrawing from North Korea.
            IdentiFinder is also used to ensure that prepositional phrases containing named entities are not
            removed during the first round of PP removal (Step 9). However, prepositional phrases containing named
            entities that are descendants of SBARs are removed before the parent SBAR is removed, since we should remove
            a smaller constituent before removing a larger constituent that subsumes it. Sentence (16) shows an example
            of a SBAR subsuming two PPs, one of which contains a named entity. (16) The commercial fishing restrictions
            in Washington will not be lifted [SBAR unless the salmon population increases [PP to a sustainable number]
            [PP in the Columbia River]]. If the PP rule were not sensitive to named entities, the PP in the Columbia
            River would be the first prepositional phrase to be removed, because it is the lowest rightmost PP in the
            parse. However, this PP provides an important piece of information: the location of the salmon population.
            The rule in Step 9 will skip the last prepositional phrase and remove the penultimate PP to a sustainable
            number . This concludes an overview of the Trimmer rules and our syntactic sentence compression algorithm.
            Given a length limit, the system will produce a single compressed version of the target sentence. Multiple
            compressions can be generated by setting the length limit to be very small and storing the state of the
            sentence after each rule application as a compressed variant. In section 4, we will describe how multiple
            compressed candidates generated by Trimmer are used as a component of a multi-document summarization system.
            1
            The reason for breaking PP removal into two stages is discussed in Section 3.1.2.

            9

            3.2

            Topiary

            We have used the Trimmer approach to compression in another variant of single-sentence summarization
            called Topiary. This system combines Trimmer with a topic discovery approach (described next) to produce a
            fluent summary along with additional context. The Trimmer algorithm is constrained to build a headline from
            a single sentence. However, it is often the case that no single sentence contains all the important
            information in a story. Relevant information can be spread over multiple sentences, linked by anaphora or
            ellipsis. In addition, the choice of lead sentence may not be ideal and our trimming rules are imperfect. On
            the other hand, approaches that construct headlines from lists of topic terms (Lewis, 1999; Schwartz et al.,
            1997) also have limitations. For example, Unsupervised Topic Discovery (UTD)— described below—rarely
            generates any topic terms that are verbs. Thus, topic lists are good at indicating the general subject but
            rarely give any direct indication of what events took place. Intuitively, we need both fluent text to tell
            what happened and topic terms to provide context. 3.2.1
            Topic Term Generation: UTD and OnTopic

            OnTopic (Schwartz et al., 1997) uses an HMM to assign topics to a document; topic models are derived
            from an annotated corpus. However, it is often difficult to acquire such data, especially for a new genre or
            language. UTD (Sista et al., 2002) was developed to overcome this limitation: it takes as input a large
            unannotated corpus and automatically creates a set of topic models with meaningful names. The UTD algorithm
            has several stages. First, it analyzes the corpus to find multi-word sequences that can be treated as single
            tokens. It does this using two methods. One method is a minimum description length criterion that detects
            phrases that occur frequently relative to the individual words. The second method uses BBN’s IdentiFinder to
            detect multi-word names. These names are added to the text as additional tokens. They are also likely to be
            chosen as potential topic names. In the second stage of UTD, we find those terms (both single-word and
            multi-word) with high tf.idf. Only those topic names that occur as high-content terms in at least four
            different documents are kept. The third stage trains topic models corresponding to these topic terms. The
            modified Expectation Maximization procedure of BBN’s OnTopic system is used to determine which words in the
            documents often signify these topic names. This produces topic models. Fourth, these topic models are used
            to find the most likely topics for each document, which is equivalent to assigning the name of the topic
            model to the document as a topic term. This often assigns topics to documents where the topic name does not
            occur in the document text. We found, in various experiments (Sista et al., 2002), that the topic names
            derived by this procedure were usually meaningful and that the topic assignment was about as good as when
            the topics were derived from a corpus that was annotated by people. We have also used this procedure on
            different languages and shown the same behavior. Since UTD is unsupervised, it can run equally well on a new
            language, as long as the documents can be divided into strings that approximate words. The topic list in
            (17) was generated by UTD and OnTopic for a story about the FBI investigation of the 1998 bombing of the
            U.S. embassy in Nairobi. (17) BIN LADEN, EMBASSY, BOMBING, POLICE OFFICIALS, PRISON, HOUSE, FIRE, KABILA
            Topiary uses UTD to generate topic terms for the collection of documents to be summarized and uses OnTopic
            to assign the topic terms to the documents. The next section will describe how topic terms and sentence
            compressions are combined to form Topiary summaries.
            10

            3.2.2

            Topiary Algorithm

            As each Trimmer rule is applied to a sentence, the resulting state of the sentence is stored as a
            compressed variant of the source sentence. Topiary selects from the variants the longest one such that there
            is room to prepend the highest scoring non-redundant topic term. Suppose the highest scoring topic term is
            “terrorism” and the length threshold is 75 characters. To make room for the topic “terrorism”, the length
            threshold is lowered by 10 characters: 9 characters for the topic and 1 character as a separator. Thus,
            Topiary chooses the longest trimmed variant under 65 characters that does not contain the word “terrorism”,
            If there is no such candidate, i.e., all the trimmed variants contain the word terrorism, Topiary would
            consider the second highest scoring topic word, “bomb”. Topiary would select the longest trimmed variant
            under 70 characters that does not contain the word “bomb”. After Topiary has selected a trimmed variant and
            prepended a topic to it, it checks to see how much unused space remains under the threshold. Additional
            topic words are added between the first topic word and the compressed sentence until all space is exhausted.
            This process results in a headline that contains one or more main topics about the story and a short
            sentence that says what happened concerning them. The combination is often more concise than a fully fluent
            sentence and compensates for the fact that the information content from the topic and the compressed
            sentence do not occur together in any single sentence from the source text. As examples, sentences (18) and
            (19) are the outputs of Trimmer and Topiary, respectively, for the same story in which UTD selected the
            topic terms in (17). (18) FBI agents this week began questioning relatives of the victims (19) BIN LADEN,
            EMBASSY, BOMBING: FBI agents this week began questioning relatives By combining topics and parse-and-trim
            compression, Topiary achieved the highest score on the single-document summarization task (i.e., headline
            generation task) in DUC-2004 (Zajic et al., 2004).
            3.3

            HMM Hedge

            Our second approach to sentence compression, implemented in HMM Hedge, treats the observed data
            (the story) as the result of unobserved data (headlines) that have been distorted by transmission through a
            noisy channel. The effect of the noisy channel is to add story words between the headline words. The model
            is biased by parameters to make the resulting headlines more like Headlinese, the observed language of
            newspaper headlines created by copy editors. Formally, we consider a story S to be a sequence of N words. We
            want to find a headline H, a subsequence of words from S, that maximizes the likelihood that H generated the
            story S, or: argmaxH P (H|S) It is difficult to directly estimate P (H|S), but this probability can be
            expressed in terms of other probabilities that are easier to compute, using Bayes’ rule: (H) P (H|S) = P
            (S|H)P P (S)
            Since the goal is to maximize this expression over H, and P (S) is constant with respect to H, the
            denominator of the above expression can be omitted. Thus we wish to find: argmaxH P (S|H)P (H) Let H be a
            headline consisting of words h1 , h2 , ..., hn . Let the special symbols start and end represent the
            beginning and end of a headline, respectively. P (H) can be estimated using a bigram model of Headlinese: P
            (H) = P (h1 |start)P (h2 |h1 )...P (end|hn ) 11
            The bigram probabilities of the words in the headline language were computed from a corpus of
            English headlines taken from 242,918 AP newswire stories in the TIPSTER corpus. The headlines contain
            2,848,194 words from a vocabulary of 88,627 distinct words. Given a story S and a headline H, the action of
            the noisy channel is to form S by adding non-headline words to H. Let G be the non-headline words added by
            the channel to the headline: g1 , g2 , ..., gm . For the moment, we assume that the headline words are
            transmitted through the channel with probability 1. We estimate P (S|H), the probability that the channel
            added non-headline words G to headline H to form story S. This is accomplished using a unigram model of
            newspaper stories that we will refer to as the general language, in contrast to the headline language. Let
            Pgl (g) be the probability of non-headline word g in the general language and Pch (h) = 1 be the probability
            that headline word h is transmitted through the channel as story word h. P (S|H) = Pgl (g1 )Pgl (g2 )...Pgl
            (gm )Pch (h1 )Pch (h2 )...Pch (hn ) = Pgl (g1 )Pgl (g2 )...Pgl (gm ) The unigram probabilities of the words
            in the general language were computed from 242,918 English AP news stories in the TIPSTER corpus. The
            stories contain 135,150,288 words from a vocabulary of 428,633 distinct words. The process by which the
            noisy channel generates a story from a headline can be represented by a Hidden Markov Model (HMM) (Baum,
            1972). An HMM is a weighted finite-state automaton in which each state probabilistically emits a string. The
            simplest HMM for generating headlines consists of two states: an H state that emits words that occur in the
            headline and a G state that emits all the other words in the story. Since we use a bigram model of
            headlines, each state that emits headline words must “remember” the previously emitted headline word. If we
            did not constrain headline words to actually occur in the story, we would need an H state for each word in
            the headline vocabulary. However, because headline words are chosen from the story words, it is sufficient
            to have an H state for each story word. For any story, the HMM consists of a start state S, an end state E,
            an H state for each word in the story, a corresponding G state for each H state, and a state Gstart that
            emits words that occur before the first headline word in the story. An H state can emit only the word it
            represents. The corresponding G state remembers which word was emitted by its H state and can emit any word
            in the story language. A headline corresponds to a path through the HMM from S to E that emits all the words
            in the story in the correct order. In practice the HMM is constructed with states for only the first N words
            of the story, where N is a constant (60), or N is the number of words in the first sentence.2 In example
            (1i), given earlier, the H states will emit the words in bold (pilots, not, allowed, to, have, guns, in,
            cockpits) and the G states will emit all the other words. The HMM will transition between the H and G states
            as needed to generate the words of the story. In the current example, the model will have states Start,
            Gstart , End, and 28 H states with 28 corresponding G states.3 The headline given in example (1ii)
            corresponds to the following sequence of states: Start, Gstart 17 times, Hpilots , Gpilots , Hnot , Gnot ,
            Hallowed , Hto , Hhave , Hguns , Hin , Gin , Hcockpits , End. This path is not the only one that could
            generate the story in (1i). Other possibilities are: (20) (i) (ii)
            Transportation Department decided airline pilots not to have guns.
            Months of the terrorist has to have cockpits.
            2

            Limiting consideration of headline words to the early part of the story is justified in Dorr et al. (2003a)
            where it
            was shown that more than half of the headline words are chosen from the first sentence of the story. Other
            methods for selecting the window of story words are possible and will be explored in future research. 3 The
            subscript of a G state indicates the H state it is associated with, not the story word it emits. In the
            example, Gpilots emits story word will , Gnot emits story word be, and Gin emits story word the.
            12

            Although (20i) and (20ii) are possible headlines for (1i), the conditional probability of (20ii) given (1i)
            will be lower than the conditional probability of (20i) given (1i). The Viterbi algorithm (Viterbi, 1967) is
            used to select the most likely headline for a given story. We use length constraints to find the most likely
            headlines consisting of W words, where W ranges from 5 to 15. Multiple backpointers are used so that we can
            find the n most likely headlines at each length. HMM Hedge is enhanced by three additional decoding
            parameters to help the system choose outputs that best mimic actual headlines: a position bias, a clump
            bias, and a gap bias. The incorporation of these biases changes the score produced by the decoder from a
            probability to a relative desirability score. The three parameters were motivated by analysis of system
            output and their values were set by trial and error. A logical extension to this work would be to learn the
            best setting of these biases, e.g., through Expectation Maximization. The position bias favors headlines
            that include words near the front of the story. This reflects our observations of human-constructed
            headlines, in which headline words tend to appear near the front of the story. The initial position bias p
            is a positive number less than one. The story word in the nth position is assigned a position bias of log(pn
            ). When an H state emits a story word, the position bias is added to the desirability score. Thus, words
            near the front of the story carry less of a position bias than words farther along. Note that this
            generalization often does not hold in the case of human interest and sports stories, which may start with a
            hook to get the reader’s attention, rather than a topic sentence. We also observed that human-constructed
            headlines tend to contain contiguous blocks of story words. Example (1ii), given earlier, illustrates this
            with the string “allowed to have guns”. The string bias is used to favor “clumpiness”. i.e., the tendency to
            generate headlines composed of clumps of contiguous story words. The log of the clump bias is added to the
            desirability score with each transition from an H state to its associated G state. With high clump biases,
            the system will favor headlines consisting of fewer but larger clumps of contiguous story words. The gap
            bias is used to disfavor headline “gappiness”, i.e., large gaps of non-headline words in the story between
            clumps of headline words. Although humans are capable of constructing fluent headlines by selecting widely
            spaced words, we observed that HMM Hedge was more likely to combine unrelated material by doing this. At
            each transition from a G state to an H state, corresponding to the end of a sequence of non-headline words
            in the story, a gap bias is applied that increases with the size of the gap between the current headline and
            the last headline word to be emitted. This can also be seen as a penalty for spending too much time in one G
            state. With high gap biases, the system will favor headlines with few large gaps. One characteristic
            difference between newspaper headline text and newspaper story text is that headlines tend to be in present
            tense while story sentences tend to be in the past tense. Past tense verbs occur more rarely in the headline
            language than in the general language. HMM Hedge mimics this aspect of Headlinese by allowing morphological
            variation between headline verbs and the corresponding story verbs. Morphological variation for verbs is
            achieved by creating an H state for each available variant of a story verb. These H states still emit the
            story verb but they are labeled with the variant. HMM Hedge can generate a headline in which proposes is the
            unobserved headline word that emits the observed story word proposed , even though proposes does not occur
            in the story. (21) (i) (ii)
            A group has proposed awarding $1 million in every general election to one randomly chosen
            voter. Group proposes awarding $1 million to randomly chosen voter.
            Finally, we constrain each headline to contain at least one verb. That is to say, we ignore headlines
            that do not contain at least one verb, no matter how desirable the decoding is. 13
            Although we have described an application of HMM Hedge to blocks of story words without reference
            to sentence boundaries, it is also possible to use HMM as a single sentence compressor by limiting the block
            to the words in a single sentence. Also, as we will see shortly, multiple alternative compressions of a
            sentence may be generated with HMM Hedge. The Viterbi algorithm is capable of discovering n-best
            compressions of a window of story words and can be constrained to consider only paths that include a
            specific number of H states, corresponding to compressions that contain a specific number of words. We use
            HMM Hedge to generate 55 compressions for each sentence by computing the five best headlines at each length,
            from 5 to 15 words. In Section 4 we will describe how HMM Hedge is used as a component of a multi-document
            summarization system.
            4

            Multi-Document Summarization

            The sentence compression tools we developed for single-document summarization have been incorporated into
            our Multi-Candidate Reduction framework for multi-document summarization. MCR
            produces a textual summary from a collection of relevant documents in three steps. First, sentences are
            selected from the source documents for compression. The most important information occurs near the front of
            the stories, so we select the first five sentences of each document for compression. Second, multiple
            compressed versions of each sentence are produced using Trimmer or HMM Hedge to create a pool of candidates
            for inclusion in the summary. Finally, a sentence selector constructs the summary by iteratively choosing
            from the pool of candidates based on a linear combination of features until the summary reaches a desired
            length. At present, weights for the features are determined by manually optimizing on a set of training data
            to maximize the Rouge-2 recall score (Lin and Hovy, 2003), using Rouge version 1.5.5. A typical
            summarization task might call for the system to generate a 250-word summary from a couple of dozen news
            stories. These summaries may be query-focused, in the sense that the summaries should be responsive to a
            particular information need, or they may be generic, in that a broad overview of the documents is desired.
            Our sentence selector adopts certain aspects of Maximal Marginal Relevance (MMR) (Carbonell and Goldstein,
            1998), an approach that attempts to balance relevance and anti-redundancy. In MCR’s selection module, the
            highest scoring sentence from the pool of eligible candidates is chosen for inclusion in the summary.
            Features that contribute to a candidate’s score can be divided into two types: dynamic and static. When a
            candidate is chosen, all other compressed variants of that sentence are eliminated. After a candidate is
            added to the summary, the dynamic features are re-computed, and the candidates are re-ranked. Candidates are
            added to the summary until the desired length is achieved. The ordering of candidates in the summary is the
            same as the order in which they were selected for inclusion. The final sentence of the summary is truncated
            if it causes the summary to exceed the length limit.
            4.1

            Static Features

            Static features are calculated before sentence selection begins and do not change during the process of
            summary construction: • Position. The zero-based position of the sentence in the document. • Sentence
            Relevance. The relevance score of the sentence to the query. • Document Relevance. The relevance score of
            the document to the query. • Sentence Centrality. The centrality score of the sentence to the topic cluster.
            14
            • Document Centrality. The centrality score of the document to the topic cluster.
            • Scores from the Compression Modules: – Trim rule application counts. For Trimmer-based MCR, the number of
            Trimmer rule instances applied to produce the candidate. – Negative Log Desirability. For HMM-based MCR, the
            relative desirability score of the candidate. We use the Uniform Retrieval Architecture (URA), University of
            Maryland’s software infrastructure for information retrieval tasks, to compute relevance and centrality
            scores for each compressed candidate. There are four such scores: the relevance score between a compressed
            sentence and the query, the relevance score between the document containing the compressed sentence and the
            query, the centrality score between a compressed sentence and the topic cluster, and the centrality score
            between the document containing the compressed sentence and the topic cluster. We define the topic cluster
            to be the entire collection of documents relevant to this particular summarization task. Centrality is a
            concept that quantifies how similar a piece of text is to all other texts that discuss the same general
            topic. We assume that sentences having higher term overlap with the query and sources more “central” to the
            topic cluster are preferred for inclusion in the final summary. The relevance score between a compressed
            sentence and the query is an idf-weighted count of overlapping terms (number of terms shared by the two text
            segments). Inverse document frequency (idf), a commonly-used measure in the information retrieval
            literature, roughly captures term salience. The idf of a term t is defined by log(N/ct ), where N is the
            total number of documents in a particular corpus and ct is the number of documents containing term t; these
            statistics were calculated from one year’s worth of LA Times articles. Weighting term overlap by inverse
            document frequency captures the intuition that matching certain terms is more important than matching
            others. Lucene, a freely-available off-the-shelf information retrieval system, is used to compute the three
            other scores. The relevance score between the document containing the compressed sentence and the query is
            computed using Lucene’s built-in similarity function. The centrality score between the compressed sentence
            and the topic cluster is the mean of the similarity between the sentence and each document comprising the
            cluster (once again, as computed by Lucene’s built-in similarity function). The document-cluster centrality
            score is also computed in much the same way, by taking the mean of the similarity of the particular document
            with every other document in the cluster. In order to obtain an accurate distribution of term frequencies to
            facilitate the similarity calculation, we indexed all relevant documents (i.e., the topic cluster) along
            with a comparable corpus (one year of the LA Times)—this additional text essentially serves as a background
            model for non-relevant documents. Some features are derived from the sentence compression modules used to
            generate candidates. For Trimmer, the rule application count feature of a candidate is the number of rules
            that were applied to a source sentence to produce the candidate. The rules are not presumed to be equally
            effective, so the rule application counts are broken down by rule type. For HMM Hedge, we use the relative
            desirability score calculated by the decoder, expressed as a negative log. The features discussed in this
            section are assigned to the candidates before summary generation begins and remain fixed throughout the
            process of summary sentence selection. The next section discusses how candidate features are assigned new
            values as summary geneneration proceeds.
            4.2

            Dynamic Features

            Dynamic features change during the process of sentence selection to reflect changes in the state of the
            summary as sentences are added.4 The dynamic features are: 4 At present the dynamic features are properties
            of the candidates, calculated with respect to the current summary state. There are no features directly
            relating to the amount of space left in the summary, so there is no mechanism that
            15

            • Redundancy. A measure of how similar the sentence is to the current summary.
            • Sentence-from-doc. The number of sentences already selected from the sentence’s document. The intuition
            behind our redundancy measure is that candidates containing words that occur much more frequently in the
            current state of the summary than they do in general English are redundant to the summary. We imagine that
            sentences in the summary are generated by the underlying word distribution of the summary rather than the
            distribution of words in the general language. If a sentence appears to have been generated by the summary
            rather than by the general language, we take it to be redundant to the summary. Suppose we have a summary
            about earthquakes. The presence in a candidate of words like earthquake, seismic, and Richter Scale, which
            have a high likelihood in the summary, will make us think that the candidate is redundant to the summary. To
            estimate the extent to which a candidate is more likely to have been generated by a summary than by the
            general language, we consider the probabilities of the words in the candidate. We estimate that the
            probability that a word w occurs in a candidate generated by the summary is P (w) = λP (w|D) + (1 − λ)P
            (w|C) where D is the summary, C is the general language corpus5 , λ is a parameter estimating the
            probability that the word was generated by the summary and (1−λ) is the probability that the word was
            generated by the general language. We have set λ = 0.3, as a general estimate of the portion of words in a
            text that are specific to the text’s topic. We estimate the probabilities by counting the words6 in the
            current summary and the general language corpus: P (w|D) =
            count of w in D
            size of D
            P (w|C) =

            count of w in C
            size of C
            We take the probability of a sentence to be the product of the probabilities of its words, so we calculate
            the probability that a sentence was generated by the summary, i.e. our redundancy metric, as: Y
            Redundancy(S) = λP (s|D) + (1 − λ)P (s|C) s∈S
            For ease of computation, we actually use log probabilities:
            X log(λP (s|D) + (1 − λ)P (s|C)) s∈S
            Redundancy is a dynamic feature because the word distribution of the current summary changes with
            every iteration of the sentence selector.
            4.3

            Examples of System Output

            We applied our MCR framework to test data from the DUC-2006 evaluation (Dang and Harman, 2006).
            Given a topic description and a set of 25 documents related to the topic (drawn from AP newswire, the New
            York Times, and the Xinhua News Agency English Service), the system’s task was to create would affect the
            distribution of compressed candidates over the iterations of the sentence selector. This issue will be
            addressed as future work in Section 7. 5 The documents in the set being summarized are used to estimate the
            general language model. 6 Actually, preprocessing for redundancy includes stopword removal and applying the
            Porter Stemmer (Porter, 1980).
            16

            Title: Native American Reservation System—pros and cons
            Narrative Description: Discuss conditions on American Indian reservations or among Native American
            communities. Include the benefits and drawbacks of the reservation system. Include legal privileges and
            problems. Figure 2: Topic D0601A from the DUC-2006 multi-document summarization task. a 250-word summary
            that addressed the information need expressed in the topic. One of the topic descriptions is shown in Figure
            2. The 25 documents in the document set have an average size of 1170 words, so a 250-word summary represents
            a compression ratio of 0.86%. Figures 3, 4 and 5 show examples of MCR output using Trimmer compression, HMM
            Hedge compression, or no compression. For readability, we use ◦ as a sentence delimiter; this is not part of
            the actual system output. The sentences compressed by Trimmer mimic Headlinese by omitting determiners and
            auxiliary verbs. For example, the first sentence in Figure 3 is a compression of the following source
            sentence: Seeking to get a more accurate count of the country’s American Indian population, the Census
            Bureau is turning to tribal leaders and residents on reservations to help overcome long-standing feelings of
            wariness or anger toward the federal government. Three determiners and a form of be have been removed from
            the source sentence in the compression that appears in the summary. The removal of this material makes the
            sentence appear more like a headline. In comparison with Trimmer compressions, HMM compressions are
            generally less readable and more likely to be misleading. Consider the final sentence in Figure 4. (22) main
            purpose of reservation to pay American Indians by poverty proposals This is a compression of the following
            source sentence: (23) But the main purpose of the visit—the first to a reservation by a president since
            Franklin Roosevelt—was simply to pay attention to American Indians, who are so raked by grinding poverty
            that Clinton’s own advisers suggested he come up with special proposals geared specifically to the Indians’
            plight. Because HMM Hedge uses a bigram model of Headlines, it is unable to capture sentence-level
            grammaticality. The same limitation makes it difficult to prevent misleading or incorrect compressions. For
            example, the third sentence from the end of Figure 4 seems to say that a court legalized gambling on Indian
            reservations: (24) Supreme Court allows legalized gambling Indian reservations However, it is a compression
            of the following source sentence: (25) Only Monday, the California Supreme Court overturned a ballot measure
            that would have allowed expansion of legalized gambling on Indian reservations. Nevertheless, we can see
            from the examples that sentence compression allows a summary to include more material from other sources.
            This increases the topic coverage of system output. 17
            Seeking to get more accurate count of country’s American Indian population, Census Bureau turning to
            tribal leaders and residents on reservations to help overcome long-standing feelings. ◦ American Indian
            reservations would get infusion. ◦ Smith and thousands seeking help for substance abuse at American Indian
            Community House, largest of handful of Native American cultural institutions. ◦ Clinton going to Pine Ridge
            Reservation for visit with Oglala Sioux nation and to participate in conference on Native American
            homeownership and economic development. ◦ Said Glen Revere, nutritionist with Indian Health Services on 2.8
            million-acre Wind River Reservation, about 100 miles east of Jackson, Wyo. “Then we came up with idea for
            this community garden, and it been bigger than we ever expected.” ◦ Road leading into Shinnecock Indian
            reservation is not welcoming one But main purpose of visit – first to reservation by president since
            Franklin Roosevelt – was simply to pay attention to American Indians, who raked by grinding poverty
            Clinton’s own advisers suggested he come up with special proposals geared specifically to Indians’ plight. ◦
            “This highlights what going on out there, since beginning of reservation system,” said Sidney Harring,
            professor at City University of New York School of Law and expert on Indian crime and criminal law. ◦
            American Indians are victims. ◦ President Clinton turned attention to arguably poorest, most forgotten ◦
            U.S. citizens: American Indians. ◦ When American Indians began embracing gambling, Hualapai tribe moved
            quickly to open casino. ◦ members of Northern Arapaho Tribe on Wind River Reservation started seven-acre
            community garden with donated land, seeds and
            Figure 3: MCR Summary for DUC-2006 Topic D0601A, using Trimmer for sentence compression.

            David Rocchio deputy legal counsel to Vermont Gov. Howard Dean who has been involved in discussions
            on Indian gambling through the National Governors Association said that the concern that governors have is
            not with the benefit casinos bring to tribes ◦ Native Americans living on reservations that maintain 50
            percent or more unemployment are exempt from the national five year family limit on welfare benefits ◦ Smith
            and thousands like her are seeking help for their substance abuse at the American Indian Community House the
            largest of a handful of Native American cultural institutions in the New York area ◦ Juvenile crime is one
            strand in the web of social problems facing urban and reservation Indian communities the report said ◦
            Soldierwolf’s family represents the problems that plague many of the 1.3 million American Indians who live
            on reservations of whom 49 percent are unemployed ◦ Powless said the Onondaga people want to work with the
            community outside the reservation to improve the economy of the region perhaps creating tourism destinations
            that might include Indian culture or setting up a free trade zone at unused manufacturing sites ◦ As Indian
            communities across the nation struggle with short funds and a long list of problems they are watching the
            Navajo Nation’s legal battle with the federal government ◦ recognize Indians not only Native Americans as
            Americans ◦ go on reservation system Harring Indian ◦ Supreme Court allows legalized gambling Indian
            reservations ◦ American Indian reservations tribal colleges rise faster than ◦ main purpose of reservation
            to pay American Indians by poverty proposals
            Figure 4: MCR Summary for DUC-2006 Topic D0601A, using HMM Hedge for sentence compression

            18

            Seeking to get a more accurate count of the country’s American Indian population, the Census Bureau is
            turning to tribal leaders and residents on reservations to help overcome long-standing feelings of wariness
            or anger toward the federal government. ◦ American Indian reservations would get an infusion of $1.2 billion
            in federal money for education, health care and law enforcement under President Clinton’s proposed 2001
            budget ◦ Smith and thousands like her are seeking help for their substance abuse at the American Indian
            Community House, the largest of a handful of Native American cultural institutions in the New York area. ◦
            Clinton was going to the Pine Ridge Reservation for a visit with the Oglala Sioux nation and to participate
            in a conference on Native American homeownership and economic development. ◦ said Glen Revere, a
            nutritionist with the Indian Health Services on the 2.8 million-acre Wind River Reservation, about 100 miles
            east of Jackson, Wyo. “Then we came up with the idea for this community garden, and it’s been bigger than we
            ever expected in so many ways.” ◦ The road leading into the Shinnecock Indian reservation is not a welcoming
            one ◦ But the main purpose of the visit – the first to a reservation by a president since Franklin Roosevelt
            – was simply to pay attention to American Indians, who are so raked by grinding poverty that Clinton’s own
            advisers suggested he come up with special proposals geared specifically to the Indians’ plight. ◦ “This
            highlights what has been going on out there for 130 years,
            Figure 5: MCR Summary for DUC-2006 Topic D0601A, with no sentence compression

            19

            R1 Recall

            R1 Precision

            R1 F

            R2 Recall

            R2 Precision

            R2 F

            HMM
            Sentence 0.23552 (0.230140.24082) 0.21896 (0.213860.22384) 0.22496 (0.219830.22978) 0.06838 (0.065460.07155)
            0.06287 (0.060170.06576) 0.06488 (0.062090.06785)
            HMM
            60 Block 0.21381 (0.209120.21827) 0.18882 (0.184440.19301) 0.19966 (0.195050.20391) 0.06133 (0.058480.06414)
            0.05351 (0.050970.05588) 0.05686 (0.054200.05942)
            Trimmer

            Topiary

            0.21014
            (0.204360.21594) 0.20183 (0.196270.20722) 0.20179 (0.196120.20718) 0.06337 (0.060300.06677) 0.06230
            (0.058870.06617) 0.06079 (0.057880.06401)
            0.25143
            (0.246320.25663) 0.23038 (0.225670.23522) 0.23848 (0.233730.24328) 0.06637 (0.063450.06958) 0.06024
            (0.057470.06326) 0.06252 (0.059760.06561)
            Table 2: Rouge scores and 95% confidence intervals for 624 documents from DUC-2003 test set.

            5

            System Evaluations

            We tested four single-document summarization systems on the DUC-2003 Task 1 test set:
            • HMM Hedge using the first sentence of each document (HMM Sentence) • HMM Hedge using the first 60 words of
            each document (HMM 60 block) • Trimmer • Topiary Task 1 from DUC-2003 was to construct generic 75-byte
            summaries for 624 documents drawn from AP Newswire and the New York Times. The average size of the documents
            was 3,997 bytes, so a 75-byte summary represents a compression ratio of 1.9%. An automatic summarization
            evaluation tool, Rouge (Lin and Hovy, 2003), was used to evaluate the results. The system parameters were
            optimized by hand to maximize the Rouge-1 recall on a comparable training corpus, 500 AP Newswire and New
            York Times articles from the DUC-2004 single-document short summary test data. The Rouge results are shown
            in Table 2. Results show that HMM Hedge 60 scored significantly lower than most other systems and that
            Topiary scored higher than all other systems for all R1 measures. In addition, HMM Hedge Sentence scored
            significantly higher than Trimmer for the R1 measures. We also evaluated Trimmer and HMM Hedge as components
            in our Multi-Candidate Reduction framework, along with a baseline that uses the same sentence selector but
            does not use sentence compression. All three systems considered the first five sentences of each document
            and used the sentence selection algorithm presented in Section 4. The feature weights were manually
            optimized to maximize Rouge-2 recall on a comparable training corpus, 1,593 Financial Times and Los Angeles
            Times articles grouped into 50 topics from the DUC-2005 query-focused multi-document summarization 20
            R1 Recall

            R2 Recall

            Trimmer

            HMM Hedge

            0.29391
            (0.285600.30247) 0.06718 (0.063320.07111)
            0.27311
            (0.265540.28008) 0.06251 (0.058730.06620)
            No
            Compression 0.27576 (0.267720.28430) 0.06126 (0.057670.06519)
            Table 3: Rouge scores and 95% confidence intervals for 50 DUC-2006 test topics, comparing three
            MCR variants.
            MCR Score
            Higher Not Different Range Lower
            Rouge-2
            0.0805 1 23 0.0678-0.0899 11
            Rouge-SU4
            0.1360 1 24 0.1238-0.1475 10
            BE-HM
            0.0413 0 27 0.0318-0.0508 8
            Table 4: Official DUC-2006 Automatic Metrics for our MCR submission (System 32).

            test data. The systems were used to generate query-focused, 250-word summaries using the DUC-2006
            test data, described in Section 4.3. The systems were evaluated using Rouge, configured to omit stopwords
            from the calculation.7 Results are shown in Table 3. MCR using Trimmer compressions scored significantly
            higher than MCR using HMM Hedge compressions and the baseline for Rouge-1, but there was not a significant
            difference among the three systems for Rouge-2. Finally, the University of Maryland and BBN submitted a
            version of MCR to the official DUC2006 evaluation. This version used Trimmer as the source of sentence
            compressions. Results show that use of sentence compression hurt the system on human evaluation of
            grammaticality. This is not surprising, since Trimmer aims to produce compressions that are grammatical in
            Headlinese, rather than standard English. Our MCR run scored significantly lower than 23 systems on NIST’s
            human evaluation of grammaticality. However, the system did not score significantly lower than any other
            system on NIST’s human evaluation of content responsiveness. A second NIST evaluation of content
            responsiveness asked evaluators to take readability into consideration. In this evaluation, MCR scored
            significantly lower than only two systems. The evaluators recognized that Trimmer compressions are not
            grammatical in standard English; yet, the content coverage was not significantly different from the best
            automatic systems and only two systems were found to be significantly more readable. NIST computed three
            “official” automatic evaluation metrics for DUC-2006: Rouge-2, RougeSU4 and BE-HM. Table 4 shows the
            official scores of the submitted MCR system for these three metrics, along with numbers of systems that
            scored significantly higher, significantly lower, or were not significantly different from our MCR run. Also
            shown is the range of scores for the systems that were not significantly different from MCR. These results
            show that the performance of our MCR run was comparable to most other systems submitted to DUC-2006. 7 This
            is a change in the Rouge configuration from the official DUC-2006 evaluation. We note that the removal of
            non-essential stopwords (typical of Headlinese) is an important component of Trimmer-based sentence
            compression. For internal system comparisons, we configure Rouge in a way that will allow us to detect
            system differences relevant to our research focus. For reporting of official Rouge results on submitted
            systems we use the community’s accepted Rouge configurations.
            21

            The evaluation in Table 3 suggests that Trimmer sentence compression is preferable to HMM
            Hedge sentence compression for generation of English summaries of collections of document in English.
            However, HMM Hedge may prove to have value with noisier data, as we discuss in the next section.
            Nevertheless, sentence compression appears to be a valuable component of our framework for multidocument
            summarization, thus validating the ideas behind Multi-Candidate Reduction.
            6

            Applications to Different Types of Texts

            We have applied the MCR framework to summarizing different types of texts. In this section we briefly
            touch on genre-specific issues that are the subject of ongoing work. Trimmer, Topiary, and HMM Hedge were
            designed for summarization of written news. In this genre, the lead sentence is almost always the first
            non-trivial sentence of the document. More sophisticated methods for finding lead sentences did not
            outperform the baseline of simply selecting the first sentence for AP wire “hard” news stories. However,
            some types of articles, such as sports stories, opinion pieces, and movie reviews often do not have
            informative lead sentences and will require additional work in finding the best sentence for compression.
            MCR has also been applied to summarizing transcripts of broadcast news—another input form where lead
            sentences are often not informative. The conventions of broadcast news introduce categories of story-initial
            light content sentences, such as “I’m Dan Rather” or “We have an update on the story we’ve been following”.
            These present challenges for the filtering stage of our MCR framework. Such texts are additionally
            complicated by a range of problems not encountered in written news: noise introduced by automatic speech
            recognizers or other faulty transcription, issues associated with sentence boundary detection and story
            boundary detection. If word error rate is high, parser failures can prevent Trimmer from producing useful
            output. In this context, HMM Hedge becomes more attractive, since our language models are more resilient to
            noisy input. We have performed an initial evaluation of Trimmer, Topiary, and a baseline consisting of the
            first 75 characters of a document, on the task of creating 75-character headlines for broadcast news
            transcriptions (Zajic, 2007). The corpus for this task consisted of 560 broadcast news stories from ABC,
            CNN, NBC, Public Radio International, and Voice of America. We used Rouge-1 recall to evaluate the summaries
            and found that both systems scored higher than the baseline and that Topiary scored higher than Trimmer.
            However there were no significant differences among the systems. Another application of our framework is the
            summarization of email threads—collections of emails that share a common topic or were written as responses
            to each other. This task can essentially be treated as a multi-document summarization problem, albeit email
            thread structure introduces some constraints with respect to the ordering of summary sentences. Noisy data
            is inherent in this problem and pre-processing to remove quoted text, attachments, and headers is crucial.
            We have found that metadata, such as the name of the sender of each included extract help make email
            summaries easier to read. We performed an initial evaluation of HMM Hedge and Trimmer as the source of
            sentence compressions for an email thread summarization system based on the MCR framework (Zajic, 2007). The
            corpus for this task consisted of 10 manually constructed email threads from the Enron Corpus (Klimt and
            Yang, 2004). We used Rouge-1 and Rouge-2 recall with jackknifing to compare the automatic systems and the
            human summarizers. We did not observe a significant difference between the two systems, but we found that
            the task of summarizing email threads was extremely difficult for the humans (one summarizer scored
            significantly worse than the automatic systems). This application of MCR to email thread summarization is an
            initial effort. The difficulty of the task for the humans suggests that the community needs to develop a
            clearer understanding of what makes a good email thread summary and to explore practical uses for them.
            22

            Finally, Trimmer and HMM Hedge have been applied to Hindi-English cross-language summarization. In this
            case, Trimmer was applied to the output of machine translation. We adapted HMM
            Hedge to cross-lingual summarization by using the mechanism developed for morphological variation to
            represent translation probabilities from Hindi story words to English headline words. For more details, see
            Dorr et al. (2003a).
            7

            Future Work

            Future work on text summarization under the Multi-Candidate Reduction framework will focus on the
            three main components of the architecture: sentence filtering, sentence compression, and candidate
            selection. For single document summarization, the simple technique of selecting the first non-trivial
            sentence of a document for compression remains the best approach. However, for human interest stories or
            sports articles, this approach is less effective. In broadcast news transcripts, the first sentence often
            does not contain important information. Currently, filtering for multi-document summarization also relies on
            the assumption that important information tends to appear near the front of documents—the first five
            sentences of each document are retained to generate compressed candidates. An interesting area of future
            work is to explore other approaches to filtering, such as using query relevance and document centrality, to
            move beyond the baseline of selecting the first n sentences. For HMM Hedge, these methods can be used to
            determine the optimal blocks of text on which to apply the decoder. Currently, Trimmer produces multiple
            compressions by applying rules in a fixed order; the state of the compressed sentence after each rule
            application becomes a candidate. A richer pool of candidates can be produced by modifying Trimmer rules to
            operate in order-independent combinations, rather than a fixed sequence. We believe that the sentence
            selector can produce better summaries if it has larger pools of candidates to choose from. Naturally,
            different sentence compressions are not the only techniques for enriching the candidate pool—other
            possibilities include merging sentences and resolving anaphora. Topiary will also be enhanced by using
            multiple combinations of compressions and topic terms in the context of headline generation. We also plan to
            enrich the candidate selector by taking into account more features of the current summary state.
            Possibilities include sentence selector iteration count and remaining summary space, as well as feature
            weights that change during the progress of summary generation. These extensions will allow us to study the
            distribution of compressed and uncompressed sentences across sentence selector iterations. System output can
            potentially be improved by finer-grained control of this distribution. These features might also help avoid
            the current problem in which the final sentence is truncated due to length restrictions (e.g., by selecting
            a final sentence of more appropriate length). Proper setting of parameters is another important area for
            future work. Systematic optimization of parameter values in HMM Hedge and the sentence selector could lead
            to significant improvements in output quality. A logical extension to this work would be to learn the best
            parameter settings, e.g., through Expectation Maximization. At present, MCR focuses exclusively on summary
            content selection and does not take sentence ordering into consideration when constructing the summary.
            Naturally, high-quality summaries should read fluently in addition to having relevant content. Recent work
            in this area that can be applied to MCR includes includes Conroy et al. (2006), Barzilay et al. (2002),
            Okazaki et al. (2004), Lapata (2003), and Dorr and Gaasterland (this special issue 2007). Within the MCR
            architecture, fluency considerations can be balanced with other important factors such as relevance and
            anti-redundancy through appropriate feature weighting.
            23

            8
        </corps>
        <conclusion>Conclusion

            This work presents Multi-Candidate Reduction, a general architecture for multi-document summarization. The
            framework integrates successful single-document compression techniques that we have
            previously developed. MCR is motivated by the insight that multiple candidate compressions of source
            sentences should be made available to subsequent processing modules, which may have access to more
            information for summary construction. This is implemented in a dynamic feature-based sentence selector that
            iteratively builds a summary from compressed variants. Evaluations show that sentence compression plays an
            important role in multi-document summarization and that our MCR framework is both flexible and extensible.
            Acknowledgments
            This work has been supported, in part, under the GALE program of the Defense Advanced Research Projects
            Agency, Contract No. HR0011-06-2-0001, the TIDES program of the Defense Advanced Research Projects Agency,
            BBNT Contract No. 9500006806, and the University of Maryland Joint Institute for Knowledge Discovery. Any
            opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do
            not necessarily reflect the views of DARPA. The first author would like to thank Naomi for proofreading,
            support, and encouragement. The second author would like to thank Steve, Carissa, and Ryan for their energy
            enablement. The third author would like to thank Esther and Kiri for their kind support.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>L. Bahl, F. Jelinek, and R. Mercer. 1983. A maximum likelihood approach to continuous speech
            recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190. M. Banko, V.
            Mittal, and M. Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the
            38th Annual Meeting of the Association for Computational Linguistics (ACL 2000), pages 318–325, Hong Kong.
            R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multidocument
            news summarization. Journal of Artificial Intelligence Research, 17:35–55. L. Baum. 1972. An inequality and
            associated maximization technique in statistical estimation of probabilistic functions of a Markov process.
            Inequalities, 3:1–8. S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz. 2003. Using knowledge-poor
            coreference resolution for text summarization. In Proceedings of the HLT-NAACL 2003 Text Summarization
            Workshop and Document Understanding Conference (DUC 2003), pages 85–92, Edmonton, Alberta. D. Bikel, R.
            Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning,
            34(1/3):211–231. S. Blair-Goldensohn, D. Evans, V. Hatzivassiloglou, K. McKeown, A. Nenkova, R. Passonneau,
            B. Schiffman, A. Schlaikjer, A. Siddharthan, and S. Siegelman. 2004. Columbia University at DUC 2004. In
            Proceedings of the 2004 Document Understanding Conference (DUC 2004) at HLT/NAACL 2004, pages 23–30, Boston,
            Massachusetts.
            24

            P. Brown, J. Cocke, S. Pietra, V. Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin. 1990. A
            statistical approach to machine translation. Computational Linguistics, 16(2):79–85. J. Carbonell and J.
            Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries.
            In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in
            Information Retrieval (SIGIR 1998), pages 335–336, Melbourne, Australia. Eugene Charniak. 2000. A
            Maximum-Entropy-Inspired Parser. In Proceedings of the First Meeting of the North American Chapter of the
            Association for Computational Linguistics (NAACL 2000), pages 132–139, Seattle, Washington. J. Clarke and M.
            Lapata. 2006. Models for sentence compression: A comparison across domains, training requirements and
            evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and
            44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL 2006), pages 377–384,
            Sydney, Australia. J. Conroy, J. Schlesinger, and J. Goldstein. 2005. CLASSY query-based multi-document
            summarization. In Proceedings of the 2005 Document Understanding Conference (DUC-2005) at NLT/EMNLP 2005,
            Vancouver, Canada. J. Conroy, J. Schlesinger, D. O’Leary, and J. Goldstein. 2006. Back to basics: CLASSY
            2006. In Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York,
            New York. D. Cutting, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proceedings of
            the Third Conference on Applied Natural Language Processing, Trento, Italy. Hoa Dang and Donna Harman. 2006.
            Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006. B. Dorr and T.
            Gaasterland. this special issue, 2007. Exploiting aspectual features and connecting words for
            summarization-inspired temporal-relation extraction. Information Processing and Management. B. Dorr, D.
            Zajic, and R. Schwartz. 2003a. Cross-language headline generation for Hindi. ACM Transactions on Asian
            Language Information Processing (TALIP), 2(3):270–289. B. Dorr, D. Zajic, and R. Schwartz. 2003b. Hedge
            Trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL 2003 Text
            Summarization Workshop and Document Understanding Conference (DUC 2003), pages 1–8, Edmonton, Alberta. T.
            Dunning. 1994. Statistical identification of language. Technical Report MCCS 94-273, New Mexico State
            University. T. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of
            13th International Workshop on Database and Expert Systems Applications (DEXA 2002), pages 215– 222,
            Aix-en-Provence, France. J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. 2000. Multi-document
            summarization by sentence extraction. In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summarization,
            pages 40–48. D. Harman and M. Liberman. 1993. TIPSTER Complete. Linguistic Data Consortium (LDC),
            Philadelphia. 25
            H. Jing and K. McKeown. 2000. Cut and paste based text summarization. In Proceedings of the First
            Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2000), pages
            178–185, Seattle, Washington. B. Klimt and Y. Yang. 2004. Introducing the Enron Corpus. In Proceedings of
            the First Conference on Email and Anti-Spam (CEAS), Mountain View, California. K. Knight and D. Marcu. 2000.
            Statistics-based summarization—step one: Sentence compression. In Proceedings of the Seventeenth National
            Conference on Artificial Intelligence (AAAI-2000), Austin, Texas. K. Knight and D. Marcu. 2002.
            Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial
            Intelligence, 139(1):91–107. M. Lapata. 2003. Probabilistic text structuring: Experiments with sentence
            ordering. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL
            2004), pages 545–552, Barcelona, Spain. David Dolan Lewis. 1999. An evaluation of phrasal and clustered
            representations on a text categorization task. In Proceedings of the 15th Annual International ACM SIGIR
            Conference on Research and Development in Information Retrieval (SIGIR 1992), pages 37–50, Copenhagen,
            Denmark. C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence
            statistics. In Proceedings of the 2003 Human Language Technology Conference and the North American Chapter
            of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2003), pages 71–78, Edmonton,
            Alberta. I. Mårdh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo. E. Mays, F.
            Damerau, and R. Mercer. 1990. Context-based spelling correction. In Proceedings of IBM Natural Language ITL,
            pages 517–522, Paris, France. S. Miller, L. Ramshaw, H. Fox, and R. Weischedel. 2000. A novel use of
            statistical parsing to extract information from text. In Proceedings of the First Meeting of the North
            American Chapter of the Association for Computational Linguistics (NAACL 2000), pages 226–233, Seattle,
            Washington. S. Muresan, E. Tzoukermann, and J. Klavans. 2001. Combining linguistic and machine learning
            techniques for email. In Proceedings of the ACL/EACL 2001 Workshop on Computational Natural Language
            Learning (ConLL), pages 290–297, Toulouse, France. N. Okazaki, Y. Matsuo, and M. Ishizuka. 2004. Improving
            chronological sentence ordering by precedence relation. In Proceedings of the 20th International Conference
            on Computational Linguistics (COLING 2004), pages 750–756, Geneva, Switzerland. M. Porter. 1980. An
            algorithm for suffix stripping. Program, 14(3):130–137. D. Radev, T. Allison, S. Blair-Goldensohn, J.
            Blitzer, A. Çelebi, S. Dimitrov, E. Drabek, A. Hakim, W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion, S.
            Teufel, M. Topper, A. Winkel, and Z. Zhang. 2004. MEAD—a platform for multidocument multilingual text
            summarization. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC
            2004), Lisbon, Portugal.
            26

            R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood model
            for topic classification of broadcast news. In Proceedings of the Fifth European Speech Communication
            Association Conference on Speech Communication and Technology (Eurospeech-97), Rhodes, Greece. S. Sista, R.
            Schwartz, T. Leek, and J. Makhoul. 2002. An algorithm for unsupervised topic discovery from broadcast news
            stories. In Proceedings of the 2002 Human Language Technology Conference (HLT), pages 99–103, San Diego,
            California. J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression.
            In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages
            290–297, Ann Arbor, Michigan. L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft Research at
            DUC2006: Task-focused summarization with sentence simplification and lexical expansion. In Proceedings of
            the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York, New York. A. Viterbi.
            1967. Error bounds for convolution codes and an asymptotically optimal decoding algorithm. IEEE Transactions
            on Information Theory, 13:260–269. R. Wang, N. Stokes, W. Doran, E. Newman, J. Carthy, and J. Dunnion. 2005.
            Comparing Topiarystyle approaches to headline generation. In Lecture Notes in Computer Science: Advances in
            Information Retrieval: 27th European Conference on IR Research (ECIR 2005), volume 3408, Santiago de
            Compostela, Spain. Springer Berlin / Heidelberg. D. Zajic, B. Dorr, and R. Schwartz. 2004. BBN/UMD at
            DUC-2004: Topiary. In Proceedings of the 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL
            2004, pages 112–119, Boston, Massachusetts. D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2005a. UMD/BBN at
            MSE2005. In Proceedings of the MSE2005 Track of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation
            Measures for MT and/or Summarization, Ann Arbor, Michigan. D. Zajic, B. Dorr, R. Schwartz, C. Monz, and J.
            Lin. 2005b. A sentence-trimming approach to multi-document summarization. In Proceedings of the 2005
            Document Understanding Conference (DUC-2005) at NLT/EMNLP 2005, pages 151–158, Vancouver, Canada. D. Zajic.
            2007. Multiple Alternative Sentence Compressions (MASC) as a Tool for Automatic Summarization Tasks. Ph.D.
            thesis, University of Maryland, College Park. L. Zhou and E. Hovy. 2003. Headline summarization at ISI. In
            Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and Document Understanding Conference (DUC
            2003), pages 174–178, Edmonton, Alberta.
            27
        </biblio>
    </article>
    <article>
        <preamble>compression_phrases_Prog-Linear-jair.txt</preamble>
        <titre>Global Inference for Sentence Compression An Integer Linear Programming Approach James Clarke</titre>
        <auteur>jclarke@ed.ac.uk</auteur>
        <abstract>Sentence compression holds promise for many applications ranging from summarization to subtitle
            generation. Our work views sentence compression as an optimization problem and uses integer linear
            programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated
            constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend
            these models with novel global constraints. Experimental results on written and spoken texts demonstrate
            improvements over state-of-the-art models.
        </abstract>
        <introduction>The computational treatment of sentence compression has recently attracted much attention in the
            literature. The task can be viewed as producing a summary of a single sentence that retains the most
            important information and remains grammatical (Jing, 2000). A sentence compression mechanism would greatly
            benefit a wide range of applications. For example, in summarization, it could improve the conciseness of the
            generated summaries (Jing, 2000; Lin, 2003; Zajic, Door, Lin, & Schwartz, 2007). Other examples include
            compressing text to be displayed on small screens such as mobile phones or PDAs (Corston-Oliver, 2001),
            subtitle generation from spoken transcripts (Vandeghinste & Pan, 2004), and producing audio scanning devices
            for the blind (Grefenstette, 1998). Sentence compression is commonly expressed as a word deletion problem:
            given an input source sentence of words x = x1 , x2 , . . . , xn , the aim is to produce a target
            compression by removing any subset of these words (Knight & Marcu, 2002). The compression problem has been
            extensively studied across different modeling paradigms, both supervised and unsupervised. Supervised models
            are typically trained on a parallel corpus of source sentences and target compressions and come in many
            flavors. Generative models aim to model the probability of a target compression given the source sentence
            either directly (Galley & McKeown, 2007) or indirectly using the noisy-channel model (Knight & Marcu, 2002;
            Turner & Charniak, 2005), whereas discriminative formulations attempt to minimize error rate on a training
            set. These include decision-tree learning (Knight & Marcu, 2002), maximum entropy (Riezler, King, Crouch, &
            Zaenen, 2003), support vector machines (Nguyen, Shimazu, Horiguchi, Ho, & Fukushi, 2004), and large-margin
            learning (McDonald, 2006). c 2008 AI Access Foundation. All rights reserved. Clarke & Lapata Unsupervised
            methods dispense with the parallel corpus and generate compressions either using rules (Turner & Charniak,
            2005) or a language model (Hori & Furui, 2004). Despite differences in formulation, all these approaches
            model the compression process using local information. For instance, in order to decide which words to drop,
            they exploit information about adjacent words or constituents. Local models can do a good job at producing
            grammatical compressions, however they are somewhat limited in scope since they cannot incorporate global
            constraints on the compression output. Such constraints consider the sentence as a whole instead of isolated
            linguistic units (words or constituents). To give a concrete example we may want to ensure that each target
            compression has a verb, provided that the source had one in the first place. Or that verbal arguments are
            present in the compression. Or that pronouns are retained. Such constraints are fairly intuitive and can be
            used to instill not only linguistic but also task specific information into the model. For instance, an
            application which compresses text to be displayed on small screens would presumably have a higher
            compression rate than a system generating subtitles from spoken text. A global constraint could force the
            former system to generate compressions with a fixed rate or a fixed number of words. Existing approaches do
            not model global properties of the compression problem for a good reason. Finding the best compression for a
            source sentence given the space of all possible compressions1 (this search process is often referred to as
            decoding or inference) can become intractable for too many constraints and overly long sentences. Typically,
            the decoding problem is solved efficiently using dynamic programming often in conjunction with heuristics
            that reduce the search space (e.g., Turner & Charniak, 2005). Dynamic programming guarantees we will find
            the global optimum provided the principle of optimality holds. This principle states that given the current
            state, the optimal decision for each of the remaining stages does not depend on previously reached stages or
            previously made decisions (Winston & Venkataramanan, 2003). However, we know this to be false in the case of
            sentence compression. For example, if we have included modifiers to the left of a noun in a compression then
            we should probably include the noun too or if we include a verb we should also include its arguments. With a
            dynamic programming approach we cannot easily guarantee such constraints hold. In this paper we propose a
            novel framework for sentence compression that incorporates constraints on the compression output and allows
            us to find an optimal solution. Our formulation uses integer linear programming (ILP), a general-purpose
            exact framework for NP-hard problems. Specifically, we show how previously proposed models can be recast as
            integer linear programs. We extend these models with constraints which we express as linear inequalities.
            Decoding in this framework amounts to finding the best solution given a linear (scoring) function and a set
            of linear constraints that can be either global or local. Although ILP has been previously used for sequence
            labeling tasks (Roth & Yih, 2004; Punyakanok, Roth, Yih, & Zimak, 2004), its application to natural language
            generation is less widespread. We present three compression models within the ILP framework, each
            representative of an unsupervised (Knight & Marcu, 2002), semi-supervised (Hori & Furui, 2004), and fully
            supervised modeling approach (McDonald, 2006). We propose a small number of constraints ensuring that the
            compressions are structurally and semantically 1. There are 2n possible compressions where n is the number
            of words in a sentence.
        </introduction>
        <corps>using rules (Turner & Charniak, 2005) or a language model (Hori & Furui, 2004). Despite differences in
            formulation, all these approaches model the compression process using local information. For instance, in
            order to decide which words to drop, they exploit information about adjacent words or constituents. Local
            models can do a good job at producing grammatical compressions, however they are somewhat limited in scope
            since they cannot incorporate global constraints on the compression output. Such constraints consider the
            sentence as a whole instead of isolated linguistic units (words or constituents). To give a concrete example
            we may want to ensure that each target compression has a verb, provided that the source had one in the first
            place. Or that verbal arguments are present in the compression. Or that pronouns are retained. Such
            constraints are fairly intuitive and can be used to instill not only linguistic but also task specific
            information into the model. For instance, an application which compresses text to be displayed on small
            screens would presumably have a higher compression rate than a system generating subtitles from spoken text.
            A global constraint could force the former system to generate compressions with a fixed rate or a fixed
            number of words. Existing approaches do not model global properties of the compression problem for a good
            reason. Finding the best compression for a source sentence given the space of all possible compressions1
            (this search process is often referred to as decoding or inference) can become intractable for too many
            constraints and overly long sentences. Typically, the decoding problem is solved efficiently using dynamic
            programming often in conjunction with heuristics that reduce the search space (e.g., Turner & Charniak,
            2005). Dynamic programming guarantees we will find the global optimum provided the principle of optimality
            holds. This principle states that given the current state, the optimal decision for each of the remaining
            stages does not depend on previously reached stages or previously made decisions (Winston & Venkataramanan,
            2003). However, we know this to be false in the case of sentence compression. For example, if we have
            included modifiers to the left of a noun in a compression then we should probably include the noun too or if
            we include a verb we should also include its arguments. With a dynamic programming approach we cannot easily
            guarantee such constraints hold. In this paper we propose a novel framework for sentence compression that
            incorporates constraints on the compression output and allows us to find an optimal solution. Our
            formulation uses integer linear programming (ILP), a general-purpose exact framework for NP-hard problems.
            Specifically, we show how previously proposed models can be recast as integer linear programs. We extend
            these models with constraints which we express as linear inequalities. Decoding in this framework amounts to
            finding the best solution given a linear (scoring) function and a set of linear constraints that can be
            either global or local. Although ILP has been previously used for sequence labeling tasks (Roth & Yih, 2004;
            Punyakanok, Roth, Yih, & Zimak, 2004), its application to natural language generation is less widespread. We
            present three compression models within the ILP framework, each representative of an unsupervised (Knight &
            Marcu, 2002), semi-supervised (Hori & Furui, 2004), and fully supervised modeling approach (McDonald, 2006).
            We propose a small number of constraints ensuring that the compressions are structurally and semantically 1.
            There are 2n possible compressions where n is the number of words in a sentence.
            400

            Global Inference for Sentence Compression

            valid and experimentally evaluate their impact on the compression task. In all cases, we
            show that the added constraints yield performance improvements. The remainder of this paper is organized as
            follows. Section 2 provides an overview of related work. In Section 3 we present the ILP framework and the
            compression models we employ in our experiments. Our constraints are introduced in Section 3.5. Section 4.3
            discusses our experimental set-up and Section 5 presents our results. Discussion of future work concludes
            the paper.
            2. Related Work
            In this paper we develop several ILP-based compression models. Before presenting these models, we briefly
            summarize previous work addressing sentence compression with an emphasis on data-driven approaches. Next, we
            describe how ILP techniques have been used in the past to solve other inference problems in natural language
            processing (NLP). 2.1 Sentence Compression Jing (2000) was perhaps the first to tackle the sentence
            compression problem. Her approach uses multiple knowledge sources to determine which phrases in a sentence
            to remove. Central to her system is a grammar checking module that specifies which sentential constituents
            are grammatically obligatory and should therefore be present in the compression. This is achieved using
            simple rules and a large-scale lexicon. Other knowledge sources include WordNet and corpus evidence gathered
            from a parallel corpus of source-target sentence pairs. A phrase is removed only if it is not grammatically
            obligatory, not the focus of the local context and has a reasonable deletion probability (estimated from a
            parallel corpus). In contrast to Jing (2000), the bulk of the research on sentence compression relies
            exclusively on corpus data for modeling the compression process without recourse to extensive knowledge
            sources (e.g., WordNet). A large number of approaches are based on the noisy-channel model (Knight & Marcu,
            2002). These approaches consist of a language model P (y) (whose role is to guarantee that compression
            output is grammatical), a channel model P (x|y) (capturing the probability that the source sentence x is an
            expansion of the target compression y), and a decoder (which searches for the compression y that maximizes P
            (y)P (x|y)). The channel model is acquired from a parsed version of a parallel corpus; it is essentially a
            stochastic synchronous context-free grammar (Aho & Ullman, 1969) whose rule probabilities are estimated
            using maximum likelihood. Modifications of this model are presented by Turner and Charniak (2005) and Galley
            and McKeown (2007) with improved results. In discriminative models (Knight & Marcu, 2002; Riezler et al.,
            2003; McDonald, 2006; Nguyen et al., 2004) sentences are represented by a rich feature space (also induced
            from parse trees) and the goal is to learn which words or word spans should be deleted in a given context.
            For instance, in Knight and Marcu’s (2002) decision-tree model, compression is performed deterministically
            through a tree rewriting process inspired by the shift-reduce parsing paradigm. Nguyen et al. (2004) render
            this model probabilistic through the use of support vector machines. McDonald (2006) formalizes sentence
            compression in a largemargin learning framework without making reference to shift-reduce parsing. In his
            model compression is a classification task: pairs of words from the source sentence are classified 401
            Clarke & Lapata

            as being adjacent or not in the target compression. A large number of features are defined
            over words, parts-of-speech, phrase structure trees and dependencies. These features are gathered over
            adjacent words in the compression and the words in-between which were dropped (see Section 3.4.3 for a more
            detailed account). While most compression models have been developed with written text in mind, Hori and
            Furui (2004) propose a model for automatically transcribed spoken text. Their model generates compressions
            through word deletion without using parallel data or syntactic information in any way. Assuming a fixed
            compression rate, it searches for the compression with the highest score using a dynamic programming
            algorithm. The scoring function consists of a language model responsible for producing grammatical output, a
            significance score indicating whether a word is topical or not, and a score representing the speech
            recognizer’s confidence in transcribing a given word correctly. 2.2 Integer Linear Programming in NLP ILPs
            are constrained optimization problems where both the objective function and the constraints are linear
            equations with integer variables (see Section 3.1 for more details). ILP techniques have been recently
            applied to several NLP tasks, including relation extraction (Roth & Yih, 2004), semantic role labeling
            (Punyakanok et al., 2004), the generation of route directions (Marciniak & Strube, 2005), temporal link
            analysis (Bramsen, Deshpande, Lee, & Barzilay, 2006), set partitioning (Barzilay & Lapata, 2006), syntactic
            parsing (Riedel & Clarke, 2006), and coreference resolution (Denis & Baldridge, 2007). Most of these
            approaches combine a local classifier with an inference procedure based on ILP. The classifier proposes
            possible answers which are assessed in the presence of global constraints. ILP is used to make a final
            decision that is consistent with the constraints and likely according to the classifier. For example, the
            semantic role labeling task involves identifying the verb-argument structure for a given sentence.
            Punyakanok et al. (2004) first use SNOW, a multi-class classifier2 (Roth, 1998), to identify and label
            candidate arguments. They observe that the labels assigned to arguments in a sentence often contradict each
            other. To resolve these conflicts they propose global constraints (e.g., each argument should be
            instantiated once for a given verb, every verb should have at least one argument) and use ILP to reclassify
            the output of SNOW. Dras (1999) develops a document paraphrasing model using ILP. The key premise of his
            work is that in some cases one may want to rewrite a document so as to conform to some global constraints
            such as length, readability, or style. The proposed model has three ingredients: a set of sentence-level
            paraphrases for rewriting the text, a set of global constraints, and an objective function which quantifies
            the effect incurred by the paraphrases. Under this formulation, ILP can be used to select which paraphrases
            to apply so that the global constraints are satisfied. Paraphrase generation falls outside the scope of the
            ILP model – sentence rewrite operations are mainly syntactic and provided by a module based on synchronous
            tree adjoining grammar (S-TAG, Shieber & Schabes, 1990). Unfortunately, only a proof-of-concept is
            presented; implementation and evaluation of this module are left to future work. 2. SNOW’s learning
            algorithm is a variation of the Winnow update rule.
            402

            Global Inference for Sentence Compression

            Our work models sentence compression as an optimization problem. We show how previously proposed models can
            be reformulated in the context of integer linear programming
            which allows us to easily incorporate constraints during the decoding process. Our constraints are
            linguistically and semantically motivated and are designed to bring less local syntactic knowledge into the
            model and help preserve the meaning of the source sentence. Previous work has identified several important
            features for the compression task (Knight & Marcu, 2002; McDonald, 2006); however, the use of global
            constraints is novel to our knowledge. Although sentence compression has not been explicitly formulated in
            terms of optimization, previous approaches rely on some optimization procedure for generating the best
            compression. The decoding process in the noisy-channel model searches for the best compression given the
            source and channel models. However, the compression found is usually sub-optimal as heuristics are used to
            reduce the search space or is only locally optimal due to the search method employed. For example, in the
            work of Turner and Charniak (2005) the decoder first searches for the best combination of rules to apply. As
            it traverses the list of compression rules, it removes sentences outside the 100 best compressions
            (according to the channel model). This list is eventually truncated to 25 compressions. In other models
            (Hori & Furui, 2004; McDonald, 2006) the compression score is maximized using dynamic programming which
            however can yield suboptimal results (see the discussion in Section 1). Contrary to most other NLP work
            using ILP (a notable exception is Roth & Yih, 2005), we do not view compression generation as a two stage
            process where learning and inference are carried out sequentially (i.e., first a local classifier
            hypothesizes a list of possible answers and then the best answer is selected using global constraints). Our
            models integrate learning with inference in a unified framework where decoding takes place in the presence
            of all available constraints, both local and global. Moreover, we investigate the influence of our
            constraint set across models and learning paradigms. Previous work typically formulates constraints for a
            single model (e.g., the SNOW classifier) and learning paradigm (e.g., supervised). We therefore assess how
            the constraint-based framework advocated in this article influences the performance of expressive models
            (which require large amounts of parallel data) and non-expressive ones (which use very little parallel data
            or none at all). In other words, we are able to pose and answer the following question: what kinds of models
            benefit most from constraint-based inference? Our work is close in spirit but rather different in content to
            Dras (1999). We concentrate on compression, a specific paraphrase type, and apply our models on the
            sentence-level. Our constraints thus do not affect the document as a whole but individual sentences.
            Furthermore, compression generation is an integral part of our ILP models, whereas Dras assumes that
            paraphrases are generated by a separate process.
            3. Framework
            In this section we present the details of the proposed framework for sentence compression. As mentioned
            earlier, our work models sentence compression directly as an optimization problem. There are 2n possible
            compressions for each source sentence and while many of these will be unreasonable, it is unlikely that only
            one compression will be satisfactory (Knight & Marcu, 2002). Ideally, we require a function that captures
            the operations 403
            Clarke & Lapata

            (or rules) that can be performed on a sentence to create a compression while at the same
            time factoring how desirable each operation makes the resulting compression. We can then perform a search
            over all possible compressions and select the best one, as determined by how desirable it is. A wide range
            of models can be expressed under this framework. The prerequisites for implementing these are fairly low, we
            only require that the decoding process be expressed as a linear function with a set of linear constraints.
            In practice, many models rely on a Markov assumption for factorization which is usually solved with a
            dynamic programming-based decoding process. Such algorithms can be formulated as integer linear programs
            with little effort. We first give a brief introduction into integer linear programming, an extension of
            linear programming for readers unfamiliar with mathematical programming. Our compression models are next
            described in Section 3.4 and constraints in Section 3.5. 3.1 Linear Programming Linear programming (LP)
            problems are optimization problems with constraints. They consist of three parts: • Decision variables.
            These are variables under our control which we wish to assign optimal values to. • A linear function (the
            objective function). This is the function we wish to minimize or maximize. This function is influences by
            the values assigned to the decision variables. • Constraints. Most problems will only allow the decision
            variables to take certain values. These restrictions are the constraints. These terms are best demonstrated
            with a simple example taken from Winston and Venkataramanan (2003). Imagine a manufacturer of tables and
            chairs which we shall call the Telfa Corporation. To produce a table, 1 hour of labor and 9 square board
            feet of wood is required. Chairs require 1 hour of labor and 5 square board feet of wood. Telfa have 6 hours
            of labor and 45 square board feet of wood available. The profit made from each table is 8 GBP and 5 GBP for
            chairs. We wish to determine the number of tables and chairs that should be manufactured to maximize Telfa’s
            profit. First, we must determine the decision variables. In our case we define: x1 = number of tables
            manufactured x2 = number of chairs manufactured Our objective function is the value we wish to maximize,
            namely the profit. Profit = 8x1 + 5x2 There are two constraints in this problem: we must not exceed 6 hours
            of labor and no more than 45 square board feet of wood must be used. Also, we cannot create a negative
            amount of chairs or tables: 404
            Global Inference for Sentence Compression

            Labor constraint
            x 1 + x2 Wood constraint 9x1 + 5x2 Variable constraints x1 x2
            ≤ 6
            ≤ 45 ≥ 0 ≥ 0
            Once the decision variables, objective function and constraints have been determined we
            can express the LP model: max z = 8x1 + 5x2 (Objective function) subject to (s.t.) x1 + x2 9x1 + 5x2 x1 x2
            ≤ 6 (Labor constraint)
            ≤ 45 (Wood constraint) ≥ 0 ≥ 0
            Two of the most basic concepts involved in solving LP problems are the feasibility region
            and optimal solution. The optimal solution is one in which all constraints are satisfied and the objective
            function is minimized or maximized. A specification of the value for each decision variable is referred to
            as a point. The feasibility region for a LP is a region consisting of the set of all points that satisfy all
            the LP’s constraints. The optimal solution lies within this feasibility region, it is the point with the
            minimum or maximum objective function value. A set of points satisfying a single linear inequality is a
            half-space. The feasibility region is defined by a the intersection of m half-spaces (for m linear
            inequalities) and forms a polyhedron. Our Telfa example forms a polyhedral set (a polyhedral convex set)
            from the intersection of our four constraints. Figure 1a shows the feasible region for the Telfa example. To
            find the optimal solution we graph a line (or hyperplane) on which all points have the same objective
            function value. In maximization problems it is called the isoprofit line and in minimization problems the
            isocost line. One isoprofit line is represented by the dashed black line in Figure 1a. Once we have one
            isoprofit line we can find all other isoprofit lines by moving parallel to the original isoprofit line. The
            extreme points of the polyhedral set are defined as the intersections of the lines that form the boundaries
            of the polyhedral set (points A B C and D in Figure 1a). It can be shown that any LP that has an optimal
            solution, has an extreme point that is globally optimal. This reduces the search space of the optimization
            problem to finding the extreme point with the highest or lowest value. The simplex algorithm (Dantzig, 1963)
            solves LPs by exploring the extreme points of a polyhedral set. Specifically, it moves from one extreme
            point to an adjacent extreme point (extreme points that lie on the same line segment) until an optimal
            extreme point is found. Although the simplex algorithm has an exponential worst-case complexity, in practice
            the algorithm is very efficient. 15 9 The optimal solution for the Telfa example is z = 165 4 , x1 = 4 , x2
            = 4 . Thus, to achieve a maximum profit of 41.25 GBP they must build 3.75 tables and 2.25 chairs. This is
            obviously impossible as we would not expect people to buy fractions of tables and chairs. Here, we want to
            be able to constrain the problem such that the decision variables can only take integer values. This can be
            done with Integer Linear Programming. 405
            Clarke & Lapata

            a.

            b.

            10

            10

            9

            9

            = LP’s feasible region

            9x1 + 5x2 = 45

            9x1+ 5x2 = 45

            8

            8

            7

            7

            6 B

            6

            x2 5

            x2 5

            4

            4

            = IP feasible point
            = IP relaxation’s feasible region
            3

            3

            Optimal LP solution

            Optimal LP solution
            2
            C

            2

            x 1 + x2 = 6

            1
            0
            A

            0

            1

            2

            3

            x1

            4

            D
            5
            6

            x 1 + x2 = 6

            11

            7

            0

            0

            1

            2

            3

            x1

            4

            5

            6

            7

            Figure 1: Feasible region for the Telfa example using linear (graph (a)) and integer linear
            (graph (b)) programming
            3.2 Integer Linear Programming
            Integer linear programming (ILP) problems are LP problems in which some or all of the variables are required
            to be non-negative integers. They are formulated in a similar manner to LP problems with the added
            constraint that all decision variables must take non-negative integer values. To formulate the Telfa problem
            as an ILP model we merely add the constraints that x1 and x2 must be integer. This gives: max z = 8x1 + 5x2
            (Objective function) subject to (s.t.) x1 + x2 9x1 + 5x2 x1 x2
            ≤
            6 (Labor constraint) ≤ 45 (Wood constraint) ≥ 0; x1 integer ≥ 0; x2 integer
            For LP models, it can be proved that the optimal solution lies on an extreme point of
            the feasible region. In the case of integer linear programs, we only wish to consider points that are
            integer values. This is illustrated in Figure 1b for the Telfa problem. In contrast to linear programming,
            which can be solved efficiently in the worst case, integer programming problems are in many practical
            situations NP-hard (Cormen, Leiserson, & Rivest, 1992). 406
            Global Inference for Sentence Compression

            Fortunately, ILPs are a well studied optimization problem and a number of techniques have
            been developed to find the optimal solution. Two such techniques are the cutting planes method (Gomory,
            1960) and the branch-and-bound method (Land & Doig, 1960). We briefly discuss these methods here. For a more
            detailed treatment we refer the interested reader to Winston and Venkataramanan (2003) or Nemhauser and
            Wolsey (1988). The cutting planes method adds extra constraints to slice parts of the feasible region until
            it contains only integer extreme points. However, this process can be difficult or impossible (Nemhauser &
            Wolsey, 1988). The branch-and-bound method enumerates all points in the ILP’s feasible region but prunes
            those sections in the region which are known to be sub-optimal. It does this by relaxing the integer
            constraints and solving the resulting LP problem (known as the LP relaxation). If the solution of the LP
            relaxation is integral, then it is the optimal solution. Otherwise, the resulting solution provides an upper
            bound on the solution for the ILP. The algorithm proceeds by creating two new sub-problems based on the
            non-integer solution for one variable at a time. These are solved and the process repeats until the optimal
            integer solution is found. Using the branch-and-bound method, we find that the optimal solution to the Telfa
            problem is z = 40, x1 = 5, x2 = 0; thus, to achieve a maximum profit of 40 GBP, Telfa must manufacture 5
            tables and 0 chairs. This is a relatively simple problem, which could be solved merely by inspection. Most
            ILP problems will involve many variables and constraints resulting in a feasible region with a large number
            of integer points. The branch-and-bound procedure can efficiently solve such ILPs in a matter of seconds and
            forms part of many commercial ILP solvers. In our experiments we use lp solve 3 , a free optimization
            package which relies on the simplex algorithm and brand-and-bound methods for solving ILPs. Note that under
            special circumstances other solving methods may be applicable. For example, implicit enumeration can be used
            to solve ILPs where all the variables are binary (also known as pure 0−1 problems). Implicit enumeration is
            similar to the branch-andbound method, it systematically evaluates all possible solutions, without however
            explicitly solving a (potentially) large number of LPs derived from the relaxation. This removes much of the
            computational complexity involved in determining if a sub-problem is infeasible. Furthermore, for a class of
            ILP problems known as minimum cost network flow problems (MCNFP), the LP relaxation always yields an
            integral solution. These problems can therefore be treated as LP problems. In general, a model will yield an
            optimal solution in which all variables are integers if the constraint matrix has a property known as total
            unimodularity. A matrix A is totally unimodular if every square sub-matrix of A has its determinant equal to
            0, +1 or −1. It is the case that the more the constraint matrix looks totally unimodular, the easier the
            problem will be to solve by branch-and-bound methods. In practice it is good to formulate ILPs where as many
            variables as possible have coefficients of 0, +1 or −1 in the constraints (Winston & Venkataramanan, 2003).
            3.3 Constraints and Logical Conditions Although integer variables in ILP problems may take arbitrary values,
            these are frequently are restricted to 0 and 1. Binary variables (0−1 variables) are particularly useful for
            rep3. The software is available from http://lpsolve.sourceforge.net/.
            407

            Clarke & Lapata

            Condition
            Implication Iff Or Xor And Not
            Statement
            if a then b a if and only if b a or b or c a xor b xor c a and b not a
            Constraint
            b−a≥0 a−b=0 a+b+c≥1 a+b+c=1 a = 1; b = 1 1−a=1
            Table 1: How to represent logical conditions using binary variables and constraints in ILP.

            resenting a variety of logical conditions within the ILP framework through the use of constraints. Table 1
            lists several logical conditions and their equivalent constraints.
            We can also express transitivity, i.e., “c if and only if a and b”. Although it is often thought that
            transitivity can only be expressed as a polynomial expression of binary variables (i.e., ab = c), it is
            possible to replace the latter by the following linear inequalities (Williams, 1999):
            (1 − c) + a ≥ 1
            (1 − c) + b ≥ 1 c + (1 − a) + (1 − b) ≥ 1 This can be easily extended to model indicator variables
            representing whether a set of binary variables can take certain values. 3.4 Compression Models In this
            section we describe three compression models which we reformulate as integer linear programs. Our first
            model is a simple language model which has been used as a baseline in previous research (Knight & Marcu,
            2002). Our second model is based on the work of Hori and Furui (2004); it combines a language model with a
            corpus-based significance scoring function (we omit here the confidence score derived from the speech
            recognizer since our models are applied to text only). This model requires a small amount of parallel data
            to learn weights for the language model and the significance score. Our third model is fully supervised, it
            uses a discriminative large-margin framework (McDonald, 2006), and is trained trained on a larger parallel
            corpus. We chose this model instead of the more popular noisy-channel or decision-tree models, for two
            reasons, a practical one and a theoretical one. First, McDonald’s (2006) model delivers performance superior
            to the decision-tree model (which in turn performs comparably to the noisy-channel). Second, the noisy
            channel is not an entirely appropriate model for sentence compression. It uses a language model trained on
            uncompressed sentences even though it represents the probability of compressed sentences. As a result, the
            model will consider compressed sentences less likely than uncompressed ones (a further discussion is
            provided by Turner & Charniak, 2005). 408
            Global Inference for Sentence Compression

            3.4.1 Language Model
            A language model is perhaps the simplest model that springs to mind. It does not require a parallel corpus
            (although a relatively large monolingual corpus is necessary for training), and will naturally prefer short
            sentences to longer ones. Furthermore, a language model can be used to drop words that are either infrequent
            or unseen in the training corpus. Knight and Marcu (2002) use a bigram language model as a baseline against
            their noisy-channel and decision-tree models. Let x = x1 , x2 , . . . , xn denote a source sentence for
            which we wish to generate a target compression. We introduce a decision variable for each word in the source
            and constrain it to be binary; a value of 0 represents a word being dropped, whereas a value of 1 includes
            the word in the target compression. Let: δi =
            (

            1 if xi is in the compression
            ∀i ∈ [1 . . . n] 0 otherwise
            If we were using a unigram language model, our objective function would maximize the
            overall sum of the decision variables (i.e., words) multiplied by their unigram probabilities (all
            probabilities throughout this paper are log-transformed): max
            n
            X
            δi · P (xi )

            (1)

            i=1

            Thus, if a word is selected, its corresponding δi is given a value of 1, and its probability
            P (xi ) according to the language model will be counted in our total score. A unigram language model will
            probably generate many ungrammatical compressions. We therefore use a more context-aware model in our
            objective function, namely a trigram model. Dynamic programming would be typically used to decode a language
            model by traversing the sentence in a left-to-right manner. Such an algorithm is efficient and provides all
            the context required for a conventional language model. However, it can be difficult or impossible to
            incorporate global constraints into such a model as decisions on word inclusion cannot extend beyond a three
            word window. By formulating the decoding process for a trigram language model as an integer linear program
            we are able to take into account constraints that affect the compressed sentence more globally. This process
            is a much more involved task than in the unigram case where there is no context, instead we must now make
            decisions based on word sequences rather than isolated words. We first create some additional decision
            variables: αi =
            (

            βij =

            
              1
            γijk =

            1 if xi starts the compression
            ∀i ∈ [1 . . . n] 0 otherwise
            if sequence xi , xj ends
            the compression ∀i ∈ [0 . . . n − 1]   0 otherwise ∀j ∈ [i + 1 . . . n]
            
              1
            if sequence xi , xj , xk ∀i ∈ [0 . . . n − 2]
            is in the compression ∀j ∈ [i + 1 . . . n − 1]   0 otherwise ∀k ∈ [j + 1 . . . n] 409
            Clarke & Lapata

            Our objective function is given in Equation (2). This is the sum of all possible trigrams
            that can occur in all compressions of the source sentence where x0 represents the ‘start’ token and xi is
            the ith word in sentence x. Equation (3) constrains the decision variables to be binary. max z =
            n
            X
            αi · P (xi |start)
            i=1 n−2 n X n−1 X X
            +

            γijk · P (xk |xi , xj )

            i=1 j=i+1 k=j+1

            +

            n−1
            X
            n
            X
            βij · P (end|xi , xj )

            (2)

            i=0 j=i+1

            subject to:

            δi , αi , βij , γijk = 0 or 1

            (3)

            The objective function in (2) allows any combination of trigrams to be selected. This
            means that invalid trigram sequences (e.g., two or more trigrams containing the ‘end’ token) could appear in
            the target compression. We avoid this situation by introducing sequential constraints (on the decision
            variables δi , γijk , αi , and βij ) that restrict the set of allowable trigram combinations. Constraint 1
            Exactly one word can begin a sentence.
            n X
            αi = 1

            (4)

            i=1

            Constraint 2 If a word is included in the sentence it must either start the sentence or be
            preceded by two other words or one other word and the ‘start’ token x0 . δk − αk −
            k−2
            X k−1 X
            γijk = 0

            (5)

            i=0 j=1

            ∀k : k ∈ [1 . . . n]
            Constraint 3 If a word is included in the sentence it must either be preceded by one word and followed by
            another or it must be preceded by one word and end the sentence. δj −
            j−1
            X
            n
            X
            γijk −

            i=0 k=j+1

            j−1
            X
            βij = 0

            (6)

            i=0

            ∀j : j ∈ [1 . . . n]

            Constraint 4 If a word is in the sentence it must be followed by two words or followed
            by one word and then the end of the sentence or it must be preceded by one word and end the sentence. δi −
            n−1
            X
            n
            X
            j=i+1 k=j+1

            γijk −

            n
            X
            j=i+1

            410

            βij −

            i−1
            X
            βhi = 0

            h=0

            ∀i : i ∈ [1 . . . n]

            (7)

            Global Inference for Sentence Compression

            Constraint 5

            Exactly one word pair can end the sentence.
            n−1 X
            n
            X
            βij = 1

            (8)

            i=0 j=i+1

            The sequential constraints described above ensure that the second order factorization (for
            trigrams) holds and are different from our compression-specific constraints which are presented in Section
            3.5. Unless normalized by sentence length, a language model will naturally prefer one-word output. This
            normalization is however non-linear and cannot be incorporated into our ILP formulation. Instead, we impose
            a constraint on the length of the compressed sentence. Equation (9) below forces the compression to contain
            at least b tokens. n X
            δi ≥ b

            (9)

            i=1

            Alternatively, we could force the compression to be exactly b tokens (by substituting the
            inequality with an equality in (9)) or to be less than b tokens (by replacing ≥ with ≤).4 The constraint in
            (9) is language model-specific and is not used elsewhere. 3.4.2 Significance Model The language model just
            described has no notion of which content words to include in the compression and thus prefers words it has
            seen before. But words or constituents will be of different relative importance in different documents or
            even sentences. Inspired by Hori and Furui (2004), we add to our objective function (see Equation (2)) a
            significance score designed to highlight important content words. In Hori and Furui’s original formulation
            each word is weighted by a score similar to un-normalized tf ∗ idf . The significance score is not applied
            indiscriminately to all words in a sentence but solely to topic-related words, namely nouns and verbs. Our
            score differs in one respect. It combines document-level with sentence-level significance. So in addition to
            tf ∗ idf , each word is weighted by its level of embedding in the syntactic tree. Intuitively, in a sentence
            with multiply nested clauses, more deeply embedded clauses tend to carry more semantic content. This is
            illustrated in Figure 2 which depicts the clause embedding for the sentence “Mr Field has said he will
            resign if he is not reselected, a move which could divide the party nationally”. Here, the most important
            information is conveyed by clauses S3 (he will resign) and S4 (if he is not reselected) which are embedded.
            Accordingly, we should give more weight to words found in these clauses than in the main clause (S1 in
            Figure 2). A simple way to enforce this is to give clauses weight proportional to the level of embedding.
            Our modified significance score becomes: I(xi ) =
            Fa
            l · fi log N Fi
            (10)

            where xi is a topic word, fi and Fi are the frequency of xi in the document and corpus
            respectively, Fa is the sum of all topic words in the corpus, l is the number of clause 4. Compression rate
            can be also limited to a range by including two inequality constraints.
            411

            Clarke & Lapata

            S1
            S2 Mr Field has said S3 he will resign S4 if he is not reselected , a move SBAR which could divide the party
            nationally
            Figure 2: The clause embedding of the sentence “Mr Field has said he will resign if he is
            not reselected, a move which could divide the party nationally”; nested boxes correspond to nested clauses.
            constituents above xi , and N is the deepest level of clause embedding. Fa and Fi are
            estimated from a large document collection, fi is document-specific, whereas Nl is sentencespecific. So, in
            Figure 2 the term Nl is 1.0 (4/4) for clause S4 , 0.75 (3/4) for clause S3 , and so on. Individual words
            inherit their weight from their clauses. The modified objective function with the significance score is
            given below: max z =
            n
            X
            δi · λI(xi ) +

            i=1
            n−2 X n−1 X
            +

            n
            X
            αi · P (xi |start)

            i=1

            n
            X
            γijk · P (xk |xi , xj )

            i=1 j=i+1 k=j+1

            +

            n−1
            X
            n
            X
            βij · P (end|xi , xj )

            (11)

            i=0 j=i+1

            We also add a weighting factor (λ) to the objective, in order to counterbalance the importance of the
            language model and the significance score. The weight is tuned on a small
            parallel corpus. The sequential constraints from Equations (4)–(8) are again used to ensure that the
            trigrams are combined in a valid way. 3.4.3 Discriminative Model As a fully supervised model, we used the
            discriminative model presented by McDonald (2006). This model uses a large-margin learning framework coupled
            with a feature set defined on compression bigrams and syntactic structure. Let x = x1 , . . . , xn denote a
            source sentence with a target compression y = y1 , . . . , ym where each yj occurs in x. The function L(yi )
            ∈ {1 . . . n} maps word yi in the target com412
            Global Inference for Sentence Compression

            pression to the index of the word in the source sentence, x. We also include the constraint
            that L(yi )< L(yi+1 ) which forces each word in x to occur at most once in the compression y. Let the score
            of a compression y for a sentence x be: (12)
            s(x, y)

            This score is factored using a first-order Markov assumption on the words in the target
            compression to give: s(x, y) =
            |y|
            X
            s(x, L(yj−1 ), L(yj ))

            (13)

            j=2

            The score function is defined to be the dot product between a high dimensional feature
            representation and a corresponding weight vector: s(x, y) =
            |y|
            X
            w · f (x, L(yj−1 ), L(yj ))

            (14)

            j=2

            Decoding in this model amounts to finding the combination of bigrams that maximizes
            the scoring function in (14). McDonald (2006) uses a dynamic programming approach where the maximum score is
            found in a left-to-right manner. The algorithm is an extension of Viterbi for the case in which scores
            factor over dynamic sub-strings (Sarawagi & Cohen, 2004; McDonald, Crammer, & Pereira, 2005a). This allows
            back-pointers to be used to reconstruct the highest scoring compression as well as the k-best compressions.
            Again this is similar to the trigram language model decoding process (see Section 3.4.1), except that here a
            bigram model is used. Consequently, the ILP formulation is slightly simpler than that of the trigram
            language model. Let: δi =
            (

            1 if xi is in the compression
            (1 ≤ i ≤ n) 0 otherwise
            We then introduce some more decision variables:
            αi = βi = γij =
            (

            (

            (

            1 if xi starts the compression
            ∀i ∈ [1 . . . n] 0 otherwise
            1 if word xi ends the compression
            0 otherwise ∀i ∈ [1 . . . n]
            1 if sequence xi , xj is in the compression ∀i ∈ [1 . . . n − 1]
            0 otherwise ∀j ∈ [i + 1 . . . n]
            The discriminative model can be now expressed as:
            max z =
            n
            X
            αi · s(x, 0, i)
            i=1 n−1 n X X
            γij · s(x, i, j)

            +

            +

            i=1 j=i+1
            n X
            βi · s(x, i, n + 1)

            i=1

            413

            (15)

            Clarke & Lapata

            Constraint 1

            Exactly one word can begin a sentence.
            n X
            αi = 1

            (16)

            i=1

            Constraint 2 If a word is included in the sentence it must either start the compression
            or follow another word.
            δj − αj −

            j
            X
            γij = 0

            (17)

            i=1

            ∀j : j ∈ [1 . . . n]
            Constraint 3 If a word is included in the sentence it must be either followed by another word or end the
            sentence.
            δi −

            n
            X
            γij − βi = 0

            (18)

            j=i+1

            ∀i : i ∈ [1 . . . n]

            Constraint 4

            Exactly one word can end a sentence.
            n X
            βi = 1

            (19)

            i=1

            Again, the sequential constraints in Equations (16)–(19) are necessary to ensure that the
            resulting combination of bigrams are valid. The current formulation provides a single optimal compression
            given the model. However, McDonald’s (2006) dynamic programming algorithm is capable of returning the k-best
            compressions; this is useful for their learning algorithm described later. In order to produce k-best
            compressions, we must rerun the ILP with extra constraints which forbid previous solutions. In other words,
            we first formulate the ILP as above, solve it, add its solution to the k-best list, and then create a set of
            constraints that forbid the configuration of δi decision variables which form the current solution. The
            procedure is repeated until k compressions are found. The computation of the compression score crucially
            relies on the dot product between a high dimensional feature representation and a corresponding weight
            vector (see Equation (14)). McDonald (2006) employs a rich feature set defined over adjacent words and
            individual parts-of-speech, dropped words and phrases from the source sentence, and dependency structures
            (also of the source sentence). These features are designed to mimic the information presented in the
            previous noisy-channel and decision-tree models of Knight and Marcu (2002). Features over adjacent words are
            used as a proxy to the language model of the noisy channel. Unlike other models, which treat the parses as
            gold standard, McDonald uses the dependency information as another form of evidence. Faced with parses that
            are noisy the learning algorithm can reduce the weighting given to those features if they prove 414
            Global Inference for Sentence Compression

            poor discriminators on the training data. Thus, the model should be much more robust
            and portable across different domains and training corpora. The weight vector, w is learned using the Margin
            Infused Relaxed Algorithm (MIRA, Crammer & Singer, 2003) a discriminative large-margin online learning
            technique (McDonald, Crammer, & Pereira, 2005b). This algorithm learns by compressing each sentence and
            comparing the result with the gold standard. The weights are updated so that the score of the correct
            compression (the gold standard) is greater than the score of all other compressions by a margin proportional
            to their loss. The loss function is the number of words falsely retained or dropped in the incorrect
            compression relative to the gold standard. A source sentence will have exponentially many compressions and
            thus exponentially many margin constraints. To render learning computationally tractable, McDonald et al.
            (2005b) create constraints only on the k compressions that currently have the highest score, bestk (x; w).
            3.5 Constraints We are now ready to describe our compression-specific constraints. The models presented in
            the previous sections contain only sequential constraints and are thus equivalent to their original
            formulation. Our constraints are linguistically and semantically motivated in a similar fashion to the
            grammar checking component of Jing (2000). However, they do not rely on any additional knowledge sources
            (such as a grammar lexicon or WordNet) beyond the parse and grammatical relations of the source sentence. We
            obtain these from RASP (Briscoe & Carroll, 2002), a domain-independent, robust parsing system for English.
            However, any other parser with broadly similar output (e.g., Lin, 2001) could also serve our purposes. Our
            constraints revolve around modification, argument structure, and discourse related factors. Modifier
            Constraints Modifier constraints ensure that relationships between head words and their modifiers remain
            grammatical in the compression: δi − δj ≥ 0
            (20)

            ∀i, j : xj ∈ xi ’s ncmods
            δi − δj ≥ 0
            (21)

            ∀i, j : xj ∈ xi ’s detmods
            Equation (20) guarantees that if we include a non-clausal modifier5 (ncmod) in the compression (such as an
            adjective or a noun) then the head of the modifier must also be included; this is repeated for determiners
            (detmod) in (21). In Table 2 we illustrate how these constraints disallow the deletion of certain words
            (starred sentences denote compressions that would not be possible given our constraints). For example, if
            the modifier word Pasok from sentence (1a) is in the compression, then its head Party will also included
            (see (1b)). We also want to ensure that the meaning of the source sentence is preserved in the compression,
            particularly in the face of negation. Equation (22) implements this by forcing not in the compression when
            the head is included (see sentence (2b) in Table 2). A similar constraint is added for possessive modifiers
            (e.g., his, our), including genitives (e.g., John’s 5. Clausal modifiers (cmod) are adjuncts modifying
            entire clauses. In the example “he ate the cake because he was hungry”, the because-clause is a modifier of
            the sentence “he ate the cake”.
            415

            Clarke & Lapata

            1a.
            1b. 2a. 2b. 2c. 3a. 3b. 3c. 3d. 3e. 3f.
            He became a power player in Greek Politics in 1974, when he founded the
            socialist Pasok Party. *He became a power player in Greek Politics in 1974, when he founded the Pasok. We
            took these troubled youth who don’t have fathers, and brought them into a room to Dads who don’t have their
            children. *We took these troubled youth who do have fathers, and brought them into a room to Dads who do
            have their children. *We took these troubled youth who don’t have fathers, and brought them into a room to
            Dads who don’t have children. The chain stretched from Uganda to Grenada and Nicaragua, since the 1970s.
            *Stretched from Uganda to Grenada and Nicaragua, since the 1970s. *The chain from Uganda to Grenada and
            Nicaragua, since the 1970s. *The chain stretched Uganda to Grenada and Nicaragua, since the 1970s. *The
            chain stretched from to Grenada and Nicaragua, since the 1970s. *The chain stretched from Uganda to Grenada
            Nicaragua, since the 1970s. Table 2: Examples of compressions disallowed by our set of constraints.
            gift), as shown in Equation (23). An example of the possessive constraint is given in
            sentence (2c) in Table 2. δi − δj = 0
            (22)

            ∀i, j : xj ∈ xi ’s ncmods ∧ xj = not
            δi − δj = 0
            (23)

            ∀i, j : xj ∈ xi ’s possessive mods
            Argument Structure Constraints We also define a few intuitive constraints that take the overall sentence
            structure into account. The first constraint (Equation (24)) ensures that if a verb is present in the
            compression then so are its arguments, and if any of the arguments are included in the compression then the
            verb must also be included. We thus force the program to make the same decision on the verb, its subject,
            and object (see sentence (3b) in Table 2). δi − δj = 0
            (24)

            ∀i, j : xj ∈ subject/object of verb xi
            Our second constraint forces the compression to contain at least one verb provided the source sentence
            contains one as well: X
            δi ≥ 1

            (25)

            i:xi ∈verbs

            The constraint entails that it is not possible to drop the main verb stretched from sentence (3a) (see also
            sentence (3c) in Table 2).
            416
            Global Inference for Sentence Compression

            Other sentential constraints include Equations (26) and (27) which apply to prepositional phrases and
            subordinate clauses. These constraints force the introducing term
            (i.e., the preposition, or subordinator) to be included in the compression if any word from within the
            syntactic constituent is also included. By subordinator we mean wh-words (e.g., who, which, how, where), the
            word that, and subordinating conjunctions (e.g., after, although, because). The reverse is also true, i.e.,
            if the introducing term is included, at least one other word from the syntactic constituent should also be
            included. δi − δj ≥ 0
            (26)

            ∀i, j : xj ∈ PP/SUB
            ∧xi starts PP/SUB X
            δi − δj ≥ 0

            (27)

            i:xi ∈PP/SUB

            ∀j : xj starts PP/SUB
            As an example consider sentence (3d) from Table 2. Here, we cannot drop the preposition from if Uganda is in
            the compression. Conversely, we must include from if Uganda is in the compression (see sentence (3e)). We
            also wish to handle coordination. If two head words are conjoined in the source sentence, then if they are
            included in the compression the coordinating conjunction must also be included: (1 − δi ) + δj ≥ 1
            (28)

            (1 − δi ) + δk ≥ 1

            (29)

            δi + (1 − δj ) + (1 − δk ) ≥ 1

            (30)

            ∀i, j, k : xj ∧ xk conjoined by xi
            Consider sentence (3f) from Table 2. If both Uganda and Nicaragua are present in the compression, then we
            must include the conjunction and. Finally, Equation (31) disallows anything within brackets in the source
            sentence from being included in the compression. This is a somewhat superficial attempt at excluding
            parenthetical and potentially unimportant material from the compression. δi = 0
            (31)

            ∀i : xi ∈ bracketed words (inc parentheses)
            Discourse Constraints Our discourse constraint concerns personal pronouns. Specifically, Equation (32)
            forces personal pronouns to be included in the compression. The constraint is admittedly more important for
            generating coherent documents (as opposed to individual sentences). It nevertheless has some impact on
            sentence-level compressions, in particular when verbal arguments are missed by the parser. When these are
            pronominal, constraint (32) will result in more grammatical output since some of the argument structure of
            the source sentence will be preserved in the compression. δi = 1 ∀i : xi ∈ personal pronouns 417
            (32)

            Clarke & Lapata

            We should note that some of the constraints described above would be captured by
            models that learn synchronous deletion rules from a corpus. For example, the noisy-channel model of Knight
            and Marcu (2002) learns not to drop the head when the latter is modified by an adjective or a noun, since
            the transformations DT NN → DT or AJD NN → ADJ are almost never seen in the data. Similarly, the
            coordination constraint (Equations (28)–(30)) would be enforced using Turner and Charniak’s (2005) special
            rules — they enhance their parallel grammar with rules modeling more structurally complicated deletions than
            those attested in their corpus. In designing our constraints we aimed at capturing appropriate deletions for
            many possible models, including those that do not rely on a training corpus or do not have an explicit
            notion of a parallel grammar (e.g., McDonald, 2006). The modification constraints would presumably be
            redundant for the noisy-channel model, which could otherwise benefit from more specialized constraints,
            e.g., targeting sparse rules or noisy parse trees, however we leave this to future work. Another feature of
            the modeling framework presented here is that deletions (or nondeletions) are treated as unconditional
            decisions. For example, we require not to drop the noun in adjective-noun sequences if the adjective is not
            deleted as well. We also require to always include a verb in the compression if the source sentence has one.
            These hardwired decisions could in some cases prevent valid compressions from being considered. For
            instance, it is not possible to compress the sentence “this is not appropriate behavior” to “this is not
            appropriate” or“Bob loves Mary and John loves Susan” to “Bob loves Mary and John Susan”. Admittedly we lose
            some expressive power, yet we ensure that the compressions will be broadly grammatically, even for
            unsupervised or semi-supervised models. Furthermore, in practice we find that our models consistently
            outperform non-constraint-based alternatives, without extensive constraint engineering. 3.6 Solving the ILP
            As we mentioned earlier (Section 3.1), solving ILPs is NP-hard. In cases where the coefficient matrix is
            unimodular, it can be shown that the optimal solution to the linear program is integral. Although the
            coefficient matrix in our problems is not unimodular, we obtained integral solutions for all sentences we
            experimented with (approximately 3,000, see Section 4.1 for details). We conjecture that this is due to the
            fact that all of our variables have 0, +1 or −1 coefficients in the constraints and therefore our constraint
            matrix shares many properties of a unimodular matrix. We generate and solve an ILP for every sentence we
            wish to compress. Solve times are less than a second per sentence (including input-output overheads) for all
            models presented here.
            4. Experimental Set-up
            Our evaluation experiments were motivated by three questions: (1) Do the constraintbased compression models
            deliver performance gains over non-constraint-based ones? We expect better compressions for the model
            variants which incorporate compression-specific constraints. (2) Are there differences among
            constraint-based models? Here, we would like to investigate how much modeling power is gained by the
            addition of the constraints. For example, it may be the case that a state-of-the-art model like McDonald’s
            (2006) does not benefit much from the addition of constraints. And that their effect is much bigger for less
            418
            Global Inference for Sentence Compression

            sophisticated models. (3) How do the models reported in this paper port across domains?
            In particular, we are interested in assessing whether the models and proposed constraints are general and
            robust enough to produce good compressions for both written and spoken texts. We next describe the data sets
            on which our models were trained and tested (Section 4.1), explain how model parameters were estimated
            (Section 4.2) and present our evaluation setup (Section 4.3). We discuss our results in Section 5. 4.1
            Corpora Our intent was to assess the performance of the models just described on written and spoken text.
            The appeal of written text is understandable since most summarization work today focuses on this domain.
            Speech data not only provides a natural test-bed for compression applications (e.g., subtitle generation)
            but also poses additional challenges. Spoken utterances can be ungrammatical, incomplete, and often contain
            artefacts such as false starts, interjections, hesitations, and disfluencies. Rather than focusing on
            spontaneous speech which is abundant in these artefacts, we conduct our study on the less ambitious domain
            of broadcast news transcripts. This lies in-between the extremes of written text and spontaneous speech as
            it has been scripted beforehand and is usually read off on autocue. Previous work on sentence compression
            has almost exclusively used the Ziff-Davis corpus for training and testing purposes. This corpus originates
            from a collection of news articles on computer products. It was created automatically by matching sentences
            that occur in an article with sentences that occur in an abstract (Knight & Marcu, 2002). The abstract
            sentences had to contain a subset of the source sentence’s words and the word order had to remain the same.
            In earlier work (Clarke & Lapata, 2006) we have argued that the Ziff-Davis corpus is not ideal for studying
            compression for several reasons. First, we showed that human-authored compressions differ substantially from
            the Ziff-Davis which tends to be more aggressively compressed. Second, humans are more likely to drop
            individual words than lengthy constituents. Third, the test portion of the Ziff-Davis contains solely 32
            sentences. This is an extremely small data set to reveal any statistically significant differences among
            systems. In fact, previous studies relied almost exclusively on human judgments for assessing the
            well-formedness of the compressed output, and significance tests are reported for by-subjects analyses only.
            We thus focused in the present study on manually created corpora. Specifically, we asked annotators to
            perform sentence compression by removing tokens on a sentence-bysentence basis. Annotators were free to
            remove any words they deemed superfluous provided their deletions: (a) preserved the most important
            information in the source sentence, and (b) ensured the compressed sentence remained grammatical. If they
            wished, they could leave a sentence uncompressed by marking it as inappropriate for compression. They were
            not allowed to delete whole sentences even if they believed they contained no information content with
            respect to the story as this would blur the task with abstracting. Following these guidelines, our
            annotators produced compressions of 82 newspaper articles (1,433 sentences) from the British National Corpus
            (BNC) and the American News Text corpus (henceforth written corpus) and 50 stories (1,370 sentences) from
            the HUB-4 1996 English Broadcast News corpus (henceforth spoken corpus). The written corpus contains
            articles from The LA 419
            Clarke & Lapata

            Times, Washington Post, Independent, The Guardian and Daily Telegraph. The spoken
            corpus contains broadcast news from a variety of networks (CNN, ABC, CSPAN and NPR) which have been manually
            transcribed and segmented at the story and sentence level. Both corpora have been split into training,
            development and testing sets6 randomly on article boundaries (with each set containing full stories) and are
            publicly available from http: //homepages.inf.ed.ac.uk/s0460084/data/. 4.2 Parameter Estimation In this work
            we present three compression models ranging from unsupervised to semisupervised, and fully supervised. The
            unsupervised model simply relies on a trigram language model for driving compression (see Section 3.4.1).
            This was estimated from 25 million tokens of the North American corpus using the CMU-Cambridge Language
            Modeling Toolkit (Clarkson & Rosenfeld, 1997) with a vocabulary size of 50,000 tokens and GoodTuring
            discounting. To discourage one-word output we force the ILP to generate compressions whose length is no less
            than 40% of the source sentence (see the constraint in (9)). The semi-supervised model is the weighted
            combination of a word-based significance score with a language model (see Section 3.4.2). The significance
            score was calculated using 25 million tokens from the American News Text corpus. We optimized its weight
            (see Equation (11)) on a small subset of the training data (three documents in each case) using Powell’s
            method (Press, Teukolsky, Vetterling, & Flannery, 1992) and a loss function based on the F-score of the
            grammatical relations found in the gold standard compression and the system’s best compression (see Section
            4.3 for details). The optimal weight was approximately 1.8 for the written corpus and 2.2 for the spoken
            corpus. McDonald’s (2006) supervised model was trained on the written and spoken training sets. Our
            implementation used the same feature sets as McDonald, the only difference being that our phrase structure
            and dependency features were extracted from the output of Roark’s (2001) parser. McDonald uses Charniak’s
            (2000) parser which performs comparably. The model was learnt using k-best compressions. On the development
            data, we found that k = 10 provided the best performance. 4.3 Evaluation Previous studies have relied almost
            exclusively on human judgments for assessing the wellformedness of automatically derived compressions. These
            are typically rated by naive subjects on two dimensions, grammaticality and importance (Knight & Marcu,
            2002). Although automatic evaluation measures have been proposed (Riezler et al., 2003; Bangalore, Rambow, &
            Whittaker, 2000) their use is less widespread, we suspect due to the small size of the test portion of the
            Ziff-Davis corpus which is commonly used in compression work. We evaluate the output of our models in two
            ways. First, we present results using an automatic evaluation measure put forward by Riezler et al. (2003).
            They compare the grammatical relations found in the system compressions against those found in a gold
            standard. This allows us to measure the semantic aspects of summarization quality in terms of
            grammatical-functional information and can be quantified using F-score. Furthermore, 6. The splits are
            908/63/462 sentences for the written corpus and 882/78/410 sentences for the spoken corpus.
            420

            Global Inference for Sentence Compression

            in Clarke and Lapata (2006) we show that relations-based F-score correlates reliably with
            human judgments on compression output. Since our test corpora are larger than ZiffDavis (by more than a
            factor of ten), differences among systems can be highlighted using significance testing. Our implementation
            of the F-score measure used the grammatical relations annotations provided by RASP (Briscoe & Carroll,
            2002). This parser is particularly appropriate for the compression task since it provides parses for both
            full sentences and sentence fragments and is generally robust enough to analyze semi-grammatical sentences.
            We calculated F-score over all the relations provided by RASP (e.g., subject, direct/indirect object,
            modifier; 15 in total). In line with previous work we also evaluate our models by eliciting human judgments.
            Following the work of Knight and Marcu (2002), we conducted two separate experiments. In the first
            experiment participants were presented with a source sentence and its target compression and asked to rate
            how well the compression preserved the most important information from the source sentence. In the second
            experiment, they were asked to rate the grammaticality of the compressed outputs. In both cases they used a
            five point rating scale where a high number indicates better performance. We randomly selected 21 sentences
            from the test portion of each corpus. These sentences were compressed automatically by the three models
            presented in this paper with and without constraints. We also included gold standard compressions. Our
            materials thus consisted of 294 (21 × 2 × 7) sourcetarget sentences. A Latin square design ensured that
            subjects did not see two different compressions of the same sentence. We collected ratings from 42 unpaid
            volunteers, all self reported native English speakers. Both studies were conducted over the Internet using a
            custom build web interface. Examples of our experimental items are given in Table 3.
            5. Results
            Let us first discuss our results when compression output is evaluated in terms of F-score. Tables 4 and 5
            illustrate the performance of our models on the written and spoken corpora, respectively. We also present
            the compression rate7 for each system. In all cases the constraint-based models (+Constr) yield better
            F-scores than the non-constrained ones. The difference is starker for the semi-supervised model (Sig). The
            constraints bring an improvement of 17.2% on the written corpus and 18.3% on the spoken corpus. We further
            examined whether performance differences among models are statistically significant, using the Wilcoxon
            test. On the written corpus all constraint models significantly outperform the models without constraints.
            The same tendency is observed on the spoken corpus except for the model of McDonald (2006) which performs
            comparably with and without constraints. We also wanted to establish which is the best constraint model. On
            both corpora we find that the language model performs worst, whereas the significance model and McDonald
            perform comparably (i.e., the F-score differences are not statistically significant). To get a feeling for
            the difficulty of the task, we calculated how much our annotators agreed in their compression output. The
            inter-annotator agreement (F-score) on the written corpus was 65.8% and on the spoken corpus 73.4%. The
            agreement is higher on spoken texts since they consists of many short utterances (e.g., Okay, That’s it for
            now, Good night) that can 7. The term refers to the percentage of words retained from the source sentence in
            the compression.
            421

            Clarke & Lapata

            Source

            The aim is to give councils some control over the future growth of second
            homes. Gold The aim is to give councils control over the growth of homes. LM The aim is to the future.
            LM+Constr The aim is to give councils control. Sig The aim is to give councils control over the future
            growth of homes. Sig+Constr The aim is to give councils control over the future growth of homes. McD The aim
            is to give councils. McD+Constr The aim is to give councils some control over the growth of homes. Source
            The Clinton administration recently unveiled a new means to encourage brownfields redevelopment in the form
            of a tax incentive proposal. Gold The Clinton administration unveiled a new means to encourage brownfields
            redevelopment in a tax incentive proposal. LM The Clinton administration in the form of tax. LM+Constr The
            Clinton administration unveiled a means to encourage redevelopment in the form. Sig The Clinton
            administration unveiled a encourage brownfields redevelopment form tax proposal. Sig+Constr The Clinton
            administration unveiled a means to encourage brownfields redevelopment in the form of tax proposal. McD The
            Clinton unveiled a means to encourage brownfields redevelopment in a tax incentive proposal. McD+Constr The
            Clinton administration unveiled a means to encourage brownfields redevelopment in the form of a incentive
            proposal. Table 3: Example compressions produced by our systems (Source: source sentence, Gold:
            gold-standard compression, LM: language model compression, LM+Constr: language model compression with
            constraints, Sig: significance model, Sig+Constr: significance model with constraints, McD: McDonald’s
            (2006) compression model, McD+Constr: McDonald’s (2006) compression model with constraints).
            be compressed only very little or not all. Note that there is a marked difference between the
            automatic and human compressions. Our best performing systems are inferior to human output by more than 20
            F-score percentage points. Differences between the automatic systems and the human output are also observed
            with respect to the compression rate. As can be seen the language model compresses most aggressively,
            whereas the significance model and McDonald tend to be more conservative and closer to the gold standard.
            Interestingly, the constraints do not necessarily increase the compression rate. The latter increases for
            the significance model but decreases for the language model and remains relatively constant for McDonald. It
            is straightforward to impose the same compression rate for all constraint-based models (e.g., by forcing the
            model P to retain b tokens ni=1 δi = b). However, we refrained from doing this since we wanted our 422
            Global Inference for Sentence Compression

            Models
            LM Sig McD LM+Constr Sig+Constr McD+Constr Gold
            CompR
            46.2 60.6 60.1 41.2 72.0 63.7 70.3
            F-score
            18.4 23.3 36.0 28.2∗ 40.5∗† 40.8∗† —
            Table 4: Results on the written corpus; compression rate (CompR) and grammatical relation F-score (F-score);
            ∗ : +Constr model is significantly different from model
            without constraints; † : significantly different from LM+Constr. Models LM Sig McD LM+Constr Sig+Constr
            McD+Constr Gold
            CompR
            52.0 60.9 68.6 49.5 78.4 68.5 76.1
            F-score
            25.4 30.4 47.6 34.8∗ 48.7∗† 50.1† —
            Table 5: Results on the spoken corpus; compression rate (CompR) and grammatical relation F-score (F-score);
            ∗ : +Constr model is significantly different from without
            constraints; † : significantly different from LM+Constr.
            models to regulate the compression rate for each sentence individually according to its
            specific information content and structure. We next consider the results of our human study which assesses
            in more detail the quality of the generated compressions on two dimensions, namely grammaticality and
            information content. F-score conflates these two dimensions and therefore in theory could unduly reward a
            system that produces perfectly grammatical output without any information loss. Tables 6 and 7 show the mean
            ratings8 for each system (and the gold standard) on the written and spoken corpora, respectively. We first
            performed an Analysis of Variance (Anova) to examine the effect of different system compressions. The Anova
            revealed a reliable effect on both grammaticality and importance for each corpus (the effect was significant
            by both subjects and items (p
            <
            0.01)). We next examine the impact of the constraints (+Constr in the tables). In most cases we observe an
            increase in ratings for both grammaticality and importance when a model is supplemented constraints.
            Post-hoc Tukey tests reveal that the grammaticality and importance ratings of the language model and
            significance model significantly improve with 8. All statistical tests reported subsequently were done using
            the mean ratings.
            423

            Clarke & Lapata

            Models

            Grammar

            Importance

            LM
            Sig McD
            2.25†$
            3.05†
            1.82†$
            2.99†$ 2.84†
            LM+Constr
            Sig+Constr McD+Constr Gold
            3.47∗†
            3.76∗ 3.50† 4.25
            2.37∗†$
            3.53∗ 3.17† 3.98
            2.26†$

            Table 6: Results on the written text corpus; average grammaticality score (Grammar) and
            average importance score (Importance) for human judgments; ∗ : +Constr model is significantly different from
            model without constraints; † : significantly different from gold standard; $ : significantly different from
            McD+Constr.
            Models

            Grammar

            Importance

            LM
            Sig McD
            2.20†$
            2.29†$ 3.33†
            1.56†
            2.64† 3.32†
            LM+Constr
            Sig+Constr McD+Constr Gold
            3.18∗†
            3.80∗† 3.60† 4.45
            2.49∗†$
            3.69∗† 3.31† 4.25
            Table 7: Results on the spoken text corpus; average grammaticality score (Grammar) and
            average importance score (Importance) for human judgments; ∗ : +Constr model is significantly different from
            model without constraints; † : significantly different from gold standard; $ : significantly different from
            McD+Constr.
            the constraints (α
            <
            0.01). In contrast, McDonald’s system sees a numerical improvement
            with the additional constraints, but this difference is not statistically significant. These tendencies are
            observed on the spoken and written corpus. Upon closer inspection, we can see that the constraints influence
            considerably the grammaticality of the unsupervised and semi-supervised systems. Tukey tests reveal that
            LM+Constr and Sig+Constr are as grammatical as McD+Constr. In terms of importance, Sig+Constr and McD+Constr
            are significantly better than LM+Constr (α
            <
            0.01). This is not surprising given that LM+Constr is a very simple model without a mechanism for
            highlighting important words in a sentence. Interestingly, Sig+Constr performs as well as McD+Constr in
            retaining the most important words, despite the fact that it requires minimal supervision. Although
            constraint-based models overall perform better than models without constraints, they receive lower ratings
            (for grammaticality and importance) in comparison to the gold standard. And the differences are significant
            in most cases. 424
            Global Inference for Sentence Compression

            In summary, we observe that the constraints boost performance. This is more pronounced for compression
            models that are either unsupervised or use small amounts of
            parallel data. For example, a simple model like Sig yields performance comparable to McDonald (2006) when
            constraints are taken into account. This is an encouraging result suggesting that ILP can be used to create
            good compression models with relatively little effort (i.e., without extensive feature engineering or
            elaborate knowledge sources). Performance gains are also obtained for competitive models like McDonald’s
            that are fully supervised. But these gains are smaller, presumably because the initial model contains a rich
            feature representation consisting of syntactic information and generally does a good job at producing
            grammatical output. Finally, our improvements are consistent across corpora and evaluation paradigms.
        </corps>
        <conclusion>6. Conclusions
            In this paper we have presented a novel method for automatic sentence compression. A key aspect of our
            approach is the use of integer linear programming for inferring globally optimal compressions in the
            presence of linguistically motivated constraints. We have shown how previous formulations of sentence
            compression can be recast as ILPs and extended these models with local and global constraints ensuring that
            the compressed output is structurally and semantic well-formed. Contrary to previous work that has employed
            ILP solely for decoding, our models integrate learning with inference in a unified framework. Our
            experiments have demonstrated the advantages of the approach. Constraint-based models consistently bring
            performance gains over models without constraints. These improvements are more impressive for models that
            require little or no supervision. A case in point here is the significance model discussed above. The
            no-constraints incarnation of this model performs poorly and considerably worse than McDonald’s (2006)
            state-of-the-art model. The addition of constraints improves the output of this model so that its
            performance is indistinguishable from McDonald. Note that the significance model requires a small amount of
            training data (50 parallel sentences), whereas McDonald is trained on hundreds of sentences. It also
            presupposes little feature engineering, whereas McDonald utilizes thousands of features. Some effort is
            associated with framing the constraints, however these are created once and are applied across models and
            corpora. We have also observed small performance gains for McDonald’s system when the latter is supplemented
            with constraints. Larger improvements are possible with more sophisticated constraints, however our intent
            was to devise a set of general constraints that are not tuned to the mistakes of any specific system in
            particular. Future improvements are many and varied. An obvious extension concerns our constraint set.
            Currently our constraints are mostly syntactic and consider each sentence in isolation. By incorporating
            discourse constraints we could highlight words that are important at the document-level. Presumably words
            topical in a document should be retained in the compression. Other constraints could manipulate the
            compression rate. For example, we could encourage a higher compression rate for longer sentences. Another
            interesting direction includes the development of better objective functions for the compression task. The
            objective functions presented so far rely on first or second-order Markov assumptions. Alternative
            objectives could take into account the structural similarity between the source 425
            Clarke & Lapata

            sentence and its target compression; or whether they share the same content which could
            be operationalized in terms of entropy. Beyond the task and systems presented in this paper, we believe the
            approach holds promise for other generation applications using decoding algorithms for searching the space
            of possible outcomes. Examples include sentence-level paraphrasing, headline generation, and summarization.
            Acknowledgments
            We are grateful to our annotators Vasilis Karaiskos, Beata Kouchnir, and Sarah Luger. Thanks to Jean
            Carletta, Frank Keller, Steve Renals, and Sebastian Riedel for helpful comments and suggestions and to the
            anonymous referees whose feedback helped to substantially improve the present paper. Lapata acknowledges the
            support of EPSRC (grant GR/T04540/01). A preliminary version of this work was published in the proceedings
            of ACL 2006.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations and the pushdown assembler. Journal of
            Computer and System Sciences, 3, 37–56. Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation
            metrics for generation. In Proceedings of the first International Conference on Natural Language Generation,
            pp. 1–8, Mitzpe Ramon, Israel. Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning for
            natural language generation. In Proceedings of the Human Language Technology Conference of the North
            American Chapter of the Association for Computational Linguistics, pp. 359– 366, New York, NY, USA. Bramsen,
            P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs. In Proceedings of the 2006
            Conference on Empirical Methods in Natural Language Processing, pp. 189–198, Sydney, Australia. Briscoe, E.
            J., & Carroll, J. (2002). Robust accurate statistical annotation of general text. In Proceedings of the
            Third International Conference on Language Resources and Evaluation, pp. 1499–1504, Las Palmas, Gran
            Canaria. Charniak, E. (2000). A maximum-entropy-inspired parser. In Proceedings of the 1st North American
            Annual Meeting of the Association for Computational Linguistics, pp. 132– 139, Seattle, WA, USA. Clarke, J.,
            & Lapata, M. (2006). Models for sentence compression: A comparison across domains, training requirements and
            evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and
            44th Annual Meeting of the Association for Computational Linguistics, pp. 377–384, Sydney, Australia.
            Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using the CMU– Cambridge toolkit. In
            Proceedings of Eurospeech’97, pp. 2707–2710, Rhodes, Greece. 426
            Global Inference for Sentence Compression

            Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction to Algorithms. The
            MIT Press. Corston-Oliver, S. (2001). Text Compaction for Display on Very Small Screens. In Proceedings of
            the Workshop on Automatic Summarization at the 2nd Meeting of the North American Chapter of the Association
            for Computational Linguistics, pp. 89–98, Pittsburgh, PA, USA. Crammer, K., & Singer, Y. (2003).
            Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3,
            951–991. Dantzig, G. B. (1963). Linear Programming and Extensions. Princeton University Press, Princeton,
            NJ, USA. Denis, P., & Baldridge, J. (2007). Joint determination of anaphoricity and coreference resolution
            using integer programming. In Human Language Technologies 2007: The Conference of the North American Chapter
            of the Association for Computational Linguistics; Proceedings of the Main Conference, pp. 236–243,
            Rochester, NY. Dras, M. (1999). Tree Adjoining Grammar and the Reluctant Paraphrasing of Text. Ph.D. thesis,
            Macquarie University. Galley, M., & McKeown, K. (2007). Lexicalized markov grammars for sentence
            compression. In In Proceedings of the North American Chapter of the Association for Computational
            Linguistics, pp. 180–187, Rochester, NY, USA. Gomory, R. E. (1960). Solving linear programming problems in
            integers. In Bellman, R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings of Symposia in Applied
            Mathematics, Vol. 10, Providence, RI, USA. Grefenstette, G. (1998). Producing Intelligent Telegraphic Text
            Reduction to Provide an Audio Scanning Service for the Blind. In Hovy, E., & Radev, D. R. (Eds.),
            Proceedings of the AAAI Symposium on Intelligent Text Summarization, pp. 111–117, Stanford, CA, USA. Hori,
            C., & Furui, S. (2004). Speech summarization: an approach through word extraction and a method for
            evaluation. IEICE Transactions on Information and Systems, E87D (1), 15–25. Jing, H. (2000). Sentence
            reduction for automatic text summarization. In Proceedings of the 6th Applied Natural Language Processing
            Conference, pp. 310–315, Seattle,WA, USA. Knight, K., & Marcu, D. (2002). Summarization beyond sentence
            extraction: a probabilistic approach to sentence compression. Artificial Intelligence, 139 (1), 91–107.
            Land, A. H., & Doig, A. G. (1960). An automatic method for solving discrete programming problems.
            Econometrica, 28, 497–520. Lin, C.-Y. (2003). Improving summarization performance by sentence compression —
            a pilot study. In Proceedings of the 6th International Workshop on Information Retrieval with Asian
            Languages, pp. 1–8, Sapporo, Japan. Lin, D. (2001). LaTaT: Language and text analysis tools. In Proceedings
            of the first Human Language Technology Conference, pp. 222–227, San Francisco, CA, USA. 427
            Clarke & Lapata

            Marciniak, T., & Strube, M. (2005). Beyond the pipeline: Discrete optimization in NLP. In
            Proceedings of the Ninth Conference on Computational Natural Language Learning, pp. 136–143, Ann Arbor, MI,
            USA. McDonald, R. (2006). Discriminative sentence compression with soft syntactic constraints. In
            Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,
            Trento, Italy. McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation with structured
            multilabel classification. In Proceedings of Human Language Technology Conference and Conference on
            Empirical Methods in Natural Language Processing, pp. 987–994, Vancouver, BC, Canada. McDonald, R., Crammer,
            K., & Pereira, F. (2005b). Online large-margin training of dependency parsers. In 43rd Annual Meeting of the
            Association for Computational Linguistics, pp. 91–98, Ann Arbor, MI, USA. Nemhauser, G. L., & Wolsey, L. A.
            (1988). Integer and Combinatorial Optimization. WileyInterscience series in discrete mathematicals and
            opitmization. Wiley, New York, NY, USA. Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M.
            (2004). Probabilistic sentence reduction using support vector machines. In Proceedings of the 20th
            international conference on Computational Linguistics, pp. 743–749, Geneva, Switzerland. Press, W. H.,
            Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical Recipes in C: The Art of Scientific
            Computing. Cambridge University Press, New York, NY, USA. Punyakanok, V., Roth, D., Yih, W., & Zimak, D.
            (2004). Semantic role labeling via integer linear programming inference. In Proceedings of the International
            Conference on Computational Linguistics, pp. 1346–1352, Geneva, Switzerland. Riedel, S., & Clarke, J.
            (2006). Incremental integer linear programming for non-projective dependency parsing. In Proceedings of the
            2006 Conference on Empirical Methods in Natural Language Processing, pp. 129–137, Sydney, Australia.
            Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensation using ambiguity
            packing and stochastic disambiguation methods for lexical-functional grammar. In Human Language Technology
            Conference and the 3rd Meeting of the North American Chapter of the Association for Computational
            Linguistics, pp. 118– 125, Edmonton, Canada. Roark, B. (2001). Probabilistic top-down parsing and language
            modeling. Computational Linguistics, 27 (2), 249–276. Roth, D. (1998). Learning to resolve natural language
            ambiguities: A unified approach. In In Proceedings of the 15th of the American Association for Artificial
            Intelligence, pp. 806–813, Madison, WI, USA. Roth, D., & Yih, W. (2004). A linear programming formulation
            for global inference in natural language tasks. In Proceedings of the Annual Conference on Computational
            Natural Language Learning, pp. 1–8, Boston, MA, USA. 428
            Global Inference for Sentence Compression

            Roth, D., & Yih, W. (2005). Integer linear programming inference for conditional random
            fields. In Proceedings of the International Conference on Machine Learning, pp. 737– 744, Bonn. Sarawagi,
            S., & Cohen, W. W. (2004). Semi-markov conditional random fields for information extraction. In Advances in
            Neural Information Processing Systems, Vancouver, BC, Canada. Shieber, S., & Schabes, Y. (1990). Synchronous
            tree-adjoining grammars. In Proceedings of the 13th International Conference on Computational Linguistics,
            pp. 253–258, Helsinki, Finland. Turner, J., & Charniak, E. (2005). Supervised and unsupervised learning for
            sentence compression. In Proceedings of the 43rd Annual Meeting of the Association for Computational
            Linguistics, pp. 290–297, Ann Arbor, MI, USA. Vandeghinste, V., & Pan, Y. (2004). Sentence compression for
            automated subtitling: A hybrid approach. In Marie-Francine Moens, S. S. (Ed.), Text Summarization Branches
            Out: Proceedings of the ACL-04 Workshop, pp. 89–95, Barcelona, Spain. Williams, H. P. (1999). Model Building
            in Mathematical Programming (4th edition). Wiley. Winston, W. L., & Venkataramanan, M. (2003). Introduction
            to Mathematical Programming: Applications and Algorithms (4th edition). Duxbury. Zajic, D., Door, B. J.,
            Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentence compression as a tool for document
            summarization tasks. Information Processing Management Special Issue on Summarization, 43 (6), 1549–1570.
            429
        </biblio>
    </article>
    <article>
        <preamble>hybrid_approach.txt</preamble>
        <titre>Sentence Compression for Automated Subtitling: A Hybrid Approach</titre>
        <auteur>Vincent Vandeghinste and Yi Pan</auteur>
        <abstract>katholieke universiteit leuven maria theresiastraat 21 be3000 leuven belgium
            vincent.vandeghinste@ccl.kuleuven.ac.be, yi.pan@ccl.kuleuven.ac.be in this paper a sentence compression tool
            is described. we describe how an input sentence gets analysed by using a.o. a tagger, a shallow parser and a
            subordinate clause detector, and how, based on this analysis, several compressed versions of this sentence
            are generated, each with an associated estimated probability. these probabilities were estimated from a
            parallel transcript/subtitle corpus. to avoid ungrammatical sentences, the tool also makes use of a number
            of rules. the evaluation was done on three different pronunciation speeds, averaging sentence reduction
            rates of 40% to 17%. the number of reasonable reductions ranges between 32.9% and 51%, depending on the
            average estimated pronunciation speed.
        </abstract>
        <introduction>A sentence compression tool has been built with the purpose of automating subtitle generation for
            the deaf and hard-of-hearing. Verbatim transcriptions cannot be presented as the subtitle presentation time
            is between 690 and 780 characters per minute, which is more or less 5.5 seconds for two lines (ITC, 1997),
            (Dewulf and Saerens, 2000), while the average speech rate contains a lot more than the equivalent of 780
            characters per minute. The actual amount of compression needed depends on the speed of the speaker and on
            the amount of time available after the sentence. In documentaries, for instance, there are often large
            silent intervals between two sentences, the speech is often slower and the speaker is off-screen, so the
            available presentation time is longer. When the speaker is off-screen, the synchrony of the subtitles with
            the speech is of minor importance. When subtitling the news the speech rate is often very high so the amount
            of reduction needed to allow the synchronous presentation of subtitles and speech is much greater. The
            sentence compression rate is a parameter which can be set for each sentence. Note that the sentence
            compression tool de- scribed in this paper is not a subtitling tool. When subtitling, only when a sentence
            needs to be reduced, and the amount of reduction is known, the sentence is sent to the sentence compression
            tool. So the sentence compression tool is a module of an automated subtitling tool. The output of the
            sentence compression tool needs to be processed according to the subtitling guidelines like (Dewulf and
            Saerens, 2000), in order to be in the correct lay-out which makes it usable for actual subtitling. Manually
            post-editing the subtitles will still be required, as for some sentences no automatic compression is
            generated. In real subtitling it often occurs that the sentences are not compressed, but to keep the
            subtitles synchronized with the speech, some sentences are entirely removed. In section 2 we describe the
            processing of a sentence in the sentence compressor, from input to output. In section 3 we describe how the
            system was evaluated and the results of the evaluation. Section 4 contains the conclusions.
        </introduction>
        <corps>subtitling, only when a sentence needs to be reduced, and the amount of reduction is known, the sentence
            is sent to the sentence compression tool. So the sentence compression tool is a module of an automated
            subtitling tool. The output of the sentence compression tool needs to be processed according to the
            subtitling guidelines like (Dewulf and Saerens, 2000), in order to be in the correct lay-out which makes it
            usable for actual subtitling. Manually post-editing the subtitles will still be required, as for some
            sentences no automatic compression is generated. In real subtitling it often occurs that the sentences are
            not compressed, but to keep the subtitles synchronized with the speech, some sentences are entirely removed.
            In section 2 we describe the processing of a sentence in the sentence compressor, from input to output. In
            section 3 we describe how the system was evaluated and the results of the evaluation. Section 4 contains the
            conclusions.
            2 From Full Sentence to Compressed
            Sentence The sentence compression tool is inspired by (Jing, 2001). Although her goal is text summarization
            and not subtitling, her sentence compression system could serve this purpose. She uses multiple sources of
            knowledge on which her sentence reduction is based. She makes use of a corpus of sentences, aligned with
            human-written sentence reductions which is similar to the parallel corpus we use (Vandeghinste and Tjong Kim
            Sang, 2004). She applies a syntactic parser to analyse the syntactic structure of the input sentences. As
            there was no syntactic parser available for Dutch (Daelemans and Strik, 2002), we created ShaRPA
            (Vandeghinste, submitted), a shallow rule-based parser which could give us a shallow parse tree of the input
            sentence. Jing uses several other knowledge sources, which are not used (not available for Dutch) or not yet
            used in our system (like WordNet).
            In figure 1 the processing flow of an input sentence is sketched.
            Input Sentence
            Tagger

            Abbreviator

            Numbers to Digits

            Chunker

            Subordinate Clause Detector

            Shallow Parse
            Tree
            Compressor

            Compressed
            Sentence
            Grammar
            Rules
            Removal,
            Non−removal, Reduction Database
            comes EU) and replaces the full form with its abbreviation. The database can also contain the tag
            of the abbreviated part (E.g. the tag for EU is N(eigen,zijd,ev,basis,stan) [E: singular non-neuter proper
            noun]). In a third step, all numbers which are written in words in the input are replaced by their form in
            digits. This is done for all numbers which are smaller than one million, both for cardinal and ordinal
            numerals. In a fourth step, the sentence is sent to ShaRPa, which will result in a shallow parse-tree of the
            sentence. The chunking accuracy for noun phrases (NPs) has an F-value of 94.7%, while the chunking accuracy
            of prepositional phrases (PPs) has an F-value of 95.1% (Vandeghinste, submitted). A last step before the
            actual sentence compression consists of rule-based clause-detection: Relative phrases (RELP), subordinate
            clauses (SSUB) and OTI-phrases (OTI is om ... te + infinitive1 ) are detected. The accuracy of these
            detections was evaluated on 30 files from the CGN component of readaloud books, which contained 7880 words.
            The evaluation results are presented in table 1. Type of S OTI RELP SSUB
            Word Reducer

            2.1 Sentence Analysis
            In order to apply an accurate sentence compression, we need a syntactic analysis of the input sentence. In a
            first step, the input sentence gets tagged for parts-of-speech. Before that, it needs to be transformed into
            a valid input format for the part-ofspeech tagger. The tagger we use is TnT (Brants, 2000) , a hidden Markov
            trigram tagger, which was trained on the Spoken Dutch Corpus (CGN), Internal Release 6. The accuracy of TnT
            trained on CGN is reported to be 96.2% (Oostdijk et al., 2002). In a second step, the sentence is sent to
            the Abbreviator. This tool connects to a database of common abbreviations, which are often pronounced in
            full words (E.g. European Union be-
            Recall
            65.22% 68.89% 60.77%
            F-value
            68.18% 69.27% 58.74%
            Table 1: Clause Detection Accuracy

            Figure 1: Sentence Processing Flow Chart
            First we describe how the sentence is analysed (2.1), then we describe how the actual sentence compression
            is done (2.2), and after that we describe how words can be reduced for extra compression (2.3). The final
            part describes the selection of the ouput sentence (2.4).
            Precision
            71.43% 69.66% 56.83%
            The errors are mainly due to a wrong analysis
            of coordinating conjunctions, which is not only the weak point in the clause-detection module, but also in
            ShaRPa. A full parse is needed to accurately solve this problem. 2.2
            Sentence Compression

            For each chunk or clause detected in the previous
            steps, the probabilities of removal, non-removal and reduction are estimated. This is described in more
            detail in 2.2.1. Besides the statistical component in the compression, there are also a number of rules in
            the compression program, which are described in more detail in 2.2.2. The way the statistical component and
            the rulebased component are combined is described in 2.2.3. 1
            There is no equivalent construction in English. OTI is a
            VP-selecting complementizer.
            2.2.1 Use of Statistics
            Chunk and clause removal, non-removal and reduction probabilities are estimated from the frequencies of
            removal, non-removal and reduction of certain types of chunks and clauses in the parallel corpus. The
            parallel corpus consists of transcripts of television programs on the one hand and the subtitles of these
            television programs on the other hand. A detailed description of how the parallel corpus was collected, and
            how the sentences and chunks were aligned is given in (Vandeghinste and Tjong Kim Sang, 2004). All sentences
            in the source corpus (transcripts) and the target corpus (subtitles) are analysed in the same way as
            described in section 2.1, and are chunk aligned. The chunk alignment accuracy is about 95% (F-value). We
            estimated the removal, non-removal and reduction probabilities for the chunks of the types NP, PP,
            adjectival phrase (AP), SSUB, RELP, and OTI, based on their chunk removal, non-removal and reduction
            frequencies. For the tokens not belonging to either of these types, the removal and non-removal
            probabilities were estimated based on the part-of-speech tag for those words. A reduced tagset was used, as
            the original CGN-tagset (Van Eynde, 2004) was too finegrained and would lead to a multiplication of the
            number of rules which are now used in ShaRPa. The first step in SharPa consists of this reduction. For the
            PPs, the SSUBs and the RELPs, as well as for the adverbs, the chunk/tag information was considered as not
            fine-grained enough, so the estimation of the removal, non-removal and reduction probabilities for these
            types are based on the first word of those phrases/clauses and the reduction, removal and non-removal
            probabilities of such phrases in the parallel corpus, as the first words of these chunk-types are almost
            always the heads of the chunk. This allows for instance to make the distinction between several adverbs in
            one sentence, so they do not all have the same removal and nonremoval probabilities. A disadvantage is that
            this approach leads to sparse data concerning the less frequent adverbs, for which a default value (average
            over all adverbs) will be employed. An example : A noun phrase. de grootste Belgische bank [E: the largest
            Belgian bank] After tagging and chunking the sentence and after detecting subordinate clauses, for every
            nonterminal node in the shallow parse tree we retrieve the measure of removal (X), of non-removal (=) and
            of reduction2 ( ). For the terminal nodes, only the
            measures of removal and of non-removal are used. NP = 0.54 X 0.27  0.05
            DET
            = 0.68 X 0.28 de
            ADJ
            = 0.56 X 0.35 grootste
            ADJ
            = 0.56 X 0.35 Belgische
            N
            = 0.65 X 0.26 bank
            For every combination the probability estimate
            is calculated. So if we generate all possible compressions (including no compression), the phrase de
            grootste Belgische bank will get the probability estimate  
              . For the phrase de Belgische bank the probability estimate is ! "
            #$$%& ## , and so on for the other alternatives. In this way, the
            probability estimate of all possible alternatives is calculated. 2.2.2 Use of Rules As the statistical
            information allows the generation of ungrammatical sentences, a number of rules were added to avoid
            generating such sentences. The procedure keeps the necessary tokens for each kind of node. The rules were
            built in a bootstrapping manner In some of these rules, this procedure is applied recursively. These are the
            rules implemented in our system:
            ' If a node is of type SSUB or RELP, keep the
            first word.
            ' If a node is of type S, SSUB or RELP, keep
            – the verbs. If there are prepositions which are particles of the verb, keep the prepositions. If there is a
            prepositional phrase which has a preposition which is in the complements list of the verb, keep the
            necessary tokens3 of that prepositional phrase. 2
            These measures are estimated probabilities and do not need
            to add up to 1, because in the parallel training corpus, sometimes a match was detected with a chunk which
            was not a reduction of the source chunk or which was not identical to the source chunk: the chunk could be
            paraphrased, or even have become longer. 3 Recursive use of the rules
            – each token which is in the list of negative words. These words are kept to avoid
            altering the meaning of the sentence by dropping words which negate the meaning. – the necessary tokens of
            the te + infinitives (TI). – the conjunctions. – the necessary tokens of each NP. – the numerals. – the
            adverbially used adjectives.
            ' If a node is of type NP, keep
            – each noun. – each nominalised adjectival phrase. – each token which is in the list of negative words. –
            the determiners.
            2.3

            – the numerals.

            After the generation of several grammatical reductions, which are ordered according to their probability
            estimated by the product of the removal,
            non-removal and reduction probabilities of all its chunks, for every word in every compressed alternative of
            the sentence it is checked whether the word can be reduced. The words are sent to a WordSplitter-module,
            which takes a word as its input and checks if it is a compound by trying to split it up in two parts: the
            modifier and the head. This is done by lexicon lookup of both parts. If this is possible, it is checked
            whether the modifier and the head can be recompounded according to the word formation rules for Dutch (Booij
            and van Santen, 1995), (Haeseryn et al., 1997). This is done by sending the modifier and the head to a
            WordBuilding-module, which is described in more detail in (Vandeghinste, 2002). This is a hybrid module
            combining the compounding rules with statistical information about the frequency of compounds with the samen
            head, the frequency of compounds with the same modifier, and the number of different compounds with the same
            head. Only if this module allows the recomposition of the modifier and the head, the word can be considered
            to be a compound, and it can potentially be reduced to its head, removing the modifier. If the words occur
            in a database which contains a list of compounds which should not be split up, the word cannot be reduced.
            For example, the word voetbal [E: football] can be split up and recompounded according to the word formation
            rules
            – the indefinite prenominal pronouns.

            ' If a node is of type PP, keep
            – the preposition. – the determiners. – the necessary tokens of the NPs.
            ' If the node is of type adjectival phrase, keep
            – the head of the adjectival phrase. – the prenominal numerals. – each word which is in the list of negative
            words.
            ' If the node is of type OTI, keep
            – the verbs. – the te + infinitives.
            ' If the node is of type TI, keep the node.
            ' If the node is a time phrase4 , keep it.
            These rules are chosen because in tests on earlier
            versions of the system, using a different test set, ungrammatical output was generated. By using these rules
            the output should be grammatical, provided that the input sentence was analysed correctly. 4
            2.2.3 Combining Statistics and Rules
            In the current version of the system, in a first stage all variations on a sentence are generated in the
            statistical part, and they are ranked according to their probability. In a second stage, all ungrammatical
            sentences are (or should be) filtered out, so the only sentence alternatives which remain should be
            grammatical ones. This is true, only if tagging as well as chunking were correct. If errors are made on
            these levels, the generation of an ungrammatical alternative is still possible. For efficiency reasons, a
            future version of the system should combine the rules and statistics in one stage, so that the statistical
            module only generates grammatically valid sentence alternatives, although there is no effect on correctness,
            as the resulting sentence alternatives would be the same if statistics and rules were better integrated.
            A time phrase, as defined in ShaRPa is used for special
            phrases, like dates, times, etc. E.g. 27 september 1998, kwart voor drie [E: quarter to three].
            Word Reduction

            for Dutch (voet [E: foot] and bal [E: ball]), but
            we should not replace the word voetbal with the word bal if we want an accurate compression, with the same
            meaning as the original sentence, as this would alter the meaning of the sentence too much. The word voetbal
            has (at least) two different meanings: soccer and the ball with which soccer is played. Reducing it to bal
            would only keep the second meaning. The word gevangenisstraf [E: prison sentence] can be split up and
            recompounded (gevangenis [E: prison] and straf [E: punishment]). We can replace the word gevangenisstraf by
            the word straf. This would still alter the meaning of the sentence, but not to the same amount as it would
            have been altered in the case of the word voetbal. 2.4 Selection of the Compressed Sentence Applying all the
            steps described in the previous sections results in an ordered list of sentence alternatives, which are
            supposedly grammatically correct. When word reduction was possible, the wordreduced alternative is inserted
            in this list, just after its full-words equivalent. The first sentence in this list with a length smaller
            than the maximal length (depending on the available presentation time) is selected. In a future version of
            the system, the word reduction information can be integrated in a better way with the rest of the module, by
            combining the probability of reduction/non-reduction of a word with the probability of the sentence
            alternative. The reduction probability of a word would then play its role in the estimated probability of
            the compressed sentence alternative containing this reduced word.
            3 Evaluation
            The evaluation of a sentence compression module is not an easy task. The output of the system needs to be
            judged manually for its accuracy. This is a very time consuming task. Unlike (Jing, 2001), we do not compare
            the system results with the human sentence reductions. Jing reports a succes rate of 81.3% for her program,
            but this measure is calculated as the percentage of decisions on which the system agrees with the decisions
            taken by the human summarizer. This means that 81.3% of all system decisions are correct, but does not say
            anything about how many sentences are correctly reduced. In our evaluation we do not expect the compressor
            to simulate human summarizer behaviour. The results presented here are calculated on the sentence level: the
            amount of valid reduced sentences, being those reductions which are judged by human raters to be accurate
            reductions: grammatical sentences with (more or less) the same meaning as the
            input sentence, taking into account the meaning of
            the previous sentences on the same topic. 3.1
            Method

            To estimate the available number of characters in a
            subtitle, it is necessary to estimate the average pronunciation time of the input sentence, provided that it
            is unknown. We estimate sentence duration by counting the number of syllables in a sentence and multiplying
            this with the average duration per syllable (ASD). The ASD for Dutch is reported to be about 177 ms
            (Koopmans-van Beinum and van Donzel, 1996), which is the syllable speed without including pauses between
            words or sentences. We did some similar research on CGN using the ASD as a unit of analysis, while we
            consider both the situation without pauses and the situation with pauses. Results of this research are
            presented in table 2. ASD All files One speaker Read-aloud
            no pauses
            186 185 188
            pauses included
            237 239 256
            Table 2: Average Syllable Duration (ms)

            We extract the word duration from all the files
            in each component of CGN. A description of the components can be found in (Oostdijk et al., 2002). We
            created a syllable counter for Dutch words, which we evaluated on all words in the CGN lexicon. For 98.3% of
            all words in the lexicon, syllables are counted correctly. Most errors occur in very low frequency words or
            in foreign words. By combining word duration information and the number of syllables we can calculate the
            average speaking speed. We evaluated sentence compression in three different conditions: The fastest ASD in
            our ASD-research was 185 ms (one speaker, no pauses), which was used for Condition A. We consider this ASD
            as the maximum speed for Dutch. The slowest ASD (256 ms) was used for Condition C. We consider this ASD to
            be the minimum speed for Dutch. We created a testset of 100 sentences mainly focused on news broadcasts in
            which we use the real pronunciation time of each sentence in the testset which results in an ASD of 192ms.
            This ASD was
            used for Condition B, and is considered as the real
            speed for news broadcasts. We created a testset of 300 sentences, of which 200 were taken from transcripts
            of television news, and 100 were taken from the ’broadcast news’ component of CGN. To evaluate the
            compressor, we estimate the duration of each sentence, by counting the number of syllables and multiplying
            that number with the ASD for that condition. This leads to an estimated pronunciation time. This is
            converted to the number of characters, which is available for the subtitle. We know the average time for
            subtitle presentation at the VRT (Flemish Broadcasting Coorporation) is 70 characters in 6 seconds, which
            gives us an average of 11.67 characters per second. So, for example, if we have a test sentence of 15
            syllables, this gives us an estimated pronunciation time of 2.775 seconds (15 syllables 185 ms/syllable) in
            condition A. When converting this to the available characters, we multiply 2.775 seconds by 11.67
            characters/second, resulting in 32 (2.775s 11.67 ch/s = 32.4 ch) available characters. In condition B
            (considered to be real-time) for the part of the test-sentences coming from CGN, the pronunciation time was
            not estimated, as it was available in CGN. 3.2
            Results

            The results of our experiments on the sentence compression module are presented in table 3.
            Condition No output (0) Avg Syllable speed (msec/syllable) Avg Reduction Rate Interrater Agreement Accurate
            Compr. +/- Acc. Compr. Reasonable Compr.
            A
            44.33%
            B
            41.67%
            C
            15.67%
            185
            39.93% 86.2% 4.8% 28.1% 32.9%
            192
            37.65% 86.9% 8.0% 26.3% 34.3%
            256
            16.93% 91.7% 28.9% 22.1% 51%
            Table 3: Sentence Compression Evaluation on the
            Sentence Level The sentence compressor does not generate output for all test sentences in all conditions: In
            those cases where no output was generated, the sentence compressor was not able to generate a sentence
            alternative which was shorter than the maximum number of characters available for that sentence. The cases
            where no output is generated are not considered as errors because it is often impossible, even for humans,
            to reduce a sentence by about 40%,
            without changing the content too much. The amount
            of test sentences where no output was generated is presented in table 3. The high percentage of sentences
            where no output was generated in conditions A and B is most probably due to the fact that the compression
            rates in these conditions are higher than they would be in a real life application. Condition C seems to be
            closer to the real life compression rate needed in subtitling. Each condition has an average reduction rate
            over the 300 test sentences. This reduction rate is based on the available amount of characters in the
            subtitle and the number of characters in the source sentence. A rater scores a compressed sentence as + when
            it is grammatically correct and semantically equivalent to the input sentence. No essential information
            should be missing. A sentence is scored as +/when it is grammatically correct, but some information is
            missing, but is clear from the context in which the sentence occurs. All other compressed sentences get
            scored as -. Each sentence is evaluated by two raters. The lowest score of the two raters is the score which
            the sentence gets. Interrater agreement is calculated on a 2 point score: if both raters score a sentence as
            + or +/- or both raters score a sentence as -, it is considered an agreed judgement. Interrater agreement
            results are presented in table 3. Sentence compression results are presented in table 3. We consider both
            the + and +/- results as reasonable compressions. The resulting percentages of reasonable compressions seem
            to be rather low, but one should keep in mind that these results are based on the sentence level. One little
            mistake in one sentence can lead to an inaccurate compression, although the major part of the decisions
            taken in the compression process can still be correct. This makes it very hard to compare our results to the
            results presented by Jing (2001), but we presented our results on sentence evaluations as it gives a clearer
            idea on how well the system would actually perform in a real life application. As we do not try to immitate
            human subtitling behaviour, but try to develop an equivalent approach, our system is not evaluated in the
            same way as the system deviced by Jing.
        </corps>
        <conclusion>4 Conclusion
            We have described a hybrid approach to sentence compression which seems to work in general. The combination
            of using statistics and filtering out invalid results because they are ungrammatical by using a set of rules
            is a feasible way for automated
            sentence compression.
            The way of combining the probability-estimates of chunk removal to get a ranking in the generated sentence
            alternatives is working reasonably well, but could be improved by using more fine-grained chunk types for
            data collection. A full syntactic analysis of the input sentence would lead to better results, as the
            current sentence analysis tools have one very weak point: the handling of coordinating conjunction, which
            leads to chunking errors, both in the input sentence as in the processing of the used parallel corpus. This
            leads to misestimations of the compression probabilities and creates noise in the behaviour of our system.
            Making use of semantics would most probably lead to better results, but a semantic lexicon and semantic
            analysis tools are not available for Dutch, and creating them would be out of the scope of the current
            project. In future research we will check the effects of improved word-reduction modules, as word reductions
            often seem to lead to inaccurate compressions. Leaving out the word-reduction module would probably lead to
            an even bigger amount of no output-cases. This will also be checked in future research.
            5 Acknowledgements
            Research funded by IWT (Institute for Innovation in Science and Technology) in the STWW program, project
            ATraNoS (Automatic Transcription and Normalisation of Speech). For more information visit
            http://atranos.esat.kuleuven.ac.be/. We would like to thank Ineke Schuurman for rating the reduced
            sentences.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>G. Booij and A. van Santen. 1995. Morfologie. De woordstructuur van het Nederlands. Amsterdam University
            Press, Amsterdam, Netherlands. T. Brants. 2000. TnT - A Statistical Part-of-Speech Tagger. Published online
            at http://www.coli.unisb.de/thorsten/tnt. W. Daelemans and H. Strik. 2002. Het Nederlands in Taal- en
            Spraaktechnologie: Prioriteiten voor Basisvoorzieningen. Technical report, Nederlandse Taalunie. B. Dewulf
            and G. Saerens. 2000. Stijlboek Teletekst Ondertiteling. Technical report, VRT, Brussel. Internal Subtitling
            Guidelines. W. Haeseryn, G. Geerts, J de Rooij, and M. van den Toorn. 1997. Algemene Nederlandse
            Spraakkunst. Martinus Nijhoff Uitgevers, Groningen.
            ITC.
            1997. Guidance on standards for subtitling. Technical report, ITC. Online at http://www.itc.org.uk/ codes
            guidelines/broadcasting/tv/sub sign audio/subtitling stnds/. H. Jing. 2001. Cut-and-Paste Text
            Summarization. Ph.D. thesis, Columbia University. F.J. Koopmans-van Beinum and M.E. van Donzel. 1996.
            Relationship Between Discourse Structure and Dynamic Speech Rate. In Proceedings ICSLP 1996, Philadelphia,
            USA. N. Oostdijk, W. Goedertier, F. Van Eynde, L. Boves, J.P. Marters, M. Moortgat, and H. Baayen. 2002.
            Experiences from the Spoken Dutch Corpus. In Proceedings of LREC 2002, volume I, pages 340– 347, Paris.
            ELRA. F. Van Eynde. 2004. Part-of-speech Tagging en Lemmatisering. Internal manual of Corpus Gesproken
            Nederlands, published online at http://www.ccl.kuleuven.ac.be/Papers/ POSmanual febr2004.pdf. V.
            Vandeghinste and E. Tjong Kim Sang. 2004. Using a parallel transcript/subtitle corpus for sentence
            compression. In Proceedings of LREC 2004, Paris. ELRA. V. Vandeghinste. 2002. Lexicon optimization:
            Maximizing lexical coverage in speech recognition through automated compounding. In Proceedings of LREC
            2002, volume IV, pages 1270– 1276, Paris. ELRA. V. Vandeghinste. submitted. ShaRPa: Shallow Rule-based
            Parsing, focused on Dutch. In Proceedings of CLIN 2003.
        </biblio>
    </article>
    <article>
        <preamble>marcu_statistics_sentence_pass_one.txt</preamble>
        <titre>Statistics-Based Summarization — Step One: Sentence Compression</titre>
        <auteur>Kevin Knight and Daniel Marcu</auteur>
        <abstract>information sciences institute and department of computer science university of southern california
            4676 admiralty way, suite 1001 marina del rey, ca 90292 {knight,marcu}@isi.edu when humans produce summaries
            of documents, they do not simply extract sentences and concatenate them. rather, they create new sentences
            that are grammatical, that cohere with one another, and that capture the most salient pieces of information
            in the original document. given that large collections of text/abstract pairs are available online, it is
            now possible to envision algorithms that are trained to mimic this process. in this paper, we focus on
            sentence compression, a simpler version of this larger challenge. we aim to achieve two goals simultaneously
            our compressions should be grammatical, and they should retain the most important pieces of information.
            these two goals can conﬂict. we devise both noisychannel and decisiontree approaches to the problem, and we
            evaluate results against manual compressions and a simple baseline.
        </abstract>
        <introduction>Most of the research in automatic summarization has focused on extraction, i.e., on identifying
            the most important clauses/sentences/paragraphs in texts (see (Mani & Maybury 1999) for a representative
            collection of papers). However, determining the most important textual segments is only half of what a
            summarization system needs to do because, in most cases, the simple catenation of textual segments does not
            yield coherent outputs. Recently, a number of researchers have started to address the problem of generating
            coherent summaries: McKeown et al. (1999), Barzilay et al. (1999), and Jing and McKeown (1999) in the
            context of multidocument summarization; Mani et al. (1999) in the context of revising single document
            extracts; and Witbrock and Mittal (1999) in the context of headline generation. The approach proposed by
            Witbrock and Mittal (1999) is the only one that applies a probabilistic model trained directly on Headline,
            Document pairs. However, this model has yet to scale up to generating multiple-sentence abstracts as well
            as well-formed, grammatical sentences. All other approaches employ sets of manually written or
            semi-automatically derived c 2000, American Association for Artiﬁcial InCopyright  telligence
            (www.aaai.org). All rights reserved. rules for deleting information that is redundant, compressing long
            sentences into shorter ones, aggregating sentences, repairing reference links, etc. Our goal is also to
            generate coherent abstracts. However, in contrast with the above work, we intend to eventually use
            Abstract, Text tuples, which are widely available, in order to automatically learn how to rewrite Texts as
            coherent Abstracts. In the spirit of the work in the statistical MT community, which is focused on
            sentence-to-sentence translations, we also decided to focus ﬁrst on a simpler problem, that of sentence
            compression. We chose this problem for two reasons: • First, the problem is complex enough to require the
            development of sophisticated compression models: Determining what is important in a sentence and determining
            how to convey the important information grammatically, using only a few words, is just a scaled down version
            of the text summarization problem. Yet, the problem is simple enough, since we do not have to worry yet
            about discourse related issues, such as coherence, anaphors, etc. • Second, an adequate solution to this
            problem has an immediate impact on several applications. For example, due to time and space constraints, the
            generation of TV captions often requires only the most important parts of sentences to be shown on a screen
            (Linke-Ellis 1999; Robert-Ribes et al. 1999). A good sentence compression module would therefore have an
            impact on the task of automatic caption generation. A sentence compression module can also be used to
            provide audio scanning services for the blind (Grefenstette 1998). In general, since all systems aimed at
            producing coherent abstracts implement manually written sets of sentence compression rules (McKeown et al.
            1999; Mani, Gates, & Bloedorn 1999; Barzilay, McKeown, & Elhadad 1999), it is likely that a good sentence
            compression module would impact the overall quality of these systems as well. This becomes particularly
            important for text genres that use long sentences. In this paper, we present two approaches to the sentence
            compression problem. Both take as input a sequence of words W = w1 , w2 , . . . , wn (one sentence). An
            algorithm may drop any subset of these words. The words that remain (order unchanged) form a compression.
            There are 2n compressions to choose from—some are reasonable, most are not. Our ﬁrst approach develops a
            probabilistic noisy-channel model for sentence compression. The second approach develops a decisionbased,
            deterministic model.
        </introduction>
        <corps>words that remain (order unchanged) form a compression. There are 2n compressions to choose from—some are
            reasonable, most are not. Our ﬁrst approach develops a probabilistic noisy-channel model for sentence
            compression. The second approach develops a decisionbased, deterministic model.
            A noisy-channel model for sentence
            compression This section describes a probabilistic approach to the compression problem. In particular, we
            adopt the noisy channel framework that has been relatively successful in a number of other NLP applications,
            including speech recognition (Jelinek 1997), machine translation (Brown et al. 1993), part-of-speech tagging
            (Church 1988), transliteration (Knight & Graehl 1998), and information retrieval (Berger & Laﬀerty 1999). In
            this framework, we look at a long string and imagine that (1) it was originally a short string, and then (2)
            someone added some additional, optional text to it. Compression is a matter of identifying the original
            short string. It is not critical whether or not the “original” string is real or hypothetical. For example,
            in statistical machine translation, we look at a French string and say, “This was originally English, but
            someone added ‘noise’ to it.” The French may or may not have been translated from English originally, but by
            removing the noise, we can hypothesize an English source—and thereby translate the string. In the case of
            compression, the noise consists of optional text material that pads out the core signal. For the larger case
            of text summarization, it may be useful to imagine a scenario in which a news editor composes a short
            document, hands it to a reporter, and tells the reporter to “ﬂesh it out” . . . which results in the article
            we read in the newspaper. As summarizers, we may not have access to the editor’s original version (which may
            or may not exist), but we can guess at it— which is where probabilities come in. As in any noisy channel
            application, we must solve three problems: • Source model. We must assign to every string s a probability
            P(s), which gives the chance that s is generated as an “original short string” in the above hypothetical
            process. For example, we may want P(s) to be very low if s is ungrammatical. • Channel model. We assign to
            every pair of strings s, t a probability P(t | s), which gives the chance that when the short string s is
            expanded, the result is the long string t. For example, if t is the same as s except for the extra word
            “not,” then we may want P(t | s) to be very low. The word “not” is not optional, additional material. •
            Decoder. When we observe a long string t, we search for the short string s that maximizes P(s | t). This is
            equivalent to searching for the s that maximizes P(s) · P (t | s).
            It is advantageous to break the problem down this
            way, as it decouples the somewhat independent goals of creating a short text that (1) looks grammatical, and
            (2) preserves important information. It is easier to build a channel model that focuses exclusively on the
            latter, without having to worry about the former. That is, we can specify that a certain substring may
            represent unimportant information, but we do not need to worry that deleting it will result in an
            ungrammatical structure. We leave that to the source model, which worries exclusively about well-formedness.
            In fact, we can make use of extensive prior work in source language modeling for speech recognition, machine
            translation, and natural language generation. The same goes for actual compression (“decoding” in
            noisy-channel jargon)—we can re-use generic software packages to solve problems in all these application
            domains.
            Statistical Models
            In the experiments we report here, we build very simple source and channel models. In a departure from the
            above discussion and from previous work on statistical channel models, we assign probabilities Ptree (s) and
            Pexpand tree (t | s) to trees rather than strings. In decoding a new string, we ﬁrst parse it into a large
            tree t (using Collins’ parser (1997)), and we then hypothesize and rank various small trees. Good source
            strings are ones that have both (1) a normal-looking parse tree, and (2) normal-looking word pairs. Ptree
            (s) is a combination of a standard probabilistic context-free grammar (PCFG) score, which is computed over
            the grammar rules that yielded the tree s, and a standard word-bigram score, which is computed over the
            leaves of the tree. For example, the tree s =(S (NP John) (VP (VB saw) (NP Mary))) is assigned a score based
            on these factors: Ptree (s) = P(TOP → S | TOP) · P(S → NP VP | S) · P(NP → John | NP) · P(VP → VB NP | VP) ·
            P(VP → saw | VB) · P(NP → Mary | NP) · P(John | EOS) · P(saw | John) · P(Mary | saw) · P(EOS | Mary) Our
            stochastic channel model performs minimal operations on a small tree s to create a larger tree t. For each
            internal node in s, we probabilistically choose an expansion template based on the labels of the node and
            its children. For example, when processing the S node in the tree above, we may wish to add a prepositional
            phrase as a third child. We do this with probability P(S → NP VP PP | S → NP VP). Or we may choose to leave
            it alone, with probability P(S → NP VP | S → NP VP). After we choose an expansion template, then for each
            new child node introduced (if any), we grow a new subtree rooted at that node—for example (PP (P in) (NP
            Pittsburgh)). Any particular subtree is grown with probability given by its PCFG factorization, as above (no
            bigrams).
            G

            G
            A
            H
            a
            C

            G
            A
            H
            D
            B

            b Q

            R

            Z

            d

            a

            e

            D

            F

            C

            D

            b

            e

            H

            K

            a

            b

            e

            c
            (t)
            Figure 1: Examples of parse trees.

            Although the modules themselves may be physically and/or
            electrically incompatible, the cable-speciﬁc jacks on them provide industry-standard connections.
            Cable-speciﬁc jacks provide industry-standard connections.
            Example
            In this section, we show how to tell whether one potential compression is more likely than another,
            according to the statistical models described above. Suppose we observe the tree t in Figure 1, which spans
            the string abcde. Consider the compression s1, which is shown in the same ﬁgure. We compute the factors
            Ptree (s1) and Pexpand tree (t | s1). Breaking this down further, the source PCFG and word-bigram factors,
            which describe Ptree (s1), are: P(TOP → G | TOP) P(G → H A | G) P(A → C D | A)
            P(H → a | H)
            P(C → b | C) P(D → e | D)
            P(a | EOS)
            P(b | a)
            P(e | b)
            P(EOS | e)
            The channel expansion-template factors and the channel PCFG (new tree growth) factors, which describe
            Pexpand tree (t | s1), are: P(G → H A | G → H A) P(A → C B D | A → C D) P(B → Q R | B) P(Q → Z | Q)
            All of our design goals were achieved and the delivered
            performance matches the speed of the underlying device. All design goals were achieved. Reach’s E-mail
            product, MailMan, is a message- management system designed initially for VINES LANs that will eventually be
            operating system-independent. MailMan will eventually be operating system-independent.
            (s2)

            (s1)

            The documentation is typical of Epson quality: excellent.
            Documentation is excellent.
            P(Z → c | Z)
            P(R → d | R)
            A diﬀerent compression will be scored with a diﬀerent
            set of factors. For example, consider a compression of t that leaves t completely untouched. In that case,
            the source costs Ptree (t) are: P(TOP → G | TOP) P(G → H A | G)
            P(H → a | H)
            P(C → b | C)
            P(a | EOS)
            P(b | a)
            P(A → C D | A)

            P(Z → c | Z)

            P(c | b)

            P(B → Q R | B)
            P(Q → Z | Q)
            P(R → d | R)
            P(D → e | D)
            P(d | c)
            P(e | d) P(EOS | e)
            The channel costs Pexpand tree (t | t) are:

            Ingres/Star prices start at $2,100.
            Ingres/Star prices start at $2,100.
            Figure 2: Examples from our parallel corpus.
            P(G → H A | G → H A) P(A → C B D | A → C B D) P(B → Q R | B → Q R) P(Q → Z | Q → Z) Now we can simply
            compare Pexpand tree (s1 | t) = Ptree (s1) · Pexpand tree (t | s1))/Ptree (t) versus Pexpand tree (t | t) =
            Ptree (t) · Pexpand tree (t | t))/Ptree (t) and select the more likely one. Note that Ptree (t) and all the
            PCFG factors can be canceled out, as they appear in any potential compression. Therefore, we need only
            compare compressions of the basis of the expansion-template probabilities and the word-bigram probabilities.
            The quantities that diﬀer between the two proposed compressions are boxed above. Therefore, s1 will be
            preferred over t if and only if: P(e | b) · P(A → C B D | A → C D) > P(b | a) · P(c | b) · P(d | c) · P(A →
            C B D | A → C B D) · P(B → Q R | B → Q R) · P(Q → Z | Q → Z)
            Training Corpus
            In order to train our system, we used the Ziﬀ-Davis corpus, a collection of newspaper articles announcing
            computer products. Many of the articles in the corpus are paired with human written abstracts. We
            automatically extracted from the corpus a set of 1067 sentence pairs. Each pair consisted of a sentence t =
            t1 , t2 , . . . , tn that occurred in the article and a possibly compressed version of it s = s1 , s2 , . .
            . , sm , which occurred in the human written abstract. Figure 2 shows a few sentence pairs extracted from
            the corpus. We decided to use such a corpus because it is consistent with two desiderata speciﬁc to
            summarization work: (i) the human-written Abstract sentences are
            grammatical; (ii) the Abstract sentences represent in a
            compressed form the salient points of the original newspaper Sentences. We decided to keep in the corpus
            uncompressed sentences as well, since we want to learn not only how to compress a sentence, but also when to
            do it.
            Learning Model Parameters
            We collect expansion-template probabilities from our parallel corpus. We ﬁrst parse both sides of the
            parallel corpus, and then we identify corresponding syntactic nodes. For example, the parse tree for one
            sentence may begin (S (NP . . . ) (VP . . . ) (PP . . . )) while the parse tree for its compressed version
            may begin (S (NP . . . ) (VP . . . )). If these two S nodes are deemed to correspond, then we chalk up one
            joint event (S → NP VP, S → NP VP PP); afterwards we normalize. Not all nodes have corresponding partners;
            some noncorrespondences are due to incorrect parses, while others are due to legitimate reformulations that
            are beyond the scope of our simple channel model. We use standard methods to estimate word-bigram
            probabilities.
            Beyond that basic level, the operations of the three products vary
            widely (1514588) Beyond that level, the operations of the three products vary widely (1430374) Beyond that
            basic level, the operations of the three products vary (1333437) Beyond that level, the operations of the
            three products vary (1249223) Beyond that basic level, the operations of the products vary (1181377)
            Decoding

            The operations of the three products vary widely (939912)

            There is a vast number of potential compressions of a
            large tree t, but we can pack them all eﬃciently into a shared-forest structure. For each node of t that has
            n children, we • generate 2n − 1 new nodes, one for each non-empty subset of the children, and • pack those
            nodes so that they are referred to as a whole. For example, consider the large tree t above. All
            compressions can be represented with the following forest: G→HA G→H G→A B→QR B→Q
            semantic representation into a vast number of potential
            English renderings. These renderings are packed into a forest, from which the most promising sentences are
            extracted using statistical scoring. For our purposes, the extractor selects the trees with the best
            combination of word-bigram and expansiontemplate scores. It returns a list of such trees, one for each
            possible compression length. For example, for the sentence Beyond that basic level, the operations of the
            three products vary, we obtain the following “best” compressions, with negative log-probabilities shown in
            parentheses (smaller = more likely):
            B→R
            Q→Z A→CBD A→CB A→CD
            A→BC
            A→C A→B A→D
            H→a
            C→b Z→c R→d D→e
            We can also assign an expansion-template probability
            to each node in the forest. For example, to the B → Q node, we can assign P(B → Q R | B → Q). If the
            observed probability from the parallel corpus is zero, then we assign a small ﬂoor value of 10−6 . In
            reality, we produce forests that are much slimmer, as we only consider compressing a node in ways that are
            locally grammatical according to the Penn Treebank—if a rule of the type A → C B has never been observed,
            then it will not appear in the forest. At this point, we want to extract a set of highscoring trees from the
            forest, taking into account both expansion-template probabilities and word-bigram probabilities.
            Fortunately, we have such a generic extractor on hand (Langkilde 2000). This extractor was designed for a
            hybrid symbolic-statistical natural language generation system called Nitrogen. In that application, a
            rule-based component converts an abstract
            The operations of the products vary widely (872066)
            The operations of the products vary (748761) The operations of products vary (690915) Operations of products
            vary (809158) The operations vary (522402) Operations vary (662642)
            Length Selection
            It is useful to have multiple answers to choose from, as one user may seek a 20% compression, while another
            seeks a 60% compression. However, for purposes of evaluation, we want our system to be able to select a
            single compression. If we rely on the log-probabilities as shown above, we will almost always choose the
            shortest compression. (Note above, however, how the threeword compression scores better than the two-word
            compression, as the models are not entirely happy removing the article “the”). To create a more fair
            competition, we divide the log-probability by the length of the compression, rewarding longer strings. This
            is commonly done in speech recognition. If we plot this normalized score against compression length, we
            usually observe a (bumpy) U-shaped curve, as illustrated in Figure 3. In a typical more diﬃcult case, a
            25-word sentence may be optimally compressed by a 17-word version. Of course, if a user requires a shorter
            compression than that, she may select another region of the curve and look for a local minimum.
            A decision-based model for sentence
            compression In this section, we describe a decision-based, history model of sentence compression. As in the
            noisy-channel approach, we again assume that we are given as input
            0.10

            4

            5

            6

            7

            8

            Finally, another advantage of broadband is distance .

            Finally another advantage of broadband is distance .

            Another advantage of broadband is distance .

            Advantage of broadband is distance .

            0.15

            Another advantage is distance .

            Advantage is distance .

            Adjusted negative log-probability of best
            compression s at a particular length n -log P(s) P( t | s) / n
            0.20

            Stack

            9

            Compression length n

            Figure 3: Adjusted log-probabilities for top-scoring
            compressions at various lengths (lower is better). a parse tree t. Our goal is to “rewrite” t into a smaller
            tree s, which corresponds to a compressed version of the original sentence subsumed by t. Suppose we observe
            in our corpus the trees t and s2 in Figure 1. In this model, we ask ourselves how we may go about rewriting
            t into s2. One possible solution is to decompose the rewriting operation into a sequence of
            shift-reduce-drop actions that are speciﬁc to an extended shift-reduce parsing paradigm. In the model we
            propose, the rewriting process starts with an empty Stack and an Input List that contains the sequence of
            words subsumed by the large tree t. Each word in the input list is labeled with the name of all syntactic
            constituents in t that start with it (see Figure 4). At each step, the rewriting module applies an operation
            that is aimed at reconstructing the smaller tree s2. In the context of our sentence-compression module, we
            need four types of operations: • shift operations transfer the ﬁrst word from the input list into the stack;
            • reduce operations pop the k syntactic trees located at the top of the stack; combine them into a new tree;
            and push the new tree on the top of the stack. Reduce operations are used to derive the structure of the
            syntactic tree of the short sentence. • drop operations are used to delete from the input list subsequences
            of words that correspond to syntactic constituents. A drop x operations deletes from the
            Input List

            Input List

            Stack
            F
            G
            H a
            H

            A

            a

            C
            b
            H

            K

            a

            b

            B
            Q Z c
            A
            B Q Z c
            C
            b
            B
            Q Z c
            R
            d
            R
            d
            R
            d
            D
            e
            SHIFT;

            K

            a

            b

            ASSIGNTYPE H

            F

            STEPS 1-2
            D e
            SHIFT;

            H

            K

            a

            b

            F

            R
            d
            D
            e
            D
            e
            DROP B
            STEP 6 SHIFT; ASSIGNTYPE D STEPS 7-8
            D

            ASSIGNTYPE K
            STEPS 3-4
            D
            e
            H

            B
            Q Z c
            H

            K

            a

            b

            e

            REDUCE 2 G
            STEP 9
            G
            REDUCE 2 F STEP 5
            F

            D

            H

            K

            a

            b

            e

            Figure 4: Example of incremental tree compression.
            input list all words that are spanned by constituent x in t. • assignType operations are used to change the
            label of trees at the top of the stack. These actions assign POS tags to the words in the compressed
            sentence, which may be diﬀerent from the POS tags in the original sentence. The decision-based model is more
            ﬂexible than the channel model because it enables the derivation of trees whose skeleton can diﬀer quite
            drastically from that of the tree given as input. For example, using the channel model, we are unable to
            obtain tree s2 from t. However, the four operations listed above enable us to rewrite a tree t into any tree
            s, as long as an in-order traversal of the leaves of s produces a sequence of words that occur in the same
            order as the words in the tree t. For example, the tree s2 can be obtained from tree t by following this
            sequence of actions, whose eﬀects are shown in Figure 4: shift; assignType H; shift; assignType K; reduce 2
            F; drop B; shift; assignType D; reduce 2 G. To save space, we show shift and assignType operations on the
            same line; however, the reader should understand that they correspond to two distinct actions. As one can
            see, the assignType K operation rewrites the POS tag of the word b; the reduce operations modify the
            skeleton of the tree given as input. To increase readability, the input list is shown in a format that
            resembles as closely as possible the graphical representation of the trees in ﬁgure 1.
            Learning the parameters of the
            decision-based model We associate with each conﬁguration of our shiftreduce-drop, rewriting model a learning
            case. The cases are generated automatically by a program that derives sequences of actions that map each of
            the large trees in our corpus into smaller trees. The rewriting procedure simulates a bottom-up
            reconstruction of the smaller trees. Overall, the 1067 pairs of long and short sentences yielded 46383
            learning cases. Each case was labeled
            with one action name from a set of 210 possible actions: There are 37 distinct assignType actions, one
            for each POS tag. There are 63 distinct drop actions, one for each type of syntactic constituent that can be
            deleted during compression. There are 109 distinct reduce actions, one for each type of reduce operation
            that is applied during the reconstruction of the compressed sentence. And there is one shift operation.
            Given a tree t and an arbitrary conﬁguration of the stack and input list, the purpose of the decision-based
            classiﬁer is to learn what action to choose from the set of 210 possible actions. To each learning example,
            we associated a set of 99 features from the following two classes: Operational features reﬂect the number of
            trees in the stack, the input list, and the types of the last ﬁve operations. They also encode information
            that denote the syntactic category of the root nodes of the partial trees built up to a certain time.
            Examples of such features are: numberTreesInStack, wasPreviousOperationShift,
            syntacticLabelOfTreeAtTheTopOfStack, etc. Original-tree-speciﬁc features denote the syntactic constituents
            that start with the ﬁrst unit in the input list. Examples of such features are: inputListStartsWithA CC,
            inputListStartsWithA PP, etc. The decision-based compression module uses the C4.5 program (Quinlan 1993) in
            order to learn decision trees that specify how large syntactic trees can be compressed into shorter trees. A
            ten-fold crossvalidation evaluation of the classiﬁer yielded an accuracy of 87.16% (± 0.14). A majority
            baseline classiﬁer that chooses the action shift has an accuracy of 28.72%.
            Employing the decision-based model
            To compress sentences, we apply the shift-reduce-drop model in a deterministic fashion. We parse the
            sentence to be compressed (Collins 1997) and we initialize the input list with the words in the sentence and
            the syntactic constituents that “begin” at each word, as shown in Figure 4. We then incrementally inquire
            the learned classiﬁer what action to perform, and we simulate the execution of that action. The procedure
            ends when the input list is empty and when the stack contains only one tree. An inorder traversal of the
            leaves of this tree produces the compressed version of the sentence given as input. Since the model is
            deterministic, it produces only one output. The advantage is that the compression is very fast: it takes
            only a few milliseconds per sentence. The disadvantage is that it does not produce a range of compressions,
            from which another system may subsequently choose. It is straightforward though to extend the model within a
            probabilistic framework by applying, for example, the techniques used by Magerman (1995).
            Evaluation
            To evaluate our compression algorithms, we randomly selected 32 sentence pairs from our parallel corpus,
            which we will refer to as the Test Corpus. We used the other 1035 sentence pairs for training. Figure 5
            shows three sentences from the Test Corpus, together with the compressions produced by humans, our
            compression algorithms, and a baseline algorithm that produces compressions with highest word-bigram scores.
            The examples are chosen so as to reﬂect good, average, and bad performance cases. The ﬁrst sentence is
            compressed in the same manner by humans and our algorithms (the baseline algorithm chooses though not to
            compress this sentence). For the second example, the output of the Decision-based algorithm is grammatical,
            but the semantics is negatively aﬀected. The noisy-channel algorithm deletes only the word “break”, which
            aﬀects the correctness of the output less. In the last example, the noisy-channel model is again more
            conservative and decides not to drop any constituents. In constrast, the decision-based algorithm compresses
            the input substantially, but it fails to produce a grammatical output. We presented each original sentence
            in the Test Corpus to four judges, together with four compressions of it: the human generated compression,
            the outputs of the noisy-channel and decision-based algorithms, and the output of the baseline algorithm.
            The judges were told that all outputs were generated automatically. The order of the outputs was scrambled
            randomly across test cases. To avoid confounding, the judges participated in two experiments. In the ﬁrst
            experiment, they were asked to determine on a scale from 1 to 5 how well the systems did with respect to
            selecting the most important words in the original sentence. In the second experiment, they were asked to
            determine on a scale from 1 to 5 how grammatical the outputs were. We also investigated how sensitive our
            algorithms are with respect to the training data by carrying out the same experiments on sentences of a
            diﬀerent genre, the scientiﬁc one. To this end, we took the ﬁrst sentence of the ﬁrst 26 articles made
            available in 1999 on the cmplg archive. We created a second parallel corpus, which we will refer to as the
            Cmplg Corpus, by generating by ourselves compressed grammatical versions of these sentences. Since some of
            the sentences in this corpus were extremely long, the baseline algorithm could not produce compressed
            versions in reasonable time. The results in Table 1 show compression rates, and mean and standard deviation
            results across all judges, for each algorithm and corpus. The results show that the decision-based algorithm
            is the most aggressive: on average, it compresses sentences to about half of their original size. The
            compressed sentences produced by both algorithms are more “grammatical” and contain more important words
            than the sentences produced by the baseline. T -test experiments showed these diﬀerences to be statistically
            signiﬁcant at p
            <
            0.01 both for individual judges and for average scores across
            Original:
            Beyond the basic level, the operations of the three products vary widely. Baseline: Beyond the basic level,
            the operations of the three products vary widely. Noisy-channel: The operations of the three products vary
            widely. Decision-based: The operations of the three products vary widely. Humans: The operations of the
            three products vary widely. Original: Arborscan is reliable and worked accurately in testing, but it
            produces very large dxf ﬁles. Baseline: Arborscan and worked in, but it very large dxf. Noisy-channel:
            Arborscan is reliable and worked accurately in testing, but it produces very large dxf ﬁles. Decision-based:
            Arborscan is reliable and worked accurately in testing very large dxf ﬁles. Humans: Arborscan produces very
            large dxf ﬁles. Original:
            Many debugging features, including user-deﬁned break points and variable-watching and
            message-watching windows, have been added. Baseline: Debugging, user-deﬁned and variable-watching and
            message-watching, have been. Noisy-channel: Many debugging features, including user-deﬁned points and
            variable-watching and message-watching windows, have been added. Decision-based: Many debugging features.
            Humans: Many debugging features have been added .
            Figure 5: Compression examples
            Corpus Test
            Avg. orig. sent. length
            21 words
            Cmplg

            26 words

            Compression
            Grammaticality Importance Compression Grammaticality Importance
            Baseline
            63.70% 1.78±1.19 2.17±0.89 – – –
            Noisy-channel
            70.37% 4.34±1.02 3.38±0.67 65.68% 4.22±0.99 3.42±0.97
            Decision-based
            57.19% 4.30±1.33 3.54±1.00 54.25% 3.72±1.53 3.24±0.68
            Humans
            53.33% 4.92±0.18 4.24 ±0.52 65.68% 4.97±0.08 4.32±0.54
            Table 1: Experimental results
            all judges. T -tests showed no signiﬁcant statistical diﬀerences between the two algorithms. As Table 1
            shows, the performance of the compression algorithms is much closer to human performance than baseline
            performance; yet, humans perform statistically better than our algorithms at p
            <
            0.01. When applied to sentences of a diﬀerent genre, the performance of the noisy-channel compression
            algorithm degrades smoothly, while the performance of the decision-based algorithm drops sharply. This is
            due to a few sentences in the Cmplg Corpus that the decisionbased algorithm over-compressed to only two or
            three words. We suspect that this problem can be ﬁxed if the decision-based compression module is extended
            in the style of Magerman (1995), by computing probabilities across the sequences of decisions that
            correspond to a compressed sentence. Likewise, there are substantial gains to be had in noisy-channel
            modeling—we see clearly in the data many statistical dependencies and processes that are not captured in our
            simple initial models. More grammatical output will come from taking account of subcategory and head-modiﬁer
            statistics (in addition to simple word-bigrams), and an expanded channel model will allow for more tree
            manipulation possibilities. Work on extending the algorithms presented in this paper to compressing multiple
            sentences is currently underway.
        </corps>
        <conclusion>Aucune conclusion trouvée.</conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>Barzilay, R.; McKeown, K.; and Elhadad, M. 1999. Information fusion in the context of multi-document
            summarization. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics
            (ACL–99), 550–557. Berger, A., and Laﬀerty, J. 1999. Information retrieval as statistical translation. In
            Proceedings of the 22nd Conference on Research and Development in Information Retrieval (SIGIR–99), 222–229.
            Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mercer, R. 1993. The mathematics of statistical machine
            translation: Parameter estimation. Computational Linguistics 19(2):263–311. Church, K. 1988. A stochastic
            parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on
            Applied Natural Language Processing, 136–143. Collins, M. 1997. Three generative, lexicalized models for
            statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational
            Linguistics (ACL–97), 16–23. Grefenstette, G. 1998. Producing intelligent telegraphic text reduction to
            provide an audio scanning service for the blind. In Working Notes of the AAAI
            Spring Symposium on Intelligent Text Summarization,
            111–118. Jelinek, F. 1997. Statistical Methods for Speech Recognition. The MIT Press. Jing, H., and McKeown,
            K. 1999. The decomposition of human-written summary sentences. In Proceedings of the 22nd Conference on
            Research and Development in Information Retrieval (SIGIR–99). Knight, K., and Graehl, J. 1998. Machine
            transliteration. Computational Linguistics 24(4):599–612. Langkilde, I. 2000. Forest-based statistical
            sentence generation. In Proceedings of the 1st Annual Meeting of the North American Chapter of the
            Association for Computational Linguistics. Linke-Ellis, N. 1999. Closed captioning in America: Looking
            beyond compliance. In Proceedings of the TAO Workshop on TV Closed Captions for the hearing impaired people,
            43–59. Magerman, D. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual
            Meeting of the Association for Computational Linguistics, 276– 283. Mani, I., and Maybury, M., eds. 1999.
            Advances in Automatic Text Summarization. The MIT Press. Mani, I.; Gates, B.; and Bloedorn, E. 1999.
            Improving summaries by revising them. In Proceedings of the 37th Annual Meeting of the Association for
            Computational Linguistics, 558–565. McKeown, K.; Klavans, J.; Hatzivassiloglou, V.; Barzilay, R.; and Eskin,
            E. 1999. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of the
            Sixteenth National Conference on Artiﬁcial Intelligence (AAAI–99). Quinlan, J. 1993. C4.5: Programs for
            Machine Learning. San Mateo, CA: Morgan Kaufmann Publishers. Robert-Ribes, J.; Pfeiﬀer, S.; Ellison, R.; and
            Burnham, D. 1999. Semi-automatic captioning of TV programs, an Australian perspective. In Proceedings of the
            TAO Workshop on TV Closed Captions for the hearing impaired people, 87–100. Witbrock, M., and Mittal, V.
            1999. Ultrasummarization: A statistical approach to generating highly condensed non-extractive summaries. In
            Proceedings of the 22nd International Conference on Research and Development in Information Retrieval
            (SIGIR’99), Poster Session, 315–316.
        </biblio>
    </article>
    <article>
        <preamble>probabilistic_sentence_reduction.txt</preamble>
        <titre>Probabilistic Sentence Reduction Using Support Vector Machines</titre>
        <auteur>Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi</auteur>
        <abstract>This paper investigates a novel application of support vector machines (SVMs) for sentence reduction.
            We also propose a new probabilistic sentence reduction method based on support vector machine learning.
            Experimental results show that the proposed methods outperform earlier methods in term of sentence reduction
            performance. 1
        </abstract>
        <introduction>The most popular methods of sentence reduction for text summarization are corpus based methods.
            Jing (Jing 00) developed a method to remove extraneous phrases from sentences by using multiple sources of
            knowledge to decide which phrases could be removed. However, while this method exploits a simple model for
            sentence reduction by using statistics computed from a corpus, a better model can be obtained by using a
            learning approach. Knight and Marcu (Knight and Marcu 02) proposed a corpus based sentence reduction method
            using machine learning techniques. They discussed a noisy-channel based approach and a decision tree based
            approach to sentence reduction. Their algorithms provide the best way to scale up the full problem of
            sentence reduction using available data. However, these algorithms require that the word order of a given
            sentence and its reduced sentence are the same. Nguyen and Horiguchi (Nguyen and Horiguchi 03) presented a
            new sentence reduction technique based on a decision tree model without that constraint. They also indicated
            that semantic information is useful for sentence reduction tasks. The major drawback of previous works on
            sentence reduction is that those methods are likely to output local optimal results, which may have lower
            accuracy. This problem is caused by the inherent sentence reduction model; that is, only a single reduced
            sentence can be obtained. As pointed out by Lin (Lin 03), the best sentence reduction output for a single
            sentence is not approximately best for text summarization. This means that “local optimal” refers to the
            best reduced output for a single sentence, while the best reduced output for the whole text is “global
            optimal”. Thus, it would be very valuable if the sentence reduction task could generate multiple reduced
            outputs and select the best one using the whole text document. However, such a sentence reduction method has
            not yet been proposed. Support Vector Machines (Vapnik 95), on the other hand, are strong learning methods
            in comparison with decision tree learning and other learning methods (Sholkopf 97). The goal of this paper
            is to illustrate the potential of SVMs for enhancing the accuracy of sentence reduction in comparison with
            previous work. Accordingly, we describe a novel deterministic method for sentence reduction using SVMs and a
            twostage method using pairwise coupling (Hastie 98). To solve the problem of generating multiple best
            outputs, we propose a probabilistic sentence reduction model, in which a variant of probabilistic SVMs using
            a two-stage method with pairwise coupling is discussed. The rest of this paper will be organized as follows:
            Section 2 introduces the Support Vector Machines learning. Section 3 describes the previous work on sentence
            reduction and our deterministic sentence reduction using SVMs. We also discuss remaining problems of
            deterministic sentence reduction. Section 4 presents a probabilistic sentence reduction method using support
            vector machines to solve this problem. Section 5 discusses implementation and our experimental results;
            Section 6 gives our conclusions and describes some problems that remain to be solved in the future.
        </introduction>
        <corps>not approximately best for text summarization. This means that “local optimal” refers to the best reduced
            output for a single sentence, while the best reduced output for the whole text is “global optimal”. Thus, it
            would be very valuable if the sentence reduction task could generate multiple reduced outputs and select the
            best one using the whole text document. However, such a sentence reduction method has not yet been proposed.
            Support Vector Machines (Vapnik 95), on the other hand, are strong learning methods in comparison with
            decision tree learning and other learning methods (Sholkopf 97). The goal of this paper is to illustrate the
            potential of SVMs for enhancing the accuracy of sentence reduction in comparison with previous work.
            Accordingly, we describe a novel deterministic method for sentence reduction using SVMs and a twostage
            method using pairwise coupling (Hastie 98). To solve the problem of generating multiple best outputs, we
            propose a probabilistic sentence reduction model, in which a variant of probabilistic SVMs using a two-stage
            method with pairwise coupling is discussed. The rest of this paper will be organized as follows: Section 2
            introduces the Support Vector Machines learning. Section 3 describes the previous work on sentence reduction
            and our deterministic sentence reduction using SVMs. We also discuss remaining problems of deterministic
            sentence reduction. Section 4 presents a probabilistic sentence reduction method using support vector
            machines to solve this problem. Section 5 discusses implementation and our experimental results; Section 6
            gives our conclusions and describes some problems that remain to be solved in the future.
            2

            Support Vector Machine

            Support vector machine (SVM)(Vapnik 95) is a
            technique of machine learning based on statistical learning theory. Suppose that we are given l training
            examples (xi , yi ), (1 ≤ i ≤ l), where xi is a feature vector in n dimensional feature space, yi is the
            class label {-1, +1 } of xi . SVM finds a hyperplane w.x + b = 0 which correctly separates the training
            examples and has a maximum margin which is the distance between two hyperplanes w.x + b ≥ 1 and w.x + b ≤
            −1. The optimal hyperplane with maximum margin can be obtained by solving the following quadratic
            programming. 1 2 kwk + C0
            min

            l
            P i
            ξi

            (1)

            s.t.
            yi (w.xi + b) ≥ 1 − ξi ξi ≥ 0
            where C0 is the constant and ξi is a slack variable for the non-separable case. In SVM, the
            optimal hyperplane is formulated as follows: f (x) = sign
            Ã l
            X
            !
            αi yi K(xi , x) + b
            (2)

            1

            where αi is the Lagrange multiple, and
            K(x0 , x00 ) is a kernel function, the SVM calculates similarity between two arguments x0 and x00 . For
            instance, the Polynomial kernel function is formulated as follow: K(x0 , x00 ) = (x0 .x00 )p
            (3)

            SVMs estimate the label of an unknown example x whether the sign of f (x) is positive or
            not.
            3

            Deterministic Sentence Reduction
            Using SVMs
            3.1 Problem Description
            In the corpus-based decision tree approach, a given input sentence is parsed into a syntax tree and the
            syntax tree is then transformed into a small tree to obtain a reduced sentence. Let t and s be syntax trees
            of the original sentence and a reduced sentence, respectively. The process of transforming syntax tree t to
            small tree s is called “rewriting process” (Knight and Marcu 02), (Nguyen and Horiguchi 03). To transform
            the syntax tree t to the syntax tree s, some terms and five rewriting actions are defined. An Input list
            consists of a sequence of words subsumed by the tree t where each word in the Input list is labelled with
            the name of all syntactic constituents in t. Let CSTACK be a stack
            that consists of sub trees in order to rewrite a
            small tree. Let RSTACK be a stack that consists of sub trees which are removed from the Input list in the
            rewriting process. • SHIFT action transfers the first word from the Input list into CSTACK. It is written
            mathematically and given the label SHIFT. • REDUCE(lk, X) action pops the lk syntactic trees located at the
            top of CSTACK and combines them in a new tree, where lk is an integer and X is a grammar symbol. • DROP X
            action moves subsequences of words that correspond to syntactic constituents from the Input list to RSTACK.
            • ASSIGN TYPE X action changes the label of trees at the top of the CSTACK. These POS tags might be
            different from the POS tags in the original sentence. • RESTORE X action takes the X element in RSTACK and
            moves it into the Input list, where X is a subtree.
            For convenience, let configuration be a status
            of Input list, CSTACK and RSTACK. Let current context be the important information in a configuration. The
            important information are defined as a vector of features using heuristic methods as in (Knight and Marcu
            02), (Nguyen and Horiguchi 03). The main idea behind deterministic sentence reduction is that it uses a rule
            in the current context of the initial configuration to select a distinct action in order to rewrite an input
            sentence into a reduced sentence. After that, the current context is changed to a new context and the
            rewriting process is repeated for selecting an action that corresponds to the new context. The rewriting
            process is finished when it meets a termination condition. Here, one rule corresponds to the function that
            maps the current context to a rewriting action. These rules are learned automatically from the corpus of
            long sentences and their reduced sentences (Knight and Marcu 02), (Nguyen and Horiguchi 03). 3.2 Example
            Figure 1 shows an example of applying a sequence of actions to rewrite the input sentence (a, b, c, d, e),
            when each character is a word. It illustrates the structure of the Input list, two stacks, and the term of a
            rewriting process based on the actions mentioned above. For example, in the first row, DROP H deletes the
            sub-tree with its root node H in the Input list and stores
            it in the RSTACK. The reduced tree s can be
            obtained after applying a sequence of actions as follows: DROP H; SHIFT; ASSIGN TYPE K; DROP B; SHIFT;
            ASSIGN TYPE H; REDUCE 2 F; RESTORE H; SHIFT; ASSIGN TYPE D; REDUCE 2G. In this example, the reduced sentence
            is (b, e, a).

            Figure 2: Example of Configuration
            that start with the first unit in the Input list.
            Figure 1: An Example of the Rewriting Process
            3.3
            Learning Reduction Rules Using
            SVMs As mentioned above, the action for each configuration can be decided by using a learning rule, which
            maps a context to an action. To obtain such rules, the configuration is represented by a vector of features
            with a high dimension. After that, we estimate the training examples by using several support vector
            machines to deal with the multiple classification problem in sentence reduction. 3.3.1 Features One
            important task in applying SVMs to text summarization is to define features. Here, we describe features used
            in our sentence reduction models. The features are extracted based on the current context. As it can be seen
            in Figure 2, a context includes the status of the Input list and the status of CSTACK and RSTACK. We define
            a set of features for a current context as described bellow. Operation feature The set of features as
            described in (Nguyen and Horiguchi 03) are used in our sentence reduction models. Original tree features
            These features denote the syntactic constituents
            For example, in Figure 2 the syntactic constituents are labels of the current element in the
            Input list from “VP” to the verb “convince”. Semantic features The following features are used in our model
            as semantic information. • Semantic information about current words within the Input list; these semantic
            types are obtained by using the named entities such as Location, Person, Organization and Time within the
            input sentence. To define these name entities, we use the method described in (Borthwick 99). • Semantic
            information about whether or not the word in the Input list is a head word. • Word relations, such as
            whether or not a word has a relationship with other words in the subcategorization table. These relations
            and the sub-categorization table are obtained using the Commlex database (Macleod 95).
            Using the semantic information, we are able to
            avoid deleting important segments within the given input sentence. For instance, the main verb, the subject
            and the object are essential and for the noun phrase, the head noun is essential, but an adjective modifier
            of the head noun is not. For example, let us consider that the verb “convince” was extracted from the Comlex
            database as follows. convince NP-PP: PVAL (“of”) NP-TO-INF-OC
            This entry indicates that the verb “convince”

            can be followed by a noun phrase and a prepositional phrase starting with the preposition “of”.
            It can be also followed by a noun phrase and a to-infinite phrase. This information shows that we cannot
            delete an “of” prepositional phrase or a to-infinitive that is the part of the verb phrase. Two-stage SVM
            Learning using Pairwise Coupling Using these features we can extract training data for SVMs. Here, a sample
            in our training data consists of pairs of a feature vector and an action. The algorithm to extract training
            data from the training corpus is modified using the algorithm described in our pervious work (Nguyen and
            Horiguchi 03). Since the original support vector machine (SVM) is a binary classification method, while the
            sentence reduction problem is formulated as multiple classification, we have to find a method to adapt
            support vector machines to this problem. For multi-class SVMs, one can use strategies such as one-vs all,
            pairwise comparison or DAG graph (Hsu 02). In this paper, we use the pairwise strategy, which constructs a
            rule for discriminating pairs of classes and then selects the class with the most winning among two class
            decisions. To boost the training time and the sentence reduction performance, we propose a two-stage SVM
            described below. Suppose that the examples in training data are divided into five groups m1 , m2 , ..., m5
            according to their actions. Let Svmc be multiclass SVMs and let Svmc-i be multi-class SVMs for a group mi .
            We use one Svmc classifier to identify the group to which a given context e should be belong. Assume that e
            belongs to the group mi . The classifier Svmc-i is then used to recognize a specific action for the context
            e. The five classifiers Svmc-1, Svmc-2,..., Svmc-5 are trained by using those examples which have actions
            belonging to SHIFT, REDUCE, DROP, ASSIGN TYPE and RESTORE. Table 1 shows the distribution of examples in
            five data groups.
            Table 1: Distribution of example data on five
            data groups Name SHIFT-GROUP REDUCE-GROUP DROP-GROUP ASSIGN-GROUP RESTORE-GROUP TOTAL
            3.3.2

            3.4

            Disadvantage of Deterministic
            Sentence Reductions The idea of the deterministic algorithm is to use the rule for each current context to
            select the next action, and so on. The process terminates when a stop condition is met. If the early steps
            of this algorithm fail to select the best ac-
            Number of examples
            13,363 11,406 4,216 13,363 2,004 44,352
            tions, then the possibility of obtaining a wrong
            reduced output becomes high. One way to solve this problem is to select multiple actions that correspond to
            the context at each step in the rewriting process. However, the question that emerges here is how to
            determine which criteria to use in selecting multiple actions for a context. If this problem can be solved,
            then multiple best reduced outputs can be obtained for each input sentence and the best one will be selected
            by using the whole text document. In the next section propose a model for selecting multiple actions for a
            context in sentence reduction as a probabilistic sentence reduction and present a variant of probabilistic
            sentence reduction.
            4

            Probabilistic Sentence Reduction
            Using SVM
            4.1
            The Probabilistic SVM Models Let A be a set of k actions A = {a1 , a2 ...ai , ..., ak } and C be a set of n
            contexts C = {c1 , c2 ...ci , ..., cn } . A probabilistic model α for sentence reduction will select an
            action a ∈ A for the context c with probability pα (a|c). The pα (a|c) can be used to score action a among
            possible actions A depending the context c that is available at the time of decision. There are several
            methods for estimating such scores; we have called these “probabilistic sentence reduction methods”. The
            conditional probability pα (a|c) is estimated using a variant of probabilistic support vector machine, which
            is described in the following sections. 4.1.1 Probabilistic SVMs using Pairwise Coupling For convenience, we
            denote uij = p(a = ai |a = ai ∨aj , c). Given a context c and an action a, we assume that the estimated
            pairwise class probabilities rij of uij are available. Here rij can be estimated by some binary classifiers.
            For instance, we could estimate rij by using the
            SVM binary posterior probabilities as described
            in (Plat 2000). Then, the goal is to estimate {pi }ki=1 , where pi = p(a = ai |c), i = 1, 2, ..., k. For
            this propose, a simple estimate of these probabilities can be derived using the following voting method: X
            pi = 2

            the search procedure ranks the derivation of incomplete and complete reduced sentences. Let
            d(s) = {a1 , a2 , ...ad } be the derivation of a small tree s, where each action ai belongs to a set of
            possible actions. The score of s is the product of the conditional probabilities of the individual actions
            in its derivation.
            I{rij >rji } /k(k − 1)

            X

            nij rij log

            i6=j

            rij
            uij
            (4)

            where rij and uij are the probabilities of a pairwise ai and aj in the estimated model and in
            our model, respectively, and nij is the number of training data in which their classes are ai or aj . To
            find the minimizer of equation (6), they first calculate ∂l(p) X rij 1 = nij (− + ). ∂pi pi pi + pj i6=j
            Thus, letting ∆l(p) = 0, they proposed to find
            a point satisfying X j:j6=i
            nij uij =

            X
            j:j6=i
            nij rij ,

            k
            P i=1
            pi = 1,

            where i = 1, 2, ...k and pi > 0.
            Such a point can be obtained by using an algorithm described elsewhere in (Hastie 98). We applied it to
            obtain a probabilistic SVM model for sentence reduction using a simple method as follows. Assume that our
            class labels belong to l groups: M = {m1 , m2 ...mi , ..., ml } , where l is a number of groups and mi is a
            group e.g., SHIFT, REDUCE ,..., ASSIGN TYPE. Then the probability p(a|c) of an action a for a given context
            c can be estimated as follows. p(a|c) = p(mi |c) × p(a|c, mi )
            (5)

            where mi is a group and a ∈ mi . Here, p(mi |c)
            and p(a|c, mi ) are estimated by the method in (Hastie 98). 4.2
            p(ai |ci )

            (6)

            ai ∈d(s)

            where I is an indicator function and k(k − 1) is
            the number of pairwise classes. However, this model is too simple; we can obtain a better one with the
            following method. Assume that uij are pairwise probabilities of the model subject to the condition that uij
            = pi /(pi +pj ). In (Hastie 98), the authors proposed to minimize the Kullback-Leibler (KL) distance between
            the rij and uij l(p) =
            Y

            Score(s) =

            j:j6=i

            Probabilistic sentence reduction
            algorithm After obtaining a probabilistic model p, we then use this model to define function score, by which
            where ci is the context in which ai was decided.
            The search heuristic tries to find the best reduced tree s∗ as follows: s∗ = argmax | {z }
            Score(s)

            (7)

            s∈tree(t)

            where tree(t) are all the complete reduced trees
            from the tree t of the given long sentence. Assume that for each configuration the actions {a1 , a2 , ...an
            } are sorted in decreasing order according to p(ai |ci ), in which ci is the context of that configuration.
            Algorithm 1 shows a probabilistic sentence reduction using the top K-BFS search algorithm. This algorithm
            uses a breadth-first search which does not expand the entire frontier, but instead expands at most the top K
            scoring incomplete configurations in the frontier; it is terminated when it finds M completed reduced
            sentences (CL is a list of reduced trees), or when all hypotheses have been exhausted. A configuration is
            completed if and only if the Input list is empty and there is one tree in the CSTACK. Note that the function
            get-context(hi , j) obtains the current context of the j th configuration in hi , where hi is a heap at step
            i. The function Insert(s,h) ensures that the heap h is sorted according to the score of each element in h.
            Essentially, in implementation we can use a dictionary of contexts and actions observed from the training
            data in order to reduce the number of actions to explore for a current context.
            5
        </corps>
        <conclusion>Conclusions

            We have presented a new probabilistic sentence
            reduction approach that enables a long sentence to be rewritten into reduced sentences based on support
            vector models. Our methods achieves better performance when compared with earlier methods. The proposed
            reduction approach can generate multiple best outputs. Experimental results showed that the top 10 reduced
            sentences returned by the reduction process might yield accuracies higher than previous work. We believe
            that a good ranking method might improve the sentence reduction performance further in a text.
        </conclusion>
        <discussion>Experiments and Discussion

            We used the same corpus as described in
            (Knight and Marcu 02), which includes 1,067 pairs of sentences and their reductions. To evaluate sentence
            reduction algorithms, we randomly selected 32 pairs of sentences from our parallel corpus, which is refered
            to as the test corpus. The training corpus of 1,035 sentences extracted 44,352 examples, in which each
            training example corresponds to an action. The SVM tool, LibSVM (Chang 01) is applied to train our model.
            The training examples were
            Algorithm 1 A probabilistic sentence reduction
            algorithm 1: CL={Empty}; i = 0; h0 ={ Initial configuration}
            2: while |CL|
            < M do
            3: if hi is empty then 4: break; 5: end if 6: u =min(|hi |, K) 7: for j = 1 to u do 8: c=get-context(hi , j)
            9: 10: 11: 12:
            Select m so that

            m
            P
            p(ai |c)< Q is maximal

                      i=1

            for l=1 to m do
            parameter=get-parameter(al ); Obtain a new configuration s by performing action al with parameter
            if Complete(s) then
            13: 14: Insert(s, CL) 15: else 16: Insert(s, hi+1 ) 17: end if 18: end for 19: end for 20: i=i+1 21: end
            while
            divided into SHIFT, REDUCE, DROP, RESTORE, and ASSIGN groups. To train our
            support vector model in each group, we used the pairwise method with the polynomial kernel function, in
            which the parameter p in (3) and the constant C0 in equation (1) are 2 and 0.0001, respectively. The
            algorithms (Knight and Marcu 02) and (Nguyen and Horiguchi 03) served as the baseline1 and the baseline2 for
            comparison with the proposed algorithms. Deterministic sentence reduction using SVM and probabilistic
            sentence reduction were named as SVM-D and SVMP, respectively. For convenience, the ten top reduced outputs
            using SVMP were called SVMP-10. We used the same evaluation method as described in (Knight and Marcu 02) to
            compare the proposed methods with previous methods. For this experiment, we presented each original sentence
            in the test corpus to three judges who are specialists in English, together with three sentence reductions:
            the human generated reduction sentence, the outputs of the proposed algorithms, and the output of the
            baseline algorithms. The judges were told that all outputs were generated automatically. The order of the
            outputs was scrambled randomly across test cases. The judges participated in two experiments. In the first,
            they were asked to determine on a scale from 1 to 10 how well the systems did with respect to selecting the
            most important words in the original sentence. In the second, they were asked to determine the grammatical
            criteria of
            reduced sentences.
            Table 2 shows the results of English language sentence reduction using a support vector machine compared
            with the baseline methods and with human reduction. Table 2 shows compression rates, and mean and standard
            deviation results across all judges, for each algorithm. The results show that the length of the reduced
            sentences using decision trees is shorter than using SVMs, and indicate that our new methods outperform the
            baseline algorithms in grammatical and importance criteria. Table 2 shows that the Table 2: Experiment
            results with Test Corpus Method Baseline1 Baseline2 SVM-D SVMP-10 Human
            Comp
            57.19% 57.15% 57.65% 57.51% 64.00%
            Gramma
            8.60 ± 2.8 8.60 ± 2.1 8.76 ± 1.2 8.80 ± 1.3 9.05 ± 0.3
            Impo
            7.18 ± 1.92 7.42 ± 1.90 7.53 ± 1.53 7.74 ± 1.39 8.50 ± 0.80
            first 10 reduced sentences produced by SVMP10 (the SVM probabilistic model) obtained the
            highest performances. We also compared the computation time of sentence reduction using support vector
            machine with that in previous works. Table 3 shows that the computational times for SVM-D and SVMP-10 are
            slower than baseline, but it is acceptable for SVM-D. Table 3: Computational times of performing reductions
            on test-set. Average sentence length was 21 words. Method Baseline1 SVM-D SVMP-10
            Computational times (sec)
            138.25 212.46 1030.25
            We also investigated how sensitive the proposed algorithms are with respect to the training data by carrying
            out the same experiment on sentences of different genres. We
            created the test corpus by selecting sentences from the web-site of the Benton Foundation
            (http://www.benton.org). The leading sentences in each news article were selected as the most relevant
            sentences to the summary of the news. We obtained 32 leading long sentences and 32 headlines for each item.
            The 32 sentences are used as a second test for our methods. We use a simple ranking criterion: the more the
            words in the reduced sentence overlap with the words in the headline, the more important the
            sentence is. A sentence satisfying this criterion
            is called a relevant candidate. For a given sentence, we used a simple method, namely SVMP-R to obtain a
            reduced sentence by selecting a relevant candidate among the ten top reduced outputs using SVMP-10. Table 4
            depicts the experiment results for the baseline methods, SVM-D, SVMP-R, and SVMP-10. The results shows that,
            when applied to sentence of a different genre, the performance of SVMP-10 degrades smoothly, while the
            performance of the deterministic sentence reductions (the baselines and SVM deterministic) drops sharply.
            This indicates that the probabilistic sentence reduction using support vector machine is more stable. Table
            4 shows that the performance of SVMP-10 is also close to the human reduction outputs and is better than
            previous works. In addition, SVMP-R outperforms the deterministic sentence reduction algorithms and the
            differences between SVMP-R’s results and SVMP10 are small. This indicates that we can obtain reduced
            sentences which are relevant to the headline, while ensuring the grammatical and the importance criteria
            compared to the original sentences. Table 4: Experiment results with Benton Corpus Method Baseline1
            Baseline2 SVM-D SVMP-R SVMP-10 Human
            6

            Comp
            54.14% 53.13% 56.64% 58.31% 57.62% 64.00%
            Gramma
            7.61 ± 2.10 7.72 ± 1.60 7.86 ± 1.20 8.25 ± 1.30 8.60 ± 1.32 9.01 ± 0.25
            Impo
            6.74 ± 1.92 7.02 ± 1.90 7.23 ± 1.53 7.54 ± 1.39 7.71 ± 1.41 8.40 ± 0.60
        </discussion>
        <biblio>A. Borthwick, “A Maximum Entropy Approach to Named Entity Recognition”, Ph.D thesis, Computer Science
            Department, New York University (1999). C.-C. Chang and C.-J. Lin, “LIBSVM: a library for support vector
            machines”, Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm. H. Jing, “Sentence reduction for
            automatic text summarization”, In Proceedings of the First Annual Meeting of the North American Chapter of
            the Association for Computational Linguistics NAACL-2000. T.T. Hastie and R. Tibshirani, “Classification by
            pairwise coupling”, The Annals of Statistics, 26(1): pp. 451-471, 1998. C.-W. Hsu and C.-J. Lin, “A
            comparison of methods for multi-class support vector machines”, IEEE Transactions on Neural Networks, 13,
            pp. 415-425, 2002. K. Knight and D. Marcu, “Summarization beyond sentence extraction: A Probabilistic
            approach to sentence compression”, Artificial Intelligence 139: pp. 91-107, 2002. C.Y. Lin, “Improving
            Summarization Performance by Sentence Compression — A Pilot Study”, Proceedings of the Sixth International
            Workshop on Information Retrieval with Asian Languages, pp.1-8, 2003. C. Macleod and R. Grishman, “COMMLEX
            syntax Reference Manual”; Proteus Project, New York University (1995). M.L. Nguyen and S. Horiguchi, “A new
            sentence reduction based on Decision tree model”, Proceedings of 17th Pacific Asia Conference on Language,
            Information and Computation, pp. 290-297, 2003 V. Vapnik, “The Natural of Statistical Learning Theory”, New
            York: Springer-Verlag, 1995. J. Platt,“ Probabilistic outputs for support vector machines and comparison to
            regularized likelihood methods,” in Advances in Large Margin Classifiers, Cambridege, MA: MIT Press, 2000.
            B. Scholkopf et al, “Comparing Support Vector Machines with Gausian Kernels to Radius Basis Function
            Classifers”, IEEE Trans. Signal Procesing, 45, pp. 2758-2765, 1997.
        </biblio>
    </article>
    <article>
        <preamble>Stolcke_1996_Automatic_linguistic.txt</preamble>
        <titre>AUTOMATIC LINGUISTIC SEGMENTATION OF CONVERSATIONAL SPEECH</titre>
        <auteur>Andreas Stolcke</auteur>
        <abstract>As speech recognition moves toward more unconstrained domains such as conversational speech, we
            encounter a need to be able to segment (or resegment) waveforms and recognizer output into linguistically
            meaningful units, such a sentences. Toward this end, we present a simple automatic segmenter of transcripts
            based on N-gram language modeling. We also study the relevance of several word-level features for
            segmentation performance. Using only word-level information, we achieve 85% recall and 70% precision on
            linguistic boundary detection. 1.
        </abstract>
        <introduction>Today’s large-vocabulary speech recognizers typically prefer to process a few tens of seconds of
            speech at a time, to keep the time and memory demands of the decoder within bounds. For longer inputs, the
            waveform is usually presegmented into shorter pieces based on simple acoustic criteria, such as nonspeech
            intervals (e.g., pauses) and turn boundaries (when several speakers are involved). We refer to such
            segmentations as acoustic segmentations. Acoustic segmentations generally do not reflect the linguistic
            structure of utterances. They may fragment sentences or semantic units, or group together spans of unrelated
            units. We examine several reasons why such behavior is undesirable, and propose that linguistic
            segmentations be used instead. This requires algorithms for automatically finding linguistic units. In this
            paper we report on first results from our ongoing efforts toward such an automatic linguistic segmentation.
            In all further discussion, unless otherwise noted, the terms ‘segment,’ ‘segmentation,’ etc. will refer to
            linguistic segmentations.
        </introduction>
        <corps>memory demands of the decoder within bounds. For longer inputs, the waveform is usually presegmented into
            shorter pieces based on simple acoustic criteria, such as nonspeech intervals (e.g., pauses) and turn
            boundaries (when several speakers are involved). We refer to such segmentations as acoustic segmentations.
            Acoustic segmentations generally do not reflect the linguistic structure of utterances. They may fragment
            sentences or semantic units, or group together spans of unrelated units. We examine several reasons why such
            behavior is undesirable, and propose that linguistic segmentations be used instead. This requires algorithms
            for automatically finding linguistic units. In this paper we report on first results from our ongoing
            efforts toward such an automatic linguistic segmentation. In all further discussion, unless otherwise noted,
            the terms ‘segment,’ ‘segmentation,’ etc. will refer to linguistic segmentations.
            2. THE IMPORTANCE OF LINGUISTIC
            SEGMENTATION Acoustic segmentations are inadequate in cases where the output of a speech recognizer is to
            serve as input for further processing based on syntactically or semantically coherent units. This includes
            most natural language (NL) parsers or NL understanding or translation systems. For such systems, the
            fragmented recognition output would have to be put back together and large spans of unrelated material would
            need to be resegmented into linguistic units. Automatic detection of linguistic segments could also improve
            the user interface of many speech systems. A spoken language system
            could use the knowledge incorporated in an automatic segmenter
            to help end-point a user’s speech input. A speech indexing and retrieval system (such as for transcribed
            broadcast audio) could process its data in more meaningful units if the locations of linguistic segment
            boundaries were known. Our main motivation for the work reported here comes from speech language modeling.
            Experiments at the 1995 Johns Hopkins Language Modeling Workshop showed that the quality of a language model
            (LM) can be improved if both training and test data are segmented linguistically, rather than acoustically
            [8]. We showed in [10] and [9] that proper modeling of filled pauses requires knowledge of linguistic
            segment boundaries. We found for example that segment-internal filled pauses condition the following words
            quite differently from segment-initial filled pauses. Finally, recent efforts in language modeling for
            conversational speech, such as [8], attempt to capitalize on the internal structure of utterances and turns.
            Such models are formulated in terms of linguistic units and therefore require linguistic segmentations to be
            applicable.
            3.

            METHOD

            Our main goal for this work was to examine to what extent various
            kinds of lexical (word-based) information were useful for automatic linguistic segmentation. This precluded
            a study based on the output of existing speech recognition systems, which currently achieve about 40-50%
            word error rate on the type of data used in our experiments. At such high error rates, the analysis of any
            segmentation algorithm and the features it uses would likely be confounded by the unreliable nature of the
            input data. We therefore chose to eliminate the problem of inaccurate speech recognition and tested our
            algorithms on hand-transcribed word-level transcripts of spontaneous speech from the Switchboard corpus [4].
            An additional benefit of this approach is that the models employed by the segmentation algorithms can also
            be directly used as language models for speech recognizers for the same type of data, an application we are
            pursuing as well. The segmentation approaches we investigated all fell within the following framework. We
            first trained a statistical language model of the N-gram variety to model the distribution of both words and
            segment boundaries. (For this purpose, segment boundaries were represented as special tokens
            <s>
            within the text.) The segmentation information was removed from the test data, and the language model was
            used to hypothesize the most probable locations of seg-
            ment boundaries. The resulting segmentations were then evaluated
            along a number of metrics.
            computation yields the likelihoods of the states at each position k:

            As training data, we used 1.4 million words of Switchboard transcripts annotated for linguistic
            segmentations by the UPenn Treebank project [7], comprising a total of 193,000 segments. One half
            of the standard Switchboard development test set, totaling 10,000 words and 1,300 segments, was used for
            testing.
            PNO-S (w1 : : : wk )

            The hand-annotated segments encompassed different kinds of linguistic structures, including

            PS (w1 : : : wk )

             Complete sentences
             Stand-alone phrases  Disfluent sentences aborted in mid-utterance1  Interjections and back-channel
            responses
            =

            PNO-S (w1 : : : wk 1 ) 
            p(wkjwk 2 wk 1 )
            +PS (w1 : : : wk 1 )

            

            p(wkj<s>wk 1 )

            =

            PNO-S (w1 : : : wk 1 ) 

            p(<s>jwk 2 wk 1 ) p(wkj<s>)

            +PS (w1 : : : wk 1 )

            

            p(<s>j<s>wk 1 ) p(wkj<s>)

            B.44: Worried that they're not going to
            get enough attention?
            <s>
            //
            A corresponding Viterbi algorithm is used to find the most likely
            sequence of S and NO-S (i.e., a segmentation) for a given word string. This language model is a full
            implementation of the model approximated in [8]. The hidden disfluency model of [10] has a similar
            structure. As indicated in the formulae above, we currently use at most two words of history in the local
            conditional probabilities p(j). Longer N-grams can be used if more state information is kept.
            A.45: Yeah,
            <s>
            and, uh, you know, colds
            and things like that
            <laughter>
            get -- //
            The local N-gram probabilities are estimated from the training data
            by using Katz backoff with Good-Turing discounting [6].
            The following excerpt illustrates the character of the data. Linguistic segment boundaries are marked<s>,
            whereas acoustic segmentations are indicated by //.

            B.46:

            Yeah.

            <s>
            //

            A.47: -- spread real easy and things,
            <s>
            but, // and they're expensive
            <s>
            and, //
            <lipsmack>
            // course, // there's a lot of different types of day care available, too, // you know, where they teach
            them academic things.
            <s>
            // B.48:
            Yes.

            <s>
            //

            This short transcript shows some of the ubiquitous features of spontaneous speech affecting segmentation,
            such as

             Mismatch between acoustic and linguistic segmentations
            (A.47)
             segments spanning several turns (A.45 and A.47)
             backchannel responses (B.46)
            5. RESULTS
            5.1.
            Baseline Segmentation Model

            The first model we looked at models only plain words and segment
            boundaries in the manner described. It was applied to the concatenation of all turns of a conversation side,
            with no additional contextual cues supplied. During testing, this model thus operates with very minimal
            information, i.e., with only the raw word sequence to be segmented. Table 1 shows results for bigram and
            trigram models. The performance metrics used are defined as follows. Recall
            Table 1: Baseline model performance
            Model Recall Precision FA SER Bigram 65.5% 56.9% 1.9% 58.9% Trigram 70.2% 60.7% 2.0% 53.1%
            4. THE MODEL
            The language models used were of the N-gram type commonly used in speech recognition [5]. In N-gram models,
            a word wn from a n 1 word history w1 : : : wn 1. If the history contains a segment boundary<s>, it is
            truncated before that location. During testing, the model is run as a hidden segment model, hypothesizing
            segment boundaries between any two words and implicitly computing the probabilities of all possible
            segmentations. Associated with each word position are two states, S and NO-S, corresponding to a segment
            starting or not before that word. A forward 1 Although complete and disfluent sentences were marked
            differently in the corpus, we modeled these with a single type of boundary token.
            is the percentage of actual segment boundaries hypothesized. Precision is the percentage of hypothesized
            boundaries that are actual.
            False Alarms (FA) are the fraction of potential boundaries incorrectly hypothesized as boundaries. Segment
            Error Rate (SER) is the percentage of actual segments identified without intervening false alarms. As can be
            seen, word context alone can identify a majority of segment boundaries at a modest false alarm rate of about
            2%. The trigram model does better than the bigram, but this is expected since it has access to a larger
            context around potential segment boundaries. to use in its decision. Given these results, we only consider
            trigram models in all following experiments.
            5.2.

            Using Turn Information

            Next we examined a richer model that incorporated information
            about the turn-taking between speakers.2 Note that turn boundaries are already present in acoustic
            segmentations, but in this case we will only use them as a cue to the identification of linguistic segments.
            Turn information is easily incorporated into the segmentation model by placing special tags at turn
            boundaries (in both training and testing). Model performance is summarized in Table 2. Table 2: Segmentation
            performance using turn information Model Recall Precision FA SER Baseline 70.2% 60.7% 2.0% 53.1% Turn-tagged
            76.9% 66.9% 1.8% 44.9%
            As can be seen, adding turn information improves performance on
            all metrics. This improvement occurs even though turn boundaries are far from perfectly correlated with
            segment boundaries. As illustrated earlier, turns can contain multiple segments, or segments may span
            multiple turns.
            5.3.

            boundaries provide some of the strongest cues for these boundaries.
            Apart from these strong lexical cues, it seems to be helpful to abstract from word identity and use POS
            information instead. In other words, the tag set could be optimized to provide the right level of resolution
            for the segmentation task. It should be noted that the results for POS-based models are optimistic in the
            sense that for an actual application one would first have to tag the input with POS labels, and then apply
            the segmentation model. The actual performance would be degraded by tagging errors.
            5.4.

            Error Trade-offs

            As an aside to our search for useful features for the segmentation task, we observe that we can optimize any
            particular language
            model by trading off recall performance for false alarm rate, or vice versa. We did this by biasing the
            likelihoods of S states by some constant factor, causing the Viterbi algorithm to choose these states more
            often. Table 4 compares two bias values, and shows that the bias can be used to increase both recall and
            precision, while also reducing the segment error rate.
            Using Part-of-Speech Information

            So far we have used only the identity of words. It is likely that
            segmentation is closely related to syntactic (as opposed to lexical) structure. Short of using a full-scale
            parser on the input we could use the parts of speech (POS) of words as a more suitable representation from
            which to predict segment boundaries. Parts of speech should also generalize much better to contexts
            containing N-grams not observed in the training data (assuming the POS of the words involved is known). We
            were able to test this hypothesis by using the POS-tagged version of the Switchboard corpus. We built two
            models based on POS from this data. Model I had all words replaced by their POS labels during training and
            test, and also used turn boundary information. Model II also used POS labels, but retained the word
            identities of certain word classes that were deemed to be particularly relevant to segmentation. These
            retained words include filled pauses, conjunctions, and certain discourse markers such as “okay,” “so,”
            “well,” etc. Results are shown in Table 3. Table 3: Segmentation performance using POS information Model
            Recall Precision FA SER Word-based 76.9% 66.9% 1.8% 44.9% POS-based I 68.9% 58.5% 2.0% 59.3% POS-based II
            79.6% 73.5% 0.9% 39.9%
            We see that POS tags alone (Model I) do not result in better segmentations than words. The fact that Model
            II performs better than both
            the all-word based model and the pure POS model indicates that certain function words that tend to occur in
            the context of segment 2 Speakers can talk over each other. We did not model this case separately; instead,
            we adopted the serialization of turns implied by the transcripts.
            Model
            Bias = 1 Bias = 2
            Table 4: Biasing segmentation
            Recall Precision FA 76.9% 66.9% 1.8% 85.2% 69.2% 2.7%
            SER
            44.9% 37.4%
        </corps>
        <conclusion>7. CONCLUSIONS
            We have argued for the need for automatic speech segmentation algorithms that can identify linguistically
            motivated, sentence-level units of speech. We have shown that transcribed speech can be segmented
            linguistically with good accuracy by using an N-gram language model for the locations of the hidden segment
            boundaries. We studied several word-level features for possible incorporation in the model, and found that
            best performance so far was achieved with a combination of function ‘cue’ words, POS labels, and turn
            markers.
            Acknowledgments
            This research was supported by DARPA and NSF, under NSF Grant IRI-9314967. The views herein are those of the
            authors and should not be interpreted as representing the policies of DARPA or the NSF. 3 Such an
            integration can be achieved in a language model using the maximum entropy paradigm [1], but this would make
            the estimation process considerably more expensive.
            8.
        </conclusion>
        <discussion>6. DISCUSSION
            6.1.
            Error Analysis

            To understand what type of errors the segmenter makes, we handchecked a set of 200 false alarms generated by
            the baseline trigram
            model. The most frequent type (34%) of false alarm corresponded to splitting of segments at
            sentence-internal clause boundaries, e.g., false alarms triggered by a conjunction that would be likely to
            start a segment. For example, the
            <s>
            in the segmentation i'm not sure how many active volcanos there are now
            <s>
            and and what the amount of material that they do uh put into the atmosphere
            represents a false alarm, presumably triggered by the following coordinating conjunction “and.”
            5% of the false alarms could be attributed to filled pauses at the end of segments, which were often
            attached to the following segment. This actually reflects a labeling ambiguity that should not be counted as
            an error. Another 7% of the false alarm we deemed to be labeling errors. Thus, a total of 12% of false
            alarms could be considered to be actually correct.
            6.2.

            Other Segmentation Algorithms

            Our language-model-based segmentation algorithm is only one of
            many that could be used to perform the linguistic segmentation task, given a set of features. Conceptually,
            segmentation is just another
            classification problem, in which each word transition must be labeled as either a segment boundary or a
            within-segment transition.
            Two natural choices for alternative approaches are decision trees and a transformation-based, error-driven
            classifier of the type developed by Eric Brill for other tagging problems [2]. Both of these methods would
            make it easier to combine diverse input features that are not readily integrated into a single probabilistic
            language model, e.g., if we wanted to use both POS and word identity for each word.3 Our approach, on the
            other hand, has the advantage of simplicity and efficiency. Furthermore, the language model used for
            segmentation can also be used for speech decoding or rescoring. We already mentioned that if POS information
            is to be used for segmentation, an automatic tagging step is required. This presents somewhat of a
            chicken-and-egg problem, in that taggers typically rely on segmentations. An appealing solution to this
            problem in the statistical tagging framework [3] would be to model both segmentation and tag assignment as a
            single hidden Markov process.
            6.3.

            Other Features for Segmentation

            All of our experiments were based on lexical information only. To
            further improve segmentation performance, and to make it less dependent on accurate speech recognition, we
            plan to combine the LM approach with a model for various acoustic and prosodic correlates of segmentation.
            These include:
             Unfilled pause durations
             Fundamental frequency patterns  Phone durations  Glottalization Our current segmentation model deals
            with each conversation side in isolation. An alternative approach is to model the two sides jointly, thereby
            allowing us to capitalize on correlations between the segment structure of one speaker and what is said by
            the other. It is likely, for example, that backchannel responses would be modeled better this way.
        </discussion>
        <biblio>1. A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maximum entropy approach to natural
            language processing. Computational Linguistics, 22(1):39–71, 1996.
            2. E. Brill. Some advances in transformation-based part of speech tagging. In Proceedings of the 12th
            National Conference on Artificial Intelligence, Seattle, WA, 1994. AAAI Press. 3. K. W. Church. A stochastic
            parts program and noun phrase parser for unrestricted text. In Second Conference on Applied Natural Language
            Processing, pages 136–143, Austin, Texas, 1988. 4. J. J. Godfrey, E. C. Holliman, and J. McDaniel.
            SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings IEEE Conference on
            Acoustics, Speech and Signal Processing, volume I, pages 517–520, San Francisco, March 1992. 5. F. Jelinek.
            Self-organized language modeling for speech recognition. In A. Waibel and K.-F. Lee, editors, Readings in
            Speech Recognition. Morgan Kaufmann, San Mateo, Ca., 1990. 6. S. M. Katz. Estimation of probabilities from
            sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech,
            and Signal Processing, 35(3):400–401, March 1987. 7. M. Meteer et al. Dysfluency annotation stylebook for
            the Switchboard corpus. Distributed by LDC, February 1995. Revised June 1995 by Ann Taylor. 8. M. Meteer and
            R. Iyer. Modeling conversational speech for speech recognition. In Proceedings of the Conference on
            Empirical Methods in Natural Language Processing, Philadelphia, PA, May 1996. 9. E. Shriberg and A. Stolcke.
            Word predictability after hesitations: A corpus-based study. In Proceedings International Conference on
            Spoken Language Processing, Philadelphia, PA, October 1996. 10. A. Stolcke and E. Shriberg. Statistical
            language modeling for speech disfluencies. In Proceedings IEEE Conference on Acoustics, Speech and Signal
            Processing, volume I, pages 405– 408, Atlanta, GA, May 1996.
        </biblio>
    </article>
    <article>
        <preamble>Torres.txt</preamble>
        <titre>Summary Evaluation</titre>
        <auteur>with and without References Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and
            Patricia Velázquez-Morales
        </auteur>
        <abstract>we study a new contentbased method for the evaluation of text summarization systems without human
            models which is used to produce system rankings. The research is carried out using a new content-based
            evaluation framework called F RESA to compute a variety of divergences among probability distributions. We
            apply our comparison framework to various well-established content-based evaluation measures in text
            summarization such as C OVERAGE, R ESPONSIVENESS, P YRAMIDS and ROUGE studying their associations in various
            text summarization tasks including generic multi-document summarization in English and French, focus-based
            multi-document summarization in English and generic single-document summarization in French and Spanish.
            Index Terms—Text summarization evaluation, content-based evaluation measures, divergences.
        </abstract>
        <introduction>T EXT summarization evaluation has always been a complex and controversial issue in computational
            linguistics. In the last decade, significant advances have been made in this field as well as various
            evaluation measures have been designed. Two evaluation campaigns have been led by the U.S. agence DARPA. The
            first one, SUMMAC, ran from 1996 to 1998 under the auspices of the Tipster program [1], and the second one,
            entitled DUC (Document Understanding Conference) [2], was the main evaluation forum from 2000 until 2007.
            Nowadays, the Text Analysis Conference (TAC) [3] provides a forum for assessment of different information
            access technologies including text summarization. Evaluation in text summarization can be extrinsic or
            intrinsic [4]. In an extrinsic evaluation, the summaries are assessed in the context of an specific task
            carried out by a human or a machine. In an intrinsic evaluation, the summaries are evaluated in reference to
            some ideal model. SUMMAC was mainly extrinsic while DUC and TAC followed an intrinsic evaluation paradigm.
            In an intrinsic evaluation, an Manuscript received June 8, 2010. Manuscript accepted for publication July
            25, 2010. Juan-Manuel Torres-Moreno is with LIA/Université d’Avignon, France and École Polytechnique de
            Montréal, Canada (juan-manuel.torres@univ-avignon.fr). Eric SanJuan is with LIA/Université d’Avignon,
            France (eric.sanjuan@univ-avignon.fr). Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain
            (horacio.saggion@upf.edu). Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain; LIA/Université
            d’Avignon, France and Instituto de Ingenierı́a/UNAM, Mexico (iria.dacunha@upf.edu). Patricia
            Velázquez-Morales is with VM Labs, France (patricia velazquez@yahoo.com). 13 Polibits (42) 2010 Juan-Manuel
            Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales of models and
            the identification, matching, and weighting of SCUs in both: models and peers. [12] evaluated the
            effectiveness of the Jensen-Shannon (J S) [13] theoretic measure in predicting systems ranks in two
            summarization tasks: query-focused and update summarization. They have shown that ranks produced by P
            YRAMIDS and those produced by J S measure correlate. However, they did not investigate the effect of the
            measure in summarization tasks such as generic multi-document summarization (DUC 2004 Task 2), biographical
            summarization (DUC 2004 Task 5), opinion summarization (TAC 2008 OS), and summarization in languages other
            than English. In this paper we present a series of experiments aimed at a better understanding of the value
            of the J S divergence for ranking summarization systems. We have carried out experimentation with the
            proposed measure and we have verified that in certain tasks (such as those studied by [12]) there is a
            strong correlation among P YRAMIDS, R ESPONSIVENESS and the J S divergence, but as we will show in this
            paper, there are datasets in which the correlation is not so strong. We also present experiments in Spanish
            and French showing positive correlation between the J S and ROUGE which is the de facto evaluation measure
            used in evaluation of non-English summarization. To the best of our knowledge this is the more extensive set
            of experiments interpreting the value of evaluation without human models. The rest of the paper is organized
            in the following way: First in Section II we introduce related work in the area of content-based evaluation
            identifying the departing point for our inquiry; then in Section III we explain the methodology adopted in
            our work and the tools and resources used for experimentation. In Section IV we present the experiments
            carried out together with the results. Section V discusses the results and Section VI concludes the paper
            and identifies future work. non-random systems, no clear conclusion was reached on the value of each of the
            studied measures. Nowadays, a widespread summarization evaluation framework is ROUGE [14], which offers a
            set of statistics that compare peer summaries with models. It counts co-occurrences of n-grams in peer and
            models to derive a score. There are several statistics depending on the used n-grams and the text processing
            applied to the input texts (e.g., lemmatization, stop-word removal). [15] proposed a method of evaluation
            based on the use of “distances” or divergences between two probability distributions (the distribution of
            units in the automatic summary and the distribution of units in the model summary). They studied two
            different Information Theoretic measures of divergence: the Kullback-Leibler (KL) [16] and Jensen-Shannon (J
            S) [13] divergences. KL computes the divergence between probability distributions P and Q in the following
            way: Pw 1X Pw log2 (1) DKL (P ||Q) = 2 w Qw While J S divergence is defined as follows: 1X 2Pw 2Qw DJ S (P
            ||Q) = Pw log2 + Qw log2 2 w Pw + Qw Pw + Qw (2) These measures can be applied to the distribution of units
            in system summaries P and reference summaries Q. The value obtained may be used as a score for the system
            summary. The method has been tested by [15] over the DUC 2002 corpus for single and multi-document
            summarization tasks showing good correlation among divergence measures and both coverage and ROUGE rankings.
            [12] went even further and, as in [5], they proposed to compare directly the distribution of words in full
            documents with the distribution of words in automatic summaries to derive a content-based evaluation
            measure. They found a high correlation between rankings produced using models and rankings produced without
            models. This last work is the departing point for our inquiry into the value of measures that do not rely on
            human models.
        </introduction>
        <corps>value of each of the studied measures. Nowadays, a widespread summarization evaluation framework is ROUGE
            [14], which offers a set of statistics that compare peer summaries with models. It counts co-occurrences of
            n-grams in peer and models to derive a score. There are several statistics depending on the used n-grams and
            the text processing applied to the input texts (e.g., lemmatization, stop-word removal). [15] proposed a
            method of evaluation based on the use of “distances” or divergences between two probability distributions
            (the distribution of units in the automatic summary and the distribution of units in the model summary).
            They studied two different Information Theoretic measures of divergence: the Kullback-Leibler (KL) [16] and
            Jensen-Shannon (J S) [13] divergences. KL computes the divergence between probability distributions P and Q
            in the following way: Pw 1X Pw log2 (1) DKL (P ||Q) = 2 w Qw While J S divergence is defined as follows: 1X
            2Pw 2Qw DJ S (P ||Q) = Pw log2 + Qw log2 2 w Pw + Qw Pw + Qw (2) These measures can be applied to the
            distribution of units in system summaries P and reference summaries Q. The value obtained may be used as a
            score for the system summary. The method has been tested by [15] over the DUC 2002 corpus for single and
            multi-document summarization tasks showing good correlation among divergence measures and both coverage and
            ROUGE rankings. [12] went even further and, as in [5], they proposed to compare directly the distribution of
            words in full documents with the distribution of words in automatic summaries to derive a content-based
            evaluation measure. They found a high correlation between rankings produced using models and rankings
            produced without models. This last work is the departing point for our inquiry into the value of measures
            that do not rely on human models.
            II. R ELATED W ORK
            One of the first works to use content-based measures in text summarization evaluation is due to [5], who
            presented an evaluation framework to compare rankings of summarization systems produced by recall and
            cosine-based measures. They showed that there was weak correlation among rankings produced by recall, but
            that content-based measures produce rankings which were strongly correlated. This put forward the idea of
            using directly the full document for comparison purposes in text summarization evaluation. [6] presented a
            set of evaluation measures based on the notion of vocabulary overlap including n-gram overlap, cosine
            similarity, and longest common subsequence, and they applied them to multi-document summarization in English
            and Chinese. However, they did not evaluate the performance of the measures in different summarization
            tasks. [7] also compared various evaluation measures based on vocabulary overlap. Although these measures
            were able to separate random from
            Polibits (42) 2010

            III. M ETHODOLOGY
            The followed methodology in this paper mirrors the one adopted in past work (e.g. [5], [7], [12]). Given a
            particular summarization task T , p data points to be summarized p−1 with input material {Ii }i=0 (e.g.
            document(s), question(s), s−1 topic(s)), s peer summaries {SUMi,k }k=0 for input i, and m−1 m model
            summaries {MODELi,j }j=0 for input i, we will compare rankings of the s peer summaries produced by various
            evaluation measures. Some measures that we use compare summaries with n of the m models: MEASUREM (SUMi,k ,
            {MODELi,j }n−1 j=0 )
            14

            (3)

            Summary Evaluation with and without References

            while other measures compare peers with all or some of the
            input material: MEASUREM (SUMi,k , Ii0 )
            3) Update-summarization task that consists of creating a
            summary out of a cluster of documents and a topic. Two sub-tasks are considered here: A) an initial summary
            has to be produced based on an initial set of documents and topic; B) an update summary has to be produced
            from a different (but related) cluster assuming documents used in A) are known. The English TAC’08 Update
            Summarization dataset is used, which consists of 48 topics with 20 documents each – 36,911 words. 4) Opinion
            summarization where systems have to analyze a set of blog articles and summarize the opinions about a target
            in the articles. The TAC’08 Opinion Summarization in English4 data set (taken from the Blogs06 Text
            Collection) is used: 25 clusters and targets (i.e., target entity and questions) were used – 1,167,735
            words. 5) Generic single-document summarization in Spanish using the Medicina Clı́nica5 corpus, which is
            composed of 50 medical articles in Spanish, each one with its corresponding author abstract – 124,929 words.
            6) Generic single document summarization in French using the “Canadien French Sociological Articles” corpus
            from the journal Perspectives interdisciplinaires sur le travail et la santé (PISTES)6 . It contains 50
            sociological articles in French, each one with its corresponding author abstract – 381,039 words. 7) Generic
            multi-document-summarization in French using data from the RPM27 corpus [18], 20 different themes consisting
            of 10 articles and 4 abstracts by reference thematic – 185,223 words.
            (4)

            where Ii0 is some subset of input Ii . The values produced
            by the measures for each summary SUMi,k are averaged for each system k = 0, . . . , s − 1 and these averages
            are used to produce a ranking. Rankings are then compared using Spearman Rank correlation [17] which is used
            to measure the degree of association between two variables whose values are used to rank objects. We have
            chosen to use this correlation to compare directly results to those presented in [12]. Computation of
            correlations is done using the Statistics-RankCorrelation-0.12 package1 , which computes the rank
            correlation between two vectors. We also verified the good conformity of the results with the correlation
            test of Kendall τ calculated with the statistical software R. The two nonparametric tests of Spearman and
            Kendall do not really stand out as the treatment of ex-æquo. The good correspondence between the two tests
            shows that they do not introduce bias in our analysis. Subsequently will mention only the ρ of Sperman more
            widely used in this field. A. Tools We carry out experimentation using a new summarization evaluation
            framework: F RESA –FRamework for Evaluating Summaries Automatically–, which includes document-based summary
            evaluation measures based on probabilities distribution2 . As in the ROUGE package, F RESA supports
            different n-grams and skip n-grams probability distributions. The F RESA environment can be used in the
            evaluation of summaries in English, French, Spanish and Catalan, and it integrates filtering and
            lemmatization in the treatment of summaries and documents. It is developed in Perl and will be made publicly
            available. We also use the ROUGE package [10] to compute various ROUGE statistics in new datasets.
            For experimentation in the TAC and the DUC datasets we use
            directly the peer summaries produced by systems participating in the evaluations. For experimentation in
            Spanish and French (single and multi-document summarization) we have created summaries at a similar ratio to
            those of reference using the following systems: – ENERTEX [19], a summarizer based on a theory of textual
            energy; – CORTEX [20], a single-document sentence extraction system for Spanish and French that combines
            various statistical measures of relevance (angle between sentence and topic, various Hamming weights for
            sentences, etc.) and applies an optimal decision algorithm for sentence selection; – SUMMTERM [21], a
            terminology-based summarizer that is used for summarization of medical articles and uses specialized
            terminology for scoring and ranking sentences; – REG [22], summarization system based on an greedy
            algorithm;
            B. Summarization Tasks and Data Sets
            We have conducted our experimentation with the following summarization tasks and data sets: 1) Generic
            multi-document-summarization in English (production of a short summary of a cluster of related documents)
            using data from DUC’043 , task 2: 50 clusters, 10 documents each – 294,636 words. 2) Focused-based
            summarization in English (production of a short focused multi-document summary focused on the question “who
            is X?”, where X is a person’s name) using data from the DUC’04 task 5: 50 clusters, 10 documents each plus a
            target person name – 284,440 words.
            4 http://www.nist.gov/tac/data/index.html

            1 http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/

            5 http://www.elsevier.es/revistas/ctl servlet? f=7032&revistaid=2

            2 F RESA

            is available at: http://lia.univavignon.fr/fileadmin/axes/TALNE/
            Ressources.html 3 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html
            6 http://www.pistes.uqam.ca/
            7 http://www-labs.sinequa.com/rpm2
            15

            Polibits (42) 2010

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            – J S summarizer, a summarization system that scores
            and ranks sentences according to their Jensen-Shannon divergence to the source document; – a lead-based
            summarization system that selects the lead sentences of the document; – a random-based summarization system
            that selects sentences at random; – Open Text Summarizer [23], a multi-lingual summarizer based on the
            frequency and – commercial systems: Word, SSSummarizer8 , Pertinence9 and Copernic10 .
            presented here we used uni-grams, 2-grams, and the skip
            2-grams with maximum skip distance of 4 (ROUGE-1, ROUGE-2 and ROUGE-SU4). ROUGE is used to compare a peer
            summary to a set of model summaries in our framework (as indicated in equation 3). – Jensen-Shannon
            divergence formula given in Equation 2 is implemented in our F RESA package with the following specification
            (Equation 6) for the probability distribution of words w. CT Pw = w N ( S Cw if w ∈ S NS Qw = (6) T Cw +δ
            otherwise N +δ∗B
            C. Evaluation Measures
            The following measures derived from human assessment of the content of the summaries are used in our
            experiments: – C OVERAGE is understood as the degree to which one peer summary conveys the same information
            as a model summary [2]. C OVERAGE was used in DUC evaluations. This measure is used as indicated in equation
            3 using human references or models. – R ESPONSIVENESS ranks summaries in a 5-point scale indicating how well
            the summary satisfied a given information need [2]. It is used in focused-based summarization tasks. This
            measure is used as indicated in equation 4 since a human judges the summary with respect to a given input
            “user need” (e.g., a question). R ESPONSIVENESS was used in DUC and TAC evaluations. – P YRAMIDS [11] is a
            content assessment measure which compares content units in a peer summary to weighted content units in a set
            of model summaries. This measure is used as indicated in equation 3 using human references or models. P
            YRAMIDS is the adopted metric for content-based evaluation in the TAC evaluations. For DUC and TAC datasets
            the values of these measures are available and we used them directly. We used the following automatic
            evaluation measures in our experiments: – ROUGE [14], which is a recall metric that takes into account
            n-grams as units of content for comparing peer and model summaries. The ROUGE formula specified in [10] is
            as follows: ROUGE-n(R, M ) = P ∈ M count m n−gram∈P match (n − gram) P P count(n-gram) m ∈M
            Where P is the probability distribution of words w in
            text T and Q is the probability distribution of words w in summary S; N is the number of words in text and T
            summary N = NT +NS , B = 1.5|V |, Cw is the number S of words in the text and Cw is the number of words in
            the summary. For smoothing the summary’s probabilities we have used δ = 0.005. We have also implemented
            other smoothing approaches (e.g. Good-Turing [24], that uses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2
            package11 ) in F RESA, but we do not use them in the experiments reported here. Following the ROUGE
            approach, in addition to word uni-grams we use 2-grams and skip n-grams computing divergences such as J S
            (using uni-grams) J S 2 (using 2-grams), J S 4 (using the skip n-grams of ROUGE-SU4), and J S M which is an
            average of the J S i . J Ss measures are used to compare a peer summary to its source document(s) in our
            framework (as indicated in equation 4). In the case of summarization of multiple documents, these are
            concatenated (in the given input order) to form a single input from which probabilities are computed. IV. E
            XPERIMENTS AND R ESULTS We first replicated the experiments presented in [12] to verify that our
            implementation of J S produced correlation results compatible with that work. We used the TAC’08 Update
            Summarization data set and computed J S and ROUGE measures for each peer summary. We produced two system
            rankings (one for each measure), which were compared to rankings produced using the manual P YRAMIDS and R
            ESPONSIVENESS scores. Spearman correlations were computed among the different rankings. The results are
            presented in Table I. These results confirm a high correlation among P YRAMIDS, R ESPONSIVENESS and J S. We
            also verified high correlation between J S and ROUGE-2 (0.83 Spearman correlation, not shown in the table)
            in this task and dataset. Then, we experimented with data from DUC’04, TAC’08 Opinion Summarization pilot
            task as well as single and
            P

            (5)

            where R is the summary to be evaluated, M is the set of
            model (human) summaries, countmatch is the number of common n-grams in m and P , and count is the number of
            n-grams in the model summaries. For the experiments 8 http://www.kryltech.com/summarizer.htm 9
            http://www.pertinence.net
            11 http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/

            10 http://www.copernic.com/en/products/summarizer

            Polibits (42) 2010

            16

            Summary Evaluation with and without References

            TABLE I
            S PEARMAN CORRELATION OF CONTENT- BASED MEASURES IN TAC’08 U PDATE S UMMARIZATION TASK
            Mesure
            ROUGE-2 JS
            P YRAMIDS
            0.96 0.85
            p-value
            p
            <
            0.005 p
            <
            0.005
            R ESPONSIVENESS
            0.92 0.74
            evaluation metrics that do not rely on human models but that
            compare summary content to input content directly [12]. We have some positive and some negative results
            regarding the direct use of the full document in content-based evaluation. We have verified that in both
            generic muti-document summarization and in topic-based multi-document summarization in English correlation
            among measures that use human models (P YRAMIDS, R ESPONSIVENESS and ROUGE) and a measure that does not use
            models (J S divergence) is strong. We have found that correlation among the same measures is weak for
            summarization of biographical information and summarization of opinions in blogs. We believe that in these
            cases content-based measures should be considered, in addition to the input document, the summarization task
            (i.e. text-based representation, description) to better assess the content of the peers [25], the task being
            a determinant factor in the selection of content for the summary. Our multi-lingual experiments in generic
            single-document summarization confirm a strong correlation among the J S divergence and ROUGE measures. It
            is worth noting that ROUGE is in general the chosen framework for presenting content-based evaluation
            results in non-English summarization. For the experiments in Spanish, we are conscious that we only have one
            model summary to compare with the peers. Nevertheless, these models are the corresponding abstracts written
            by the authors. As the experiments in [26] show, the professionals of a specialized domain (as, for example,
            the medical domain) adopt similar strategies to summarize their texts and they tend to choose roughly the
            same content chunks for their summaries. Previous studies have shown that author abstracts are able to
            reformulate content with fidelity [27] and these abstracts are ideal candidates for comparison purposes.
            Because of this, the summary of the author of a medical article can be taken as reference for summaries
            evaluation. It is worth noting that there is still debate on the number of models to be used in
            summarization evaluation [28]. In the French corpus PISTES, we suspect the situation is similar to the
            Spanish case.
            p-value
            p
            <
            0.005 p
            <
            0.005
            multi-document summarization in Spanish and French. In spite
            of the fact that the experiments for French and Spanish corpora use less data points (i.e., less summarizers
            per task) than for English, results are still quite significant. For DUC’04, we computed the J S measure for
            each peer summary in tasks 2 and 5 and we used J S, ROUGE, C OVERAGE and R ESPONSIVENESS scores to produce
            systems’ rankings. The various Spearman’s rank correlation values for DUC’04 are presented in Tables II (for
            task 2) and III (for task 5). For task 2, we have verified a strong correlation between J S and C OVERAGE.
            For task 5, the correlation between J S and C OVERAGE is weak, and that between J S and R ESPONSIVENESS is
            weak and negative. Although the Opinion Summarization (OS) task is a new type of summarization task and its
            evaluation is a complicated issue, we have decided to compare J S rankings with those obtained using P
            YRAMIDS and R ESPONSIVENESS in TAC’08. Spearman’s correlation values are listed in Table IV. As it can be
            seen, there is weak and negative correlation of J S with both P YRAMIDS and R ESPONSIVENESS. Correlation
            between P YRAMIDS and R ESPONSIVENESS rankings is high for this task (0.71 Spearman’s correlation value).
            For experimentation in mono-document summarization in Spanish and French, we have run 11 multi-lingual
            summarization systems; for experimentation in French, we have run 12 systems. In both cases, we have
            produced summaries at a compression rate close to the compression rate of the authors’ provided abstracts.
            We have then computed J S and ROUGE measures for each summary and we have averaged the measure’s values for
            each system. These averages were used to produce rankings per each measure. We computed Spearman’s
            correlations for all pairs of rankings. Results are presented in Tables V, VI and VII. All results show
            medium to strong correlation between the J S measures and ROUGE measures. However the J S measure based on
            uni-grams has lower correlation than J Ss which use n-grams of higher order. Note that table VII presents
            results for generic multi-document summarization in French, in this case correlation scores are lower than
            correlation scores for single-document summarization in French, a result which may be expected given the
            diversity of input in multi-document summarization.
        </corps>
        <conclusion>VI. C ONCLUSIONS AND F UTURE W ORK
            This paper has presented a series of experiments in content-based measures that do not rely on the use of
            model summaries for comparison purposes. We have carried out extensive experimentation with different
            summarization tasks drawing a clearer picture of tasks where the measures could be applied. This paper makes
            the following contributions: – We have shown that if we are only interested in ranking summarization systems
            according to the content of their automatic summaries, there are tasks were models could be subtituted by
            the full document in the computation of the J S measure obtaining reliable rankings. However, we have also
            found that the substitution of models by full-documents is not always advisable. We have
            V. D ISCUSSION
            The departing point for our inquiry into text summarization evaluation has been recent work on the use of
            content-based
            17

            Polibits (42) 2010

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            TABLE II
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC’04 TASK 2 Mesure ROUGE-2 JS
            C OVERAGE
            0.79 0.68
            p-value
            p
            <
            0.0050 p
            <
            0.0025
            TABLE III
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN DUC’04 TASK 5 Mesure ROUGE-2 JS
            C OVERAGE
            0.78 0.40
            p-value
            p
            <
            0.001 p
            <
            0.050
            R ESPONSIVENESS
            0.44 -0.18
            p-value
            p
            <
            0.05 p
            <
            0.25
            TABLE IV
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN TAC’08 OS TASK Mesure JS
            P YRAMIDS
            -0.13
            p-value
            p
            <
            0.25
            R ESPONSIVENESS
            -0.14
            p-value
            p
            <
            0.25
            TABLE V
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clı́nica C ORPUS (S PANISH ) Mesure JS J
            S2 J S4 J SM
            ROUGE -1
            0.56 0.88 0.88 0.82
            p-value
            p
            <
            0.100 p
            <
            0.001 p
            <
            0.001 p
            <
            0.005
            ROUGE -2
            0.46 0.80 0.80 0.71
            ROUGE -SU4
            0.45 0.81 0.81 0.71
            p-value
            p
            <
            0.200 p
            <
            0.005 p
            <
            0.005 p
            <
            0.010
            a representation of the task/topic in the calculation of
            measures. To carry out these comparisons, however, we are dependent on the existence of references. F RESA
            will also be used in the new question-answer task campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/
            qa.asp) for the evaluation of long answers. This task aims to answer a question by extraction and
            agglomeration of sentences in Wikipedia. This kind of task corresponds to those for which we have found a
            high correlation among the measures J S and evaluation methods with human intervention. Moreover, the J S
            calculation will be among the summaries produced and a representative set of relevant passages from
            Wikipedia. F RESA will be used to compare three types of systems, although different tasks: the
            multi-document summarizer guided by a query, the search systems targeted information (focused IR) and the
            question answering systems.
            found weak correlation among different rankings in
            complex summarization tasks such as the summarization of biographical information and the summarization of
            opinions. – We have also carried out large-scale experiments in Spanish and French which show positive
            medium to strong correlation among system’s ranks produced by ROUGE and divergence measures that do not use
            the model summaries. – We have also presented a new framework, F RESA, for the computation of measures based
            on J S divergence. Following the ROUGE approach, F RESA package use word uni-grams, 2-grams and skip n-grams
            computing divergences. This framework will be available to the community for research purposes. Although we
            have made a number of contributions, this paper leaves many open questions than need to be addressed. In
            order to verify correlation between ROUGE and J S, in the short term we intend to extend our investigation
            to other languages such as Portuguese and Chinesse for which we have access to data and summarization
            technology. We also plan to apply F RESA to the rest of the DUC and TAC summarization tasks, by using
            several smoothing techniques. As a novel idea, we contemplate the possibility of adapting the evaluation
            framework for the phrase compression task [29], which, to our knowledge, does not have an efficient
            evaluation measure. The main idea is to calculate J S from an automatically-compressed sentence taking the
            complete sentence by reference. In the long term, we plan to incorporate
            Polibits (42) 2010

            p-value
            p
            <
            0.100 p
            <
            0.002 p
            <
            0.002 p
            <
            0.020
        </conclusion>
        <discussion>V. D ISCUSSION
            The departing point for our inquiry into text summarization evaluation has been recent work on the use of
            content-based
            17

            Polibits (42) 2010

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            TABLE II
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC’04 TASK 2 Mesure ROUGE-2 JS
            C OVERAGE
            0.79 0.68
            p-value
            p
            <
            0.0050 p
            <
            0.0025
            TABLE III
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN DUC’04 TASK 5 Mesure ROUGE-2 JS
            C OVERAGE
            0.78 0.40
            p-value
            p
            <
            0.001 p
            <
            0.050
            R ESPONSIVENESS
            0.44 -0.18
            p-value
            p
            <
            0.05 p
            <
            0.25
            TABLE IV
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN TAC’08 OS TASK Mesure JS
            P YRAMIDS
            -0.13
            p-value
            p
            <
            0.25
            R ESPONSIVENESS
            -0.14
            p-value
            p
            <
            0.25
            TABLE V
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clı́nica C ORPUS (S PANISH ) Mesure JS J
            S2 J S4 J SM
            ROUGE -1
            0.56 0.88 0.88 0.82
            p-value
            p
            <
            0.100 p
            <
            0.001 p
            <
            0.001 p
            <
            0.005
            ROUGE -2
            0.46 0.80 0.80 0.71
            ROUGE -SU4
            0.45 0.81 0.81 0.71
            p-value
            p
            <
            0.200 p
            <
            0.005 p
            <
            0.005 p
            <
            0.010
            a representation of the task/topic in the calculation of
        </discussion>
        <biblio>p-value
            p
            <
            0.05 p
            <
            0.05 p
            <
            0.10 p
            <
            0.05
            ROUGE -SU4
            0.741 0.680 0.620 0.740
            p-value
            p
            <
            0.01 p
            <
            0.02 p
            <
            0.05 p
            <
            0.01
            [18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
            “A French Human Reference Corpus for multi-documents summarization and sentence compression,” in LREC’10,
            vol. 2, Malta, 2010, p. In press. [19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy of
            Associative Memories: performants applications of Enertex algorithm in text summarization and topic
            segmentation,” in MICAI’07, 2007, pp. 861–871. [20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G.
            Meunier, “Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2, St Malo, France, 2002,
            pp. 723–734. [21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales, “Automatic
            summarization using terminological and semantic resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.
            [22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton appliqué au résumé automatique de
            texte,” in JADT’10. Rome, 2010, p. In press. [23] V. Yatsko and T. Vishnyakov, “A method for evaluating
            modern systems of automatic text summarization,” Automatic Documentation and Mathematical Linguistics, vol.
            41, no. 3, pp. 93–103, 2007. [24] C. D. Manning and H. Schütze, Foundations of Statistical Natural Language
            Processing. Cambridge, Massachusetts: The MIT Press, 1999. [25] K. Spärck Jones, “Automatic summarising:
            The state of the art,” IPM, vol. 43, no. 6, pp. 1449–1481, 2007. [26] I. da Cunha, L. Wanner, and M. T.
            Cabré, “Summarization of specialized discourse: The case of medical articles in spanish,” Terminology, vol.
            13, no. 2, pp. 249–286, 2007. [27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL
            Student Research Workshop. Toulouse, France: Association for Computational Linguistics, 9-11 July 2001 2001,
            pp. 49–54. [28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries: Metrics under varying data
            conditions,” in UCNLG+Sum’09, Suntec, Singapore, August 2009, pp. 23–30. [29] K. Knight and D. Marcu,
            “Statistics-based summarization-step one: Sentence compression,” in Proceedings of the National Conference
            on Artificial Intelligence. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2000, pp.
            703–710.
            [1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
            B. Sundheim, “Summac: a text summarization evaluation,” Natural Language Engineering, vol. 8, no. 1, pp.
            43–68, 2002. [2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43, no. 6, pp. 1506–1520,
            2007. [3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland, USA: NIST, November 17-19
            2008. [4] K. Spärck Jones and J. Galliers, Evaluating Natural Language Processing Systems, An Analysis and
            Review, ser. Lecture Notes in Computer Science. Springer, 1996, vol. 1083. [5] R. L. Donaway, K. W. Drummey,
            and L. A. Mather, “A comparison of rankings produced by summarization evaluation measures,” in NAACL
            Workshop on Automatic Summarization, 2000, pp. 69–78. [6] H. Saggion, D. Radev, S. Teufel, and W. Lam,
            “Meta-evaluation of Summaries in a Cross-lingual Environment using Content-based Metrics,” in COLING 2002,
            Taipei, Taiwan, August 2002, pp. 849–855. [7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi,
            A. Çelebi, D. Liu, and E. Drábek, “Evaluation challenges in large-scale document summarization,” in
            ACL’03, 2003, pp. 375–382. [8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method for
            automatic evaluation of machine translation,” in ACL’02, 2002, pp. 311–318. [9] K. Pastra and H. Saggion,
            “Colouring summaries BLEU,” in Evaluation Initiatives in Natural Language Processing. Budapest, Hungary:
            EACL, 14 April 2003. [10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of Summaries,” in Text
            Summarization Branches Out: ACL-04 Workshop, M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp.
            74–81. [11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in Summarization: The Pyramid
            Method,” in HLT-NAACL, 2004, pp. 145–152. [12] A. Louis and A. Nenkova, “Automatically Evaluating Content
            Selection in Summarization without Human Models,” in Empirical Methods in Natural Language Processing,
            Singapore, August 2009, pp. 306–314. [Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
            [13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE Transactions on Information Theory,
            vol. 37, no. 145-151, 1991. [14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using N-gram
            Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ, USA: Association for Computational Linguistics,
            2003, pp. 71–78. [15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic approach to
            automatic evaluation of summaries,” in HLT-NAACL, Morristown, USA, 2006, pp. 463–470. [16] S. Kullback and
            R. Leibler, “On information and sufficiency,” Ann. of Math. Stat., vol. 22, no. 1, pp. 79–86, 1951. [17] S.
            Siegel and N. Castellan, Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill, 1998.
            19

            Polibits (42) 2010
        </biblio>
    </article>
    <article>
        <preamble>Word2Vec.txt</preamble>
        <titre>Efficient Estimation of Word Representations in Vector Space</titre>
        <auteur>Tomas Mikolov Google Inc., Mountain View, CA tmikolov@google.com</auteur>
        <abstract>We propose two novel model architectures for computing continuous vector representations of words from
            very large data sets. The quality of these representations is measured in a word similarity task, and the
            results are compared to the previously best performing techniques based on different types of neural
            networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less
            than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that
            these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word
            similarities. 1
        </abstract>
        <introduction>Many current NLP systems and techniques treat words as atomic units - there is no notion of
            similarity between words, as these are represented as indices in a vocabulary. This choice has several good
            reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data
            outperform complex systems trained on less data. An example is the popular N-gram model used for statistical
            language modeling - today, it is possible to train N-grams on virtually all available data (trillions of
            words [3]). However, the simple techniques are at their limits in many tasks. For example, the amount of
            relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated
            by the size of high quality transcribed speech data (often just millions of words). In machine translation,
            the existing corpora for many languages contain only a few billions of words or less. Thus, there are
            situations where simple scaling up of the basic techniques will not result in any significant progress, and
            we have to focus on more advanced techniques. With progress of machine learning techniques in recent years,
            it has become possible to train more complex models on much larger data set, and they typically outperform
            the simple models. Probably the most successful concept is to use distributed representations of words [10].
            For example, neural network based language models significantly outperform N-gram models [1, 27, 17]. 1.1
            Goals of the Paper The main goal of this paper is to introduce techniques that can be used for learning
            high-quality word vectors from huge data sets with billions of words, and with millions of words in the
            vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained
            on more 1 than a few hundred of millions of words, with a modest dimensionality of the word vectors between
            50 - 100. We use recently proposed techniques for measuring the quality of the resulting vector
            representations, with the expectation that not only will similar words tend to be close to each other, but
            that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of
            inflectional languages - for example, nouns can have multiple word endings, and if we search for similar
            words in a subspace of the original vector space, it is possible to find words that have similar endings
            [13, 14]. Somewhat surprisingly, it was found that similarity of word representations goes beyond simple
            syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the
            word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a
            vector that is closest to the vector representation of the word Queen [20]. In this paper, we try to
            maximize accuracy of these vector operations by developing new model architectures that preserve the linear
            regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic
            regularities1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss
            how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the
            training data. 1.2 Previous Work Representation of words as continuous vectors has a long history [10, 26,
            8]. A very popular model architecture for estimating neural network language model (NNLM) was proposed in
            [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was
            used to learn jointly the word vector representation and a statistical language model. This work has been
            followed by many others. Another interesting architecture of NNLM was presented in [13, 14], where the word
            vectors are first learned using neural network with a single hidden layer. The word vectors are then used to
            train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we
            directly extend this architecture, and focus just on the first step where the word vectors are learned using
            a simple model. It was later shown that the word vectors can be used to significantly improve and simplify
            many NLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different model
            architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were
            made available for future research and comparison2 . However, as far as we know, these architectures were
            significantly more computationally expensive for training than the one proposed in [13], with the exception
            of certain version of log-bilinear model where diagonal weight matrices are used [23].
        </introduction>
        <corps>architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward
            neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the
            word vector representation and a statistical language model. This work has been followed by many others.
            Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned
            using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the
            word vectors are learned even without constructing the full NNLM. In this work, we directly extend this
            architecture, and focus just on the first step where the word vectors are learned using a simple model. It
            was later shown that the word vectors can be used to significantly improve and simplify many NLP
            applications [4, 5, 29]. Estimation of the word vectors itself was performed using different model
            architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were
            made available for future research and comparison2 . However, as far as we know, these architectures were
            significantly more computationally expensive for training than the one proposed in [13], with the exception
            of certain version of log-bilinear model where diagonal weight matrices are used [23].
            2

            Model Architectures

            Many different types of models were proposed for estimating continuous representations of words,
            including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this
            paper, we focus on distributed representations of words learned by neural networks, as it was previously
            shown that they perform significantly better than LSA for preserving linear regularities among words [20,
            31]; LDA moreover becomes computationally very expensive on large data sets. Similar to [18], to compare
            different model architectures we define first the computational complexity of a model as the number of
            parameters that need to be accessed to fully train the model. Next, we will try to maximize the accuracy,
            while minimizing the computational complexity. 1
            The test set is available at www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt
            http://ronan.collobert.com/senna/ http://metaoptimize.com/projects/wordreprs/
            http://www.fit.vutbr.cz/˜imikolov/rnnlm/ http://ai.stanford.edu/˜ehhuang/ 2
            2

            For all the following models, the training complexity is proportional to
            O = E × T × Q,
            (1)

            where E is number of the training epochs, T is the number of the words in the training set and Q is
            defined further for each model architecture. Common choice is E = 3 − 50 and T up to one billion. All models
            are trained using stochastic gradient descent and backpropagation [26]. 2.1
            Feedforward Neural Net Language Model (NNLM)

            The probabilistic feedforward neural network language model has been proposed in [1]. It consists
            of input, projection, hidden and output layers. At the input layer, N previous words are encoded using
            1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P
            that has dimensionality N × D, using a shared projection matrix. As only N inputs are active at any given
            time, composition of the projection layer is a relatively cheap operation. The NNLM architecture becomes
            complex for computation between the projection and the hidden layer, as values in the projection layer are
            dense. For a common choice of N = 10, the size of the projection layer (P ) might be 500 to 2000, while the
            hidden layer size H is typically 500 to 1000 units. Moreover, the hidden layer is used to compute
            probability distribution over all the words in the vocabulary, resulting in an output layer with
            dimensionality V . Thus, the computational complexity per each training example is Q = N × D + N × D × H + H
            × V,
            (2)

            where the dominating term is H × V . However, several practical solutions were proposed for
            avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models
            completely by using models that are not normalized during training [4, 9]. With binary tree representations
            of the vocabulary, the number of output units that need to be evaluated can go down to around log2 (V ).
            Thus, most of the complexity is caused by the term N × D × H. In our models, we use hierarchical softmax
            where the vocabulary is represented as a Huffman binary tree. This follows previous observations that the
            frequency of words works well for obtaining classes in neural net language models [16]. Huffman trees assign
            short binary codes to frequent words, and this further reduces the number of output units that need to be
            evaluated: while balanced binary tree would require log2 (V ) outputs to be evaluated, the Huffman tree
            based hierarchical softmax requires only about log2 (U nigram perplexity(V )). For example when the
            vocabulary size is one million words, this results in about two times speedup in evaluation. While this is
            not crucial speedup for neural network LMs as the computational bottleneck is in the N ×D ×H term, we will
            later propose architectures that do not have hidden layers and thus depend heavily on the efficiency of the
            softmax normalization. 2.2
            Recurrent Neural Net Language Model (RNNLM)

            Recurrent neural network based language model has been proposed to overcome certain limitations
            of the feedforward NNLM, such as the need to specify the context length (the order of the model N ), and
            because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks
            [15, 2]. The RNN model does not have a projection layer; only input, hidden and output layer. What is
            special for this type of model is the recurrent matrix that connects hidden layer to itself, using
            time-delayed connections. This allows the recurrent model to form some kind of short term memory, as
            information from the past can be represented by the hidden layer state that gets updated based on the
            current input and the state of the hidden layer in the previous time step. The complexity per training
            example of the RNN model is Q = H × H + H × V,
            (3)

            where the word representations D have the same dimensionality as the hidden layer H. Again, the
            term H × V can be efficiently reduced to H × log2 (V ) by using hierarchical softmax. Most of the complexity
            then comes from H × H. 3
            2.3

            Parallel Training of Neural Networks

            To train models on huge data sets, we have implemented several models on top of a large-scale
            distributed framework called DistBelief [6], including the feedforward NNLM and the new models proposed in
            this paper. The framework allows us to run multiple replicas of the same model in parallel, and each replica
            synchronizes its gradient updates through a centralized server that keeps all the parameters. For this
            parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure
            called Adagrad [7]. Under this framework, it is common to use one hundred or more model replicas, each using
            many CPU cores at different machines in a data center.
            3

            New Log-linear Models

            In this section, we propose two new model architectures for learning distributed representations
            of words that try to minimize computational complexity. The main observation from the previous section was
            that most of the complexity is caused by the non-linear hidden layer in the model. While this is what makes
            neural networks so attractive, we decided to explore simpler models that might not be able to represent the
            data as precisely as neural networks, but can possibly be trained on much more data efficiently. The new
            architectures directly follow those proposed in our earlier work [13, 14], where it was found that neural
            network language model can be successfully trained in two steps: first, continuous word vectors are learned
            using simple model, and then the N-gram NNLM is trained on top of these distributed representations of
            words. While there has been later substantial amount of work that focuses on learning word vectors, we
            consider the approach proposed in [13] to be the simplest one. Note that related models have been proposed
            also much earlier [26, 8]. 3.1
            Continuous Bag-of-Words Model

            The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden
            layer is removed and the projection layer is shared for all words (not just the projection matrix); thus,
            all words get projected into the same position (their vectors are averaged). We call this architecture a
            bag-of-words model as the order of words in the history does not influence the projection. Furthermore, we
            also use words from the future; we have obtained the best performance on the task introduced in the next
            section by building a log-linear classifier with four future and four history words at the input, where the
            training criterion is to correctly classify the current (middle) word. Training complexity is then Q = N × D
            + D × log2 (V ). (4) We denote this model further as CBOW, as unlike standard bag-of-words model, it uses
            continuous distributed representation of the context. The model architecture is shown at Figure 1. Note that
            the weight matrix between the input and the projection layer is shared for all word positions in the same
            way as in the NNLM. 3.2
            Continuous Skip-gram Model

            The second architecture is similar to CBOW, but instead of predicting the current word based on the
            context, it tries to maximize classification of a word based on another word in the same sentence. More
            precisely, we use each current word as an input to a log-linear classifier with continuous projection layer,
            and predict words within a certain range before and after the current word. We found that increasing the
            range improves quality of the resulting word vectors, but it also increases the computational complexity.
            Since the more distant words are usually less related to the current word than those close to it, we give
            less weight to the distant words by sampling less from those words in our training examples. The training
            complexity of this architecture is proportional to Q = C × (D + D × log2 (V )),
            (5)

            where C is the maximum distance of the words. Thus, if we choose C = 5, for each training word
            we will select randomly a number R in range
            <
            1; C >, and then use R words from history and 4
            INPUT

            PROJECTION

            OUTPUT

            INPUT

            PROJECTION

            w(t-2)

            OUTPUT

            w(t-2)

            w(t-1)

            w(t-1)
            SUM w(t)
            w(t)

            w(t+1)

            w(t+1)

            w(t+2)

            w(t+2)

            CBOW

            Skip-gram

            Figure 1: New model architectures. The CBOW architecture predicts the current word based on the
            context, and the Skip-gram predicts surrounding words given the current word.
            R words from the future of the current word as correct labels. This will require us to do R × 2
            word classifications, with the current word as input, and each of the R + R words as output. In the
            following experiments, we use C = 10.
            4

            Results

            To compare the quality of different versions of word vectors, previous papers typically use a table
            showing example words and their most similar words, and understand them intuitively. Although it is easy to
            show that word France is similar to Italy and perhaps some other countries, it is much more challenging when
            subjecting those vectors in a more complex similarity task, as follows. We follow previous observation that
            there can be many different types of similarities between words, for example, word big is similar to bigger
            in the same sense that small is similar to smaller. Example of another type of relationship can be word
            pairs big - biggest and small - smallest [20]. We further denote two pairs of words with the same
            relationship as a question, as we can ask: ”What is the word that is similar to small in the same sense as
            biggest is similar to big?” Somewhat surprisingly, these questions can be answered by performing simple
            algebraic operations with the vector representation of words. To find a word that is similar to small in the
            same sense as biggest is similar to big, we can simply compute vector X = vector(”biggest”) − vector(”big”)
            + vector(”small”). Then, we search in the vector space for the word closest to X measured by cosine
            distance, and use it as the answer to the question (we discard the input question words during this search).
            When the word vectors are well trained, it is possible to find the correct answer (word smallest) using this
            method. Finally, we found that when we train high dimensional word vectors on a large amount of data, the
            resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and
            the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic
            relationships could be used to improve many existing NLP applications, such as machine translation,
            information retrieval and question answering systems, and may enable other future applications yet to be
            invented. 5
            Table 1: Examples of five types of semantic and nine types of syntactic questions in the SemanticSyntactic
            Word Relationship test set.
            Type of relationship Common capital city All capital cities Currency City-in-state Man-Woman Adjective to
            adverb Opposite Comparative Superlative Present Participle Nationality adjective Past tense Plural nouns
            Plural verbs
            4.1

            Word Pair 1
            Athens Greece Astana Kazakhstan Angola kwanza Chicago Illinois brother sister apparent apparently possibly
            impossibly great greater easy easiest think thinking Switzerland Swiss walking walked mouse mice work works
            Word Pair 2
            Oslo Norway Harare Zimbabwe Iran rial Stockton California grandson granddaughter rapid rapidly ethical
            unethical tough tougher lucky luckiest read reading Cambodia Cambodian swimming swam dollar dollars speak
            speaks
            Task Description

            To measure quality of the word vectors, we define a comprehensive test set that contains five types
            of semantic questions, and nine types of syntactic questions. Two examples from each category are shown in
            Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were
            created in two steps: first, a list of similar word pairs was created manually. Then, a large list of
            questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities
            and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have
            included in our test set only single token words, thus multi-word entities are not present (such as New
            York). We evaluate the overall accuracy for all question types, and for each question type separately
            (semantic, syntactic). Question is assumed to be correctly answered only if the closest word to the vector
            computed using the above method is exactly the same as the correct word in the question; synonyms are thus
            counted as mistakes. This also means that reaching 100% accuracy is likely to be impossible, as the current
            models do not have any input information about word morphology. However, we believe that usefulness of the
            word vectors for certain applications should be positively correlated with this accuracy metric. Further
            progress can be achieved by incorporating information about structure of words, especially for the syntactic
            questions. 4.2
            Maximization of Accuracy

            We have used a Google News corpus for training the word vectors. This corpus contains about
            6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we are facing
            time constrained optimization problem, as it can be expected that both using more data and higher
            dimensional word vectors will improve the accuracy. To estimate the best choice of model architecture for
            obtaining as good as possible results quickly, we have first evaluated models trained on subsets of the
            training data, with vocabulary restricted to the most frequent 30k words. The results using the CBOW
            architecture with different choice of word vector dimensionality and increasing amount of the training data
            are shown in Table 2. It can be seen that after some point, adding more dimensions or adding more training
            data provides diminishing improvements. So, we have to increase both vector dimensionality and the amount of
            the training data together. While this observation might seem trivial, it must be noted that it is currently
            popular to train word vectors on relatively large amounts of data, but with insufficient size 6
            Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word
            vectors from the CBOW architecture with limited vocabulary. Only questions containing words from the most
            frequent 30k words are used. Dimensionality / Training words 50 100 300 600
            24M
            13.4 19.4 23.2 24.0
            49M
            15.7 23.1 29.2 30.1
            98M
            18.6 27.8 35.3 36.5
            196M
            19.1 28.7 38.6 40.8
            391M
            22.5 33.4 43.7 46.6
            783M
            23.2 32.2 45.9 50.4
            Table 3: Comparison of architectures using models trained on the same data, with 640-dimensional
            word vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the
            syntactic relationship test set of [20] Model Architecture RNNLM NNLM CBOW Skip-gram
            Semantic-Syntactic Word Relationship test set
            Semantic Accuracy [%] Syntactic Accuracy [%] 9 36 23 53 24 64 55 59
            MSR Word Relatedness
            Test Set [20] 35 47 61 56
            (such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the
            same increase of computational complexity as increasing vector size twice. For the experiments reported in
            Tables 2 and 4, we used three training epochs with stochastic gradient descent and backpropagation. We chose
            starting learning rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last
            training epoch. 4.3
            Comparison of Model Architectures

            First we compare different model architectures for deriving the word vectors using the same training
            data and using the same dimensionality of 640 of the word vectors. In the further experiments, we use full
            set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to the 30k
            vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic similarity
            between words3 . The training data consists of several LDC corpora and is described in detail in [18] (320M
            words, 82K vocabulary). We used these data to provide a comparison to a previously trained recurrent neural
            network language model that took about 8 weeks to train on a single CPU. We trained a feedforward NNLM with
            the same number of 640 hidden units using the DistBelief parallel training [6], using a history of 8
            previous words (thus, the NNLM has more parameters than the RNNLM, as the projection layer has size 640 ×
            8). In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly on
            the syntactic questions. The NNLM vectors perform significantly better than the RNN - this is not
            surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden layer. The CBOW
            architecture works better than the NNLM on the syntactic tasks, and about the same on the semantic one.
            Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but
            still better than the NNLM), and much better on the semantic part of the test than all the other models.
            Next, we evaluated our models trained using one CPU only and compared the results against publicly available
            word vectors. The comparison is given in Table 4. The CBOW model was trained on subset 3
            We thank Geoff Zweig for providing us the test set.

            7

            Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relationship test set,
            and word vectors from our models. Full vocabularies are used.
            Model
            Vector
            Dimensionality
            Collobert-Weston NNLM
            Turian NNLM Turian NNLM Mnih NNLM Mnih NNLM Mikolov RNNLM Mikolov RNNLM Huang NNLM Our NNLM Our NNLM Our
            NNLM CBOW Skip-gram
            50
            50 200 50 100 80 640 50 20 50 100 300 300
            Training
            words
            Accuracy [%]
            Semantic 9.3 1.4 1.4 1.8 3.3 4.9 8.6 13.3 12.9 27.9 34.2 15.5 50.0
            660M
            37M 37M 37M 37M 320M 320M 990M 6B 6B 6B 783M 783M
            Syntactic
            12.3 2.6 2.2 9.1 13.2 18.4 36.5 11.6 26.4 55.8 64.5 53.1 55.9
            Total
            11.0 2.1 1.8 5.8 8.8 12.7 24.6 12.3 20.3 43.2 50.8 36.1 53.3
            Table 5: Comparison of models trained for three epochs on the same data and models trained for
            one epoch. Accuracy is reported on the full Semantic-Syntactic data set. Model
            3 epoch CBOW
            3 epoch Skip-gram 1 epoch CBOW 1 epoch CBOW 1 epoch CBOW 1 epoch Skip-gram 1 epoch Skip-gram 1 epoch
            Skip-gram
            Vector
            Dimensionality
            Training
            words
            300
            300 300 300 600 300 300 600
            783M
            783M 783M 1.6B 783M 783M 1.6B 783M
            Accuracy [%]
            Semantic 15.5 50.0 13.8 16.1 15.4 45.6 52.2 56.7
            Syntactic
            53.1 55.9 49.9 52.6 53.3 52.2 55.1 54.5
            Training time
            [days] Total 36.1 53.3 33.6 36.1 36.2 49.2 53.8 55.5
            1
            3 0.3 0.6 0.7 1 2 2.5
            of the Google News data in about a day, while training time for the Skip-gram model was about three
            days. For experiments reported further, we used just one training epoch (again, we decrease the learning
            rate linearly so that it approaches zero at the end of training). Training a model on twice as much data
            using one epoch gives comparable or better results than iterating over the same data for three epochs, as is
            shown in Table 5, and provides additional small speedup. 4.4
            Large Scale Parallel Training of Models

            As mentioned earlier, we have implemented various models in a distributed framework called DistBelief. Below
            we report the results of several models trained on the Google News 6B data set,
            with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Adagrad [7].
            We used 50 to 100 model replicas during the training. The number of CPU cores is an 8
            Table 6: Comparison of models trained using the DistBelief distributed framework. Note that
            training of NNLM with 1000-dimensional vectors would take too long to complete. Model
            NNLM
            CBOW Skip-gram
            Vector
            Dimensionality 100 1000 1000
            Training
            words 6B 6B 6B
            Accuracy [%]
            Semantic 34.2 57.3 66.1
            Syntactic
            64.5 68.9 65.1
            Training time
            [days x CPU cores] Total 50.8 63.7 65.6
            14 x 180
            2 x 140 2.5 x 125
            Table 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.
            Architecture 4-gram [32] Average LSA similarity [32] Log-bilinear model [24] RNNLMs [19] Skip-gram Skip-gram
            + RNNLMs
            Accuracy [%]
            39 49 54.8 55.4 48.0 58.9
            estimate since the data center machines are shared with other production tasks, and the usage can
            fluctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of the CBOW
            model and the Skip-gram model are much closer to each other than their single-machine implementations. The
            result are reported in Table 6. 4.5
            Microsoft Research Sentence Completion Challenge

            The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing
            language modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one word is
            missing in each sentence and the goal is to select word that is the most coherent with the rest of the
            sentence, given a list of five reasonable choices. Performance of several techniques has been already
            reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a
            combination of recurrent neural networks that currently holds the state of the art performance of 55.4%
            accuracy on this benchmark [19]. We have explored the performance of Skip-gram architecture on this task.
            First, we train the 640dimensional model on 50M words provided in [32]. Then, we compute score of each
            sentence in the test set by using the unknown word at the input, and predict all surrounding words in a
            sentence. The final sentence score is then the sum of these individual predictions. Using the sentence
            scores, we choose the most likely sentence. A short summary of some previous results together with the new
            results is presented in Table 7. While the Skip-gram model itself does not perform on this task better than
            LSA similarity, the scores from this model are complementary to scores obtained with RNNLMs, and a weighted
            combination leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set
            and 58.7% on the test part of the set).
            5

            Examples of the Learned Relationships

            Table 8 shows words that follow various relationships. We follow the approach described above: the
            relationship is defined by subtracting two word vectors, and the result is added to another word. Thus for
            example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although there is clearly
            a lot of room for further improvements (note that using our accuracy metric that 9
            Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skipgram model
            trained on 783M words with 300 dimensionality).
            Relationship France - Paris big - bigger Miami - Florida Einstein - scientist Sarkozy - France copper - Cu
            Berlusconi - Silvio Microsoft - Windows Microsoft - Ballmer Japan - sushi
            Example 1
            Italy: Rome small: larger Baltimore: Maryland Messi: midfielder Berlusconi: Italy zinc: Zn Sarkozy: Nicolas
            Google: Android Google: Yahoo Germany: bratwurst
            Example 2
            Japan: Tokyo cold: colder Dallas: Texas Mozart: violinist Merkel: Germany gold: Au Putin: Medvedev IBM:
            Linux IBM: McNealy France: tapas
            Example 3
            Florida: Tallahassee quick: quicker Kona: Hawaii Picasso: painter Koizumi: Japan uranium: plutonium Obama:
            Barack Apple: iPhone Apple: Jobs USA: pizza
            assumes exact match, the results in Table 8 would score only about 60%). We believe that word
            vectors trained on even larger data sets with larger dimensionality will perform significantly better, and
            will enable the development of new innovative applications. Another way to improve accuracy is to provide
            more than one example of the relationship. By using ten examples instead of one to form the relationship
            vector (we average the individual vectors together), we have observed improvement of accuracy of our best
            models by about 10% absolutely on the semantic-syntactic test. It is also possible to apply the vector
            operations to solve different tasks. For example, we have observed good accuracy for selecting
            out-of-the-list words, by computing average vector for a list of words, and finding the most distant word
            vector. This is a popular type of problems in certain human intelligence tests. Clearly, there is still a
            lot of discoveries to be made using these techniques.
            6
        </corps>
        <conclusion>Conclusion

            In this paper we studied the quality of vector representations of words derived by various models on
            a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality
            word vectors using very simple model architectures, compared to the popular neural network models (both
            feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute
            very accurate high dimensional word vectors from a much larger data set. Using the DistBelief distributed
            framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion
            words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the
            best previously published results for similar models. An interesting task where the word vectors have
            recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2
            [11]. The publicly available RNN vectors were used together with other techniques to achieve over 50%
            increase in Spearman’s rank correlation over the previous best result [31]. The neural network based word
            vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase
            detection [28]. It can be expected that these applications can benefit from the model architectures
            described in this paper. Our ongoing work shows that the word vectors can be successfully applied to
            automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts.
            Results from machine translation experiments also look very promising. In the future, it would be also
            interesting to compare our techniques to Latent Relational Analysis [30] and others. We believe that our
            comprehensive test set will help the research community to improve the existing techniques for estimating
            the word vectors. We also expect that high quality word vectors will become an important building block for
            future NLP applications. 10
            7

            Follow-Up Work

            After the initial version of this paper was written, we published single-machine multi-threaded C++
            code for computing the word vectors, using both the continuous bag-of-words and skip-gram architectures4 .
            The training speed is significantly higher than reported earlier in this paper, i.e. it is in the order of
            billions of words per hour for typical hyperparameter choices. We also published more than 1.4 million
            vectors that represent named entities, trained on more than 100 billion words. Some of our follow-up work
            will be published in an upcoming NIPS 2013 paper [21].
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>[1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Machine
            Learning Research, 3:1137-1155, 2003. [2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In:
            Large-Scale Kernel Machines, MIT Press, 2007. [3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
            Large language models in machine translation. In Proceedings of the Joint Conference on Empirical Methods in
            Natural Language Processing and Computational Language Learning, 2007. [4] R. Collobert and J. Weston. A
            Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In
            International Conference on Machine Learning, ICML, 2008. [5] R. Collobert, J. Weston, L. Bottou, M. Karlen,
            K. Kavukcuoglu and P. Kuksa. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning
            Research, 12:24932537, 2011. [6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A.
            Ranzato, A. Senior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012. [7]
            J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
            optimization. Journal of Machine Learning Research, 2011. [8] J. Elman. Finding Structure in Time. Cognitive
            Science, 14, 179-211, 1990. [9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word
            Representations via Global Context and Multiple Word Prototypes. In: Proc. Association for Computational
            Linguistics, 2012. [10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In:
            Parallel distributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations, MIT
            Press, 1986. [11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring
            degrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic Evaluation
            (SemEval 2012), 2012. [12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word
            vectors for sentiment analysis. In Proceedings of ACL, 2011. [13] T. Mikolov. Language Modeling for Speech
            Recognition in Czech, Masters thesis, Brno University of Technology, 2007. [14] T. Mikolov, J. Kopecký, L.
            Burget, O. Glembek and J. Černocký. Neural network based language models for higly inflective languages,
            In: Proc. ICASSP 2009. [15] T. Mikolov, M. Karafiát, L. Burget, J. Černocký, S. Khudanpur. Recurrent
            neural network based language model, In: Proceedings of Interspeech, 2010. [16] T. Mikolov, S. Kombrink, L.
            Burget, J. Černocký, S. Khudanpur. Extensions of recurrent neural network language model, In: Proceedings
            of ICASSP 2011. [17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. Černocký. Empirical Evaluation and
            Combination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011. 4
            The code is available at https://code.google.com/p/word2vec/

            11

            [18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. Černocký. Strategies for Training Large Scale
            Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understanding, 2011. [19] T.
            Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology,
            2012. [20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Representations.
            NAACL HLT 2013. [21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations
            of Words and Phrases and their Compositionality. Accepted to NIPS 2013. [22] A. Mnih, G. Hinton. Three new
            graphical models for statistical language modelling. ICML, 2007. [23] A. Mnih, G. Hinton. A Scalable
            Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems 21, MIT Press,
            2009. [24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language models.
            ICML, 2012. [25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,
            2005. [26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by
            backpropagating errors. Nature, 323:533.536, 1986. [27] H. Schwenk. Continuous space language models.
            Computer Speech and Language, vol. 21, 2007. [28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D.
            Manning. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011. [29]
            J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for Semi-Supervised
            Learning. In: Proc. Association for Computational Linguistics, 2010. [30] P. D. Turney. Measuring Semantic
            Similarity by Latent Relational Analysis. In: Proc. International Joint Conference on Artificial
            Intelligence, 2005. [31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models
            for Measuring Relational Similarity. NAACL HLT 2013. [32] G. Zweig, C.J.C. Burges. The Microsoft Research
            Sentence Completion Challenge, Microsoft Research Technical Report MSR-TR-2011-129, 2011.
            12
        </biblio>
    </article>
</articles>
