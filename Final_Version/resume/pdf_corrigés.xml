<articles>
    <article>
        <preamble>Cabrera_RESUMES_2019.pdf</preamble>
        <titre>Ranking résumés automatically using only résumés: A method free of job offers</titre>
        <auteur>
            E-mail addresses: diegol@edgehill.ac.uk (L.A. Cabrera-Diego), marc.elbeze@univavignon.fr (M. El-Béze),
            juan-manuel.torres@univ-avignon.fr (J.-M. Torres-Moreno),
            durette@adoc-tm.com (B. Durette).
            1
            Present address: Department of Computing, Edge Hill University, St. Helens
            Road, L39 4QP Ormskirk, UK
            https://doi.org/10.1016/j.eswa.2018.12.054
            0957-4174/© 2019 Elsevier Ltd. All rights reserved.
        </auteur>
        <abstract>
            With the success of the electronic recruitment, now it is easier to ﬁnd a job offer and apply for it.
            However, due to this same success, nowadays, human resource managers tend to receive high volumes of
            applications for each job offer. These applications turn into large quantities of documents, known as
            résumés or curricula vitae, that need to be processed quickly and correctly. To reduce the time necessary
            to process the résumés, human resource managers have been working with the scientiﬁc community to
            create systems that automate their ranking. Until today, most of these systems are based on the comparison
            of job offers and résumés. Nevertheless, this comparison is impossible to do in data sets where job
            offers are no longer available, as it happens in this work. We present two methods to rank résumés that
            do not use job offers or any semantic resource, unlike existing state-of-the-art systems. The methods are
            based on what we call Inter-Résumé Proximity, which is the lexical similarity between only résumés sent
            by candidates in response to the same job offer. Besides, we propose the use of Relevance Feedback, at
            general and lexical levels to improve the ranking of résumés. Relevance Feedback is applied using techniques
            based on similarity coeﬃcients and vocabulary scoring. All the methods have been tested on a
            large corpus of 171 real selection processes, which correspond to more than 14,0 0 0 résumés. The developed
            methods can rank correctly, in average, 93% of the résumés sent to each job posting. The outcomes
            presented here show that it is not necessary to use job offers or semantic resources to provide high
            quality results. Furthermore, we observed that résumés have particular characteristics that as ensemble,
            work as a facial composite and provide more information about the job posting than the job offer. This
            certainly will change how systems analyze and rank résumés.
            © 2019 Elsevier Ltd. All rights reserved.
        </abstract>
        <introduction>
            For at least 15 years, the process of attracting possible candidates for a job, i.e., recruitment process,
            moved from traditional
            means, like newspapers and job boards, to the Internet and started
            to be known as electronic recruitment or e-Recruitment (Kessler,
            Béchet, Roche, Torres-Moreno, & El-Bèze, 2012; Radevski & Trichet,
            2006).
            The success of e-Rectruitment over traditional recruitment processes lies in the advantages it brings to
            users and especially to
            ∗
            Expert Systems With Applications 123 (2019) 91–107

            Contents lists available at ScienceDirect

            Expert Systems With Applications
            journal homepage: www.elsevier.com/locate/eswa

            Ranking résumés automatically using only résumés: A method free of
            job offers
            Luis Adrián Cabrera-Diego a,c,1,∗, Marc El-Béze a, Juan-Manuel Torres-Moreno a,b,
            Barthélémy Durette c
            a
            b
            c

            LIA, Avignon Université, 91022 Chemin des Meinajariès, Avignon 84022, France
            Polytechnique Montréal, Canada
            Adoc Talent Management, 21 Rue du Faubourg Saint-Antoine, Paris 75011, France

            a r t i c l e

            i n f o

            Article history:
            Received 18 May 2018
            Revised 30 November 2018
            Accepted 29 December 2018
            Available online 31 December 2018
            Keywords:
            Résumé
            Curriculum vitae
            Recommendation system
            Relevance feedback
            e-Recruitment
            Ranking
            Mean average precision


            Human Resources Managers (HRMs). Today, due to e-Recruitment,
            job offers can more easily reach not only specialized communities (Arthur, 2001, page 126) but also wider
            audiences locally, nationally or internationally (Montuschi, Gatteschi, Lamberti, Sanna,
            & Demartini, 2014). HRMs’ operational costs have been reduced, in
            certain cases to one-twentieth of the original expenses (Chapman
            & Webster, 2003). Now, job seekers can search for job offers
            through the Internet (Looser, Ma, & Schewe, 2013) and apply to
            them faster by sending an e-mail or ﬁlling out a web form with
            an electronic résumé or CV attached (Elkington, 2005). The greatest e-Recruitment’s advantage is the
            possibility of being in contact with job seekers, employers and HRM all the time around the
            world (Barber, 2006, page 1).
            Although e-Recruitment has helped HRMs with the task of
            identifying and attracting potential candidates, its use has brought
            a number of undesirable consequences, especially when high vol-

            92

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            umes of applications are received (Barber, 2006, page 11). After the
            recruitment process, an HRM must select the group of applicants
            that are relevant for the job offered. This selection is performed
            by manually screening résumés.2 The manual screening consists
            of examining and comparing applicant information, found in the
            résumé, with respect to the speciﬁcations of the position or person speciﬁcation3 (Armstrong and Taylor,
            2014, Page 226). However,
            given the large number of applications, HRMs have trouble screening them correctly and rapidly (Trichet,
            Bourse, Leclère, & Morin,
            2004). Furthermore, HRMs have seen an increase in applications
            from unqualiﬁed candidates (Faliagka, Kozanidis, Stamou, Tsakalidis, & Tzimas, 2011), meaning they lose
            valuable time during the
            screening process.
            The scientiﬁc community has proposed multiple systems to reduce the negative impacts of e-Recruitment. The
            vast majority of
            the developed systems are based on comparing résumés and job
            offers, e.g., using measures like Cosine Similarity (Kessler, Béchet,
            Torres-Moreno, Roche, & El-Bèze, 2009; Singh, Rose, Visweswariah,
            Chenthamarakshan, & Kambhatla, 2010). In some cases, to improve the matching, they include ontologies or
            semantic resources
            that are expected to ameliorate the similarity between documents, like those shown in Senthil Kumaran and
            Sankar (2013) and
            Montuschi et al. (2014).
            The work that is here presented occurs in the following context. It is the outcome of a collaboration
            project with a Human
            Resources enterprise that had a large database of recruitment and
            selection processes conducted by them previously. The database is
            divided by job postings4 in which we can ﬁnd the applications sent
            by the interested or directly contacted candidates. Each application
            is composed, at least, of a résumé and the outcome of the selection process. This database, however, has a
            particular characteristic,
            for most of the job postings, neither the job offer nor the person
            speciﬁcation is available.5 This characteristic is due to the software
            used to store automatically the incoming applications did not provide the option to keep these documents.
            Due to the fact that it is impossible to apply state-of-the-art’s
            methods for all the database, we decided to explore how to rank
            résumés without making use of job offers. The result of this exploration are innovative and simple methods
            that use uniquely
            the proximity between résumés sent for the same job posting.
            To this end, we use a similarity measure and Relevance Feedback
            (Rocchio, 1971) applied with methods based on a similarity quotient and a vocabulary scoring.
            Despite in this work, we do not make use of more complex
            methods, like deep-learning neural networks, or dense text representations, i.e., word embedding, the idea
            of using them was always present. There were several reasons why not to use these

            techniques, but the main was that in Cabrera-Diego, Durette, Lafon,
            Torres-Moreno, and El-Bèze (2015) we started to observe that résumés could be used to rank themselves using
            similarity measures.
            Thus, a simple method, like the one here presented could work.
            Moreover, by using methods based on neural networks, we reduce
            the chances of understanding and providing the reasons of why a
            candidate has been chosen to be interviewed, something that it is
            being looked for, like in Martinez-Gil, Paoletti, and Schewe (2016).
            The results obtained from applying our methods over a large set
            of real recruitment and selection processes, show that our methods, despite not using job offers or semantic
            resources, can reach
            great performance. By just applying the method based on résumés
            proximity, we can rank correctly in average 61% of the résumés
            sent for a job posting. Nonetheless, this value can reach 93% when
            it is used along with our proposed Relevance Feedback methods,
            in which an HRM just need to analyze 20 résumés per job posting,
            i.e., no more than 50% of the applications sent to the job posting.
            In summary, this work present multiple and diverse contributions. The ﬁrst contribution is that we offer an
            innovative method,
            completely different to the ones found in the state-of-the-art, that
            can rank résumés correctly and automatically. Although this system is used in a very speciﬁc context, where
            job offers are not
            always present, it can be applied in any condition where the goal
            is to rank résumés sent to the very same job posting. The second
            contribution is the use of two different Relevance Feedback that
            can improve to a great extent other résumé ranking systems. The
            third and ﬁnal contribution is the methodology used in this article, which can be used by people to do a
            posteriori analyses of selection processes. For example, HRMs can use the methodology to
            understand how the selection of candidates was done and which
            were the keywords that represented the selected and rejected candidates. As well, HRMs can use the tool to
            determine whether a
            candidate that should have been called for an interview was left
            aside. Whereas, psychologist can use the outcome of our methods
            as a way to determine whether HRM infers aspects like personality
            (Cole, Feild, Giles, & Harris, 2009) or whether they are affected by
            errors like misspellings (Martin-Lacroux, 2017). In addition, other
            systems could use our methods’ outputs to generate feedback that
            rejected candidates could ﬁnd useful to improve their proﬁles.
            This work is divided into eight sections. In Section 2, we
            introduce the state-of-the-art methods and our previous work.
            The methodology and the data are explained in Section 3 and
            Section 4, respectively. We introduce the experimental and evaluative settings in Section 5. The outcomes
            from the experiments
            are presented in Section 6. We discuss the results in Section 7.
            The work’s conclusions and possible future work are presented in
            Section 8.
        </introduction>
        <corps>
            2. Related work
            2
            According to Thompson (20 0 0), a résumé, also known as resume, curriculum vitae or CV, is a document
            prepared by a job candidate, for potential employers, that
            describes one’s education, qualiﬁcations and professional experience. In this paper
            we will use résumé as common term.
            3
            This is a document detailing which characteristics, mandatory and optional,
            should be found in a résumé according to the employer. This document can evolve
            through the time depending on the job market.
            4
            A job posting is composed of three elements: a job offer, a person speciﬁcation and a set of applications.
            The job offer is the document that describes the job
            position (e.g., technician, researcher) but also which are the characteristics that are
            searched; this document is visible to the job seekers. The person speciﬁcation, see
            Footnote 3, is a document only accessible to the HRM and the employer. The set
            of applications corresponds to the résumés and other documents that are proportioned by the job seekers
            interested in the job offer.
            5
            At the beginning of this work, none of the job postings was linked with its
            respective job offer. However, after a manual search, we arrived to manually link
            a portion of job postings, from the database, with their respective job offers. With
            this subset we created a baseline.

            In 2002, Harzallah, Leclère, and Trichet (2002) presented the
            project CommOnCV that consists of an automatic analysis and
            matching of competencies between résumés and job offers. To the
            best of our knowledge, this was the ﬁrst project where the scientiﬁc community became interested in the
            automated analysis of
            résumés. Since the publication of this project, several systems were
            developed with different approaches and goals. We have grouped
            the systems into three types: Résumé matchers, Résumé classiﬁers
            and Résumé rankers.
            Résumé matchers are systems created for on-line job boards
            that match uploaded résumés with a job offer or a query, e.g.,
            García-Sánchez, Martínez-Béjar, Contreras, Fernández-Breis, and
            Castellanos-Nieves (2006); Guo, Alamudun, and Hammond (2016);
            Radevski and Trichet (2006); Sen, Das, Ghosh, and Ghosh (2012).
            To achieve the matching of résumés, these systems use mainly on-

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            tologies and rules, but they can use some kind of Relevance Feedback,6 like in Hutterer (2011) to improve
            the match results.
            Résumé classiﬁers consist of systems that bypass HRMs by automatically classifying résumés into relevant or
            irrelevant candidates. These kinds of systems, such as Kessler, Torres-Moreno, and
            El-Bèze (2008b) and Faliagka et al. (2013), use machine learning
            methods to perform this task. In other words, they create a model
            using data from previous selection processes. The model contains,
            in theory, the features that make an applicant to appear relevant
            or irrelevant to an HRM.
            Résumé rankers are systems that sort résumés based on proximity between a résumé and a job offer, or even
            others résumés.
            As these systems propose rankings, an HRM can decide the point
            in which résumés become irrelevant for a job and stop reading
            them. In this kind of systems, proximity between elements can
            be lexical (Cabrera-Diego, 2015; Kessler, Béchet, Roche, El-Bèze, &
            Torres-Moreno, 2008a; Singh et al., 2010), semantic (Kmail, Maree,
            & Belkhatir, 2015; Montuschi et al., 2014; Tinelli, Colucci, Donini,
            Di Sciascio, & Giannini, 2017) or ontological (Senthil Kumaran &
            Sankar, 2013). In the following paragraphs we discuss the most
            representative résumé rankers found in the literature.
            E-Gen (Kessler et al., 2009) is a system that can create résumé rankings based on the lexical proximity
            between résumés
            and a particular job offer. More speciﬁcally, E-Gen compares résumés and a speciﬁc job offer using measures
            such as Cosine Similarity and Minkowski Distance. The résumés are ranked according to how proximal they are
            to the job offer. The documents,
            i.e., résumés and job offers, are represented using a Vector Space
            Model. As well, they make use of a Relevance Feedback method
            that consists of enriching the job offer vocabulary by concatenating already analyzed relevant résumés from
            the same job posting.
            In Kessler et al. (2012) the authors improved the system’s performance by adding an automatic text
            summarization tool to obtain
            the most relevant information from job offers and résumés.
            PROSPECT is a system developed by Singh et al. (2010) that
            has a résumé ranker among its tools. PROSPECT extracts relevant
            information from résumés and job offers using Conditional Random Fields (CRF), a lexicon, a named-entity
            recognizer and a data
            normalizer. Then, to rank the résumés based on the job offer,
            PROSPECT compares the information from both documents using
            Okapi BM25, Kullback-Leibler Divergence or Lucene Scoring.
            We note in the literature the LO-MATCH platform
            (Montuschi et al., 2014). It is a web-based system developed
            to match professional competencies from résumés and job offers.
            The LO-MATCH platform is based on ontologies which are used
            to enhance information from résumés and job offers. The ranking
            of résumés with respect to a job offer is determined through
            semantic similarity. LO-MATCH establishes to what degree the
            words found in a résumé have similar or related meanings to the
            words occurring in a job offer. The résumés most similar to the
            job offer are ranked near the top.
            EXPERT (Senthil Kumaran & Sankar, 2013) is another system that
            ranks résumés. However, each résumé and job offer is individually
            represented by an ontology. To generate each ontology, EXPERT analyzes the information with an ontology and
            a set of previously deﬁned rules (Senthil Kumaran & Sankar, 2012). EXPERT ranks the résumés by determining
            how close the job offer ontology is with respect to each résumé ontology. The résumés with ontologies most
            similar to those of the job offer are ranked near the top.
            MatchingSem (Kmail et al., 2015) is a ranking system designed
            to use multiple ontologies to ﬁnd the most similar résumés for

            6
            Relevance Feedback is the interaction of a human user with an information retrieval system, in order to
            evaluate its results and to modify requests for improving
            data retrieval Rocchio (1971).

            93

            a job posting. The reason to design a system capable to extract
            information from multiple ontologies is to represent several domains and/or decrease their lack of coverage.
            Thanks to ontologies,
            MatchingSem can create semantic networks that are matched using the Jaro-Winkler distance.
            I.M.P.A.K.T. (Tinelli et al., 2017) is a platform that allows HRM
            ranking candidates automatically and obtain the reasons of putting
            a résumé in a certain position. It is based on Relational database
            Management Systems which help in the creation of improved
            knowledge bases. As well, the platform allows deﬁning which competencies are required and which are only
            desired. I.M.P.A.K.T. offers to HRMs information about conﬂicts or underspeciﬁed features
            found in a résumé.
            Another résumé ranker is the one detailed in our previous work
            (Cabrera-Diego, 2015). There, we present the ﬁrst version of the
            method that in this work is extended and improved. It consists of
            using a measure that we call Average Inter-Résumé Proximity (AIRP).
            This measure determines the relevance of a résumé according to
            how similar it is to other résumés from the same job posting. To
            improve the ranking of résumés, we use Relevance Feedback and
            apply it with a factor that increases when a résumé is closer to
            those considered by an HRM as relevant.
            In the last years, some other researchers have worked on tasks
            related to the automatic ranking of résumés. For example, in
            Martinez-Gil et al. (2016) the authors propose an approach to improve the ranking of résumés by matching
            learning; as well, how
            to use matching learning to represent, in the future, documents
            using a common vocabulary. As well, related to the previous work,
            Martinez-Gil, Paoletti, Rácz, Sali, and Schewe (2018) propose a theory of how to match résumés and job
            offers, but also ranking
            them by using knowledge bases, lattice graphs and lattice ﬁlters.
            Another example is the analysis of social media to evaluate the
            emotional intelligence of candidates (Menon & Rahulnath, 2016).
            In Zaroor, Maree, and Sabha (2017), for instance, résumés and job
            offers are classiﬁed automatically in occupational categories; semantic networks are used to ﬁnd the best
            matching between these
            documents.

            3. Methodology
            Our methodology is composed of two parts. In the ﬁrst part, we
            determine the similarity of résumés in order to rank them. In the
            second, which is optional although suggested, we ask the HRM for
            Relevance Feedback and apply it. More speciﬁcally, the methodology used in this article is composed of ﬁve
            steps which are graphically represented in Fig. 1.
            In step I, we calculate the proximity between pairs of résumés
            using Inter-Résumé Proximity (Section 3.1). Once all the proximity
            values have been calculated, we estimate the Average or Median
            Inter-Résumé Proximity for each résumé in step II (Section 3.2). It
            is in this step where we formulate the hypothesis that the resulting values indicate the relevance of the
            résumés for the job posting. In step III, we sort the scores obtained in step II in descending
            order to rank the résumés.
            If we want to improve a ranking, we can make use of Relevance Feedback (Section 3.3). This process starts in
            step IV where
            an HRM analyzes a small set of résumés in order to determine
            whether they are relevant or not. Furthermore, they can identify
            and sort the terms that represent better relevancy. Once the HRM
            has ﬁnished, the Relevance Feedback is processed in step V. In this
            case, we can process the Relevance Feedback using the Relevance
            Factor (Section 3.3.1) and Vocabulary Scoring (Section 3.3.2). The
            output of the Relevance Feedback is then introduced in step III to
            re-rank the remaining résumés, i.e those not seen during the Relevance Feedback.

            94

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            Fig. 1. Methodology overview.

            3.1. Inter-Résumé Proximity

            3.2. Average and median Inter-Résumé Proximity

            The Inter-Résumé Proximity (IRP) is deﬁned as the degree of similarity between two résumés that were sent by
            different candidates
            applying for the same job positing. To mathematically deﬁne the
            IRP, consider J as the set of résumés gathering all the candidates
            that applied to the same job posting, J = {r1 , r2 , r3 , . . . r j }. Every
            résumé r in J is unique and from a different applicant, i.e. there
            are no duplicated résumés or candidates in the job posting. We
            present the deﬁnition of Inter-Résumé Proximity (IRP) by Eq. (1).

            In Cabrera-Diego et al. (2015) we determined through a statistical analysis that, on average, the similarity
            between relevant
            résumés is greater than the similarity between irrelevant ones.
            Equally, we observed that relevant résumés tend to be dissimilar
            to the group of irrelevant résumés. From this outcome, we can infer that relevant résumés should have
            multiple terms in common,
            while irrelevant résumés should present a variety of terms that are
            not shared, either by other irrelevant résumés or by the relevant
            ones. Based on this interpretation, we designed what we call the
            Average Inter-Résumé Proximity (AIRP). It is a method of ﬁnding relevant résumés based on their proximity to
            other résumés. The concept is that a relevant résumé will have, on average, higher values
            of IRP than an irrelevant résumé.8
            The mathematical deﬁnition of AIRP is presented by Eq. (4).

            IRP(r, rx ) = σ (r, rx ); ∀r = rx ; r, rx ∈ J

            (1)

            where r and rx are two different résumés from J; σ is a proximity
            measure.
            In this study, we use Dice’s Coeﬃcient as σ because in CabreraDiego et al. (2015) we observed, through
            statistical analyses, that
            this similarity measure is the most adequate for this task.7 Although Dice’s Coeﬃcient is frequently deﬁned
            in terms of sets, as
            in Eq. (2), we have redeﬁned it in Eq. (3) to be used in a vector
            representation.

            2 · |r ∩ rx |
            |r | + |rx |
            
            2 · n min(αi , αxi )
            Dice’s Coeﬃcient(r, rx ) = n i
            n
            i αi +
            i αxi
            Dice’s Coeﬃcient(r, rx ) =

            (2)
            (3)

            where r = {α1 , α2 , . . . , αn } and rx = {αx1 , αx2 , . . . , αxn } are vector
            representations of the résumés r and rx respectively. Each vector
            has n dimensions and their components are expressed by α ; min
            is a function that outputs the smallest component between r and
            rx in each vector dimension.
            Note that Dice’s Coeﬃcient has a closed interval [0, 1], where 1
            means that both documents are identical and 0 indicates they are
            completely different and have nothing in common.
            7
            Other measures tested in Cabrera-Diego et al. (2015) were Cosine Similarity, Jaccard’s Index, Manhattan
            distance and Euclidean distance. However, it was Dice’s Coeﬃcient the one that presented the best
            performance in the analysis of résumés.

            1 
            IRP(r, rx )
            j−1
            j

            AIRP(r ) =

            (4)

            x=1

            where r is a résumé selected for analysis from J, rx is another résumé related to J but different from r and
            j is the number of résumés sent to J.
            We introduce as well the Median Inter-Résumé Proximity (MIRP).
            It is a variation of AIRP, but it consists of calculating the median
            instead of the average of a set of Inter-Résumé Proximity values.
            The main reason to use this central-tendency measure is that it
            is more robust against skewness and outliers9 than the mean. The
            formula for calculating the MIRP is given by Eq. (5).

            MIRP(r ) = MEDIAN[IRP(r, rx )]x=1
            j

            (5)

            8
            A relevant résumé should have high values of IRP with respect to other relevant
            résumés and low values of IRP with respect to irrelevant ones. However, irrelevant
            résumés should have constantly low values of IRP in accordance with the analyses
            done in Cabrera-Diego et al. (2015).
            9
            An outlier is a value with an atypical magnitude with respect to the total set
            (Mason, Gunst, and Hess, 2003, page 70).

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            where r and rx are two different résumés from J and j is the number of résumés sent to J.
            3.3. Relevance Feedback
            In addition to AIRP and MIRP, we propose to use Relevance
            Feedback as a method for validating and enriching the information
            used by our ranking methods.
            In our study, Relevance Feedback is the process where an HRM
            determines which résumés, from a sample of the ranking given
            by AIRP or MIRP, are relevant and irrelevant for the job posting.
            Furthermore, an HRM can indicate the terms that better characterize the relevant and irrelevant résumés
            found during the previous
            step. Based on these inputs, we process and apply the feedback
            to offer an improved ranking of the remaining résumés. The Relevance Feedback given for one job posting does
            not affect the way
            we rank other job postings, as the inputs can differ.
            We propose two methods for applying Relevance Feedback.
            The ﬁrst method, called Relevance Factor and presented in
            Section 3.3.1, consists of calculating a quotient that takes into account the Inter-Résumé Proximity between
            a résumé and those
            considered relevant or irrelevant during Relevance Feedback. This
            method, as seen in Fig. 1, is introduced into the ranking process
            by a simple multiplication during the calculation of either AIRP or
            MIRP. The second method (Section 3.3.2) resides in weighting the
            terms indicated by the HRM that better represent the relevant and
            irrelevant résumés seen during Relevance Feedback. Because of its
            characteristics, explained in its respective section, this last method
            modiﬁes the Relevance Factor.
            3.3.1. Relevance factor
            The ﬁrst method for introducing Relevance Feedback consists of
            determining the proximity between the remaining résumés from a
            job posting and those, from the same job posting, that were analyzed during the Relevance Feedback. We
            achieve this with a formula that we have called Relevance Factor (RFa). The Relevance Factor goal is to
            improve the ranking of résumés. Thus, on one hand,
            the Relevance Factor pushes to the ranking’s top the résumés that
            are more proximal to those considered as relevant during the Relevance Feedback. On the other hand, it pulls
            down, to the ranking’s
            bottom, those résumés which are more proximal to the irrelevant
            ones.
            Let us consider F = {r1 , r2 , . . . , r f } as the set of résumés sent by
            applicants for a job posting J that were analyzed during a Relevance Feedback process. Each résumé from F
            was classiﬁed by an
            HRM into one class, either relevant (R) or irrelevant (I). We have
            deﬁned the Relevance Factor, RFa, in Eq. (6).

            RFa(r ) =

            +

            

            IRP(r, rxR )
             + |I|
            
            ·
            ;
             + |R|
             + IRP(r, rxI )

            ∀rxR ∈ R; rxI ∈ I; rxR , rxI ∈ F

            (6)

            where r is the résumé to be analyzed, R and I represent the set
            of résumés considered, respectively, as relevant and irrelevant during the Relevance Feedback process.
            Furthermore,  is a constant,
            empirically set to 1 × 10−10 which is used to avoid undetermined
            values10 and IRP is the function described in Eq. (1).
            The behavior of the Relevance Factor depends on the interval
            of the proximity measure used to determine IRP (Eq. (1)). Since
            we use Dice’s Coeﬃcient, the Relevance Factor will be greater than
            one (RFa(r) > 1) when the résumé r is more proximal to the relevant résumés. It is going to be RFa(r ) = 1
            if r is equally similar

            Table 1
            Example of how the Relevance Factor would be calculated for three résumés, A,
            B and C, that belong to a hypothetical job posting J containing eight different
            résumés, J = {R1 , R2 , R3 , I1 , I2 , A, B, C }. The example considers that J has three relevant résumés
            (R1 , R2 , R3 ) and two irrelevant ones (I1 , I2 ) previously detected by
            an HRM during a Relevance Feedback process.
            r

            rxR IRP(r, rxR )  IRP(r, rxR ) rxI IRP(r, rxI )  IRP(r, rxI ) RFa(r)

            A R1
            R2
            R3
            B R1
            R2
            R3
            C R1
            R2
            R3

            0.90
            0.75
            0.80
            0.35
            0.55
            0.45
            0.30
            0.40
            0.20

            In some cases during the Relevance Feedback, it is possible to ﬁnd only relevant
            or irrelevant résumés, but not both. Without this constant one side of the formula
            would be 0/0.

            2.45

            I1
            I2

            0.20
            0.30

            0.50

            2.45
            · 0.250 = 3.26
            3

            1.35

            I1
            I2

            0.40
            0.50

            0.90

            1.35
            · 0.290 = 1.00
            3

            0.90

            I1
            I2

            0.80
            0.75

            1.55

            0.90
            · 1.255 = 0.38
            3

            to relevant and irrelevant résumés. And, if the résumé r has more
            in common with the irrelevant résumés, the Relevance Factor will
            approach to zero.
            The introduction of the Relevance Factor into the ranking of résumés is done by simple multiplication, i.e.,
            the Relevance Factor of
            a résumé is multiplied by its respective score determined by either
            AIRP or MIRP.
            To understand the Relevance Factor better, it should be indicated that Eq. (6), can be split into two parts.
            The left side calculates IRP with respect to the relevant résumés, while the right side
            is in accordance with the irrelevant résumés. We describe in the
            following paragraph a hypothetical process of its calculation.
            Let us consider a job posting J composed of eight different résumés, J = {R1 , R2 , R3 , I1 , I2 , A, B, C
            }. During a Relevance Feedback process, an HRM analyzed ﬁve of these résumés, i.e., F =
            {R1 , R2 , R3 , I1 , I2 }, and found out that three were relevant (R1 , R2 ,
            R3 ), while two were irrelevant (I1 , I2 ). In Table 1, we present how
            the Relevance Factor would be calculated for the résumés that
            were not analyzed by the HRM (A, B, C). As it can be observed
            in Table 1, the résumé A is very similar to relevant résumés, therefore, its RFa(A ) = 3.26; this means that
            its score, either AIRP or
            MIRP, will be multiplied by a factor of 3.26. Regarding résumé B,
            it has a RFa(B ) = 1.00, this means that it is equally similar to relevant and irrelevant résumés; the AIRP
            or MIRP of B will rest the
            same. Concerning résumé C, it has a RFa(C ) = 0.38 due to its high
            similarity to irrelevant résumés and, in consequence, its AIRP or
            MIRP will be affected by a factor of 0.38.
            3.3.2. Vocabulary Scoring
            The second method for applying Relevance Feedback consists of
            processing the vocabulary that, in accordance with the HRM, better represents the résumés marked as relevant
            or irrelevant during
            the Relevance Feedback. The objective is to adjust the weights of
            the terms that cause a candidate to be considered by an HRM as
            relevant or irrelevant for the job posting. To achieve this, during
            the Relevance Feedback an HRM indicates and sorts which terms,
            seen in the analyzed résumés, characterized what made a candidate to be relevant or irrelevant. The sorting
            of the terms should
            be done regarding their representativeness.
            Formally, consider Vc = {t1 , t2 , . . . , tv } as the vocabulary selected
            and sorted by an HRM that better represents the résumés from
            class c during Relevance Feedback. For each term from Vc , we compute its Term Score Tc (t), i.e., a value
            that allows us to boost or
            minimize the terms that deﬁne each class c. In Eq. (7), we deﬁne
            the Term Score Tc (t) for a term t appearing in Vc .

            

            Tc (t ) = 5
            10

            95

            1
            ; ∀t ∈ Vc
            rank(t )

            (7)

            where rank(t) is the position of term t deﬁned by an HRM in Vc .
            The Term Score always has a value within the half-closed interval

            96

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            1.05
            1.00
            0.95
            0.90

            Term Score (t)

            0.85
            0.80
            0.75
            0.70
            0.65
            0.60
            0.55
            0.50
            0.45
            0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50

            Rank(t)

            Fig. 2. Plot of the term score for ranks of t between 1 and 50.

            [1, 0). A term with a value close to 1 expresses a high representativeness of class c, while a term with a
            value near to zero means
            that it hardly represents class c and should be minimized.
            Using a root in Eq. (7), speciﬁcally the 5th root, should be discussed. We empirically chose this function
            for two reasons. First,
            it allows us to create a score between 1 and 0. Second, it slowly
            decreases and preserves the sense of representativeness provided
            by the HRM, i.e., the way that terms were sorted by the HRM is
            kept. It can be seen in Fig. 2 how the Term Score changes in accordance to the rank of t; for instance, the
            term that is ranked ﬁrst
            has a Term score equal to 1; the second ranked term has a score
            Tc = 0.870; and for the ﬁftieth score, Tc = 0.457.
            As there is always a set of terms that will not appear in V but
            that are found in other résumés for the same job posting J, it is
            essential to give to these terms a Term Score Tc in order to keep
            the model balanced. In other words, we cannot leave the terms
            that did not appear in a Vc with higher values than those that were
            analyzed by an HRM. We assign a value of 0.01 to all the terms
            belonging to the résumés of J that are not present in a Vc .11 This
            ﬁgure was chosen empirically to minimize the terms that are not
            representative of the model without deleting them.12
            Since the Term Score Tc (t) of every term t is different for each
            class c (relevant and irrelevant), we use the Term Score uniquely
            within the Relevance Factor (Section 3.3.1), as it calculates the
            Inter-Résumé Proximity with respect to relevant and irrelevant résumés separately. To be more speciﬁc, the
            Term Scores only modify terms’ weights of each class used at the computation of InterRésumé Proximity in Eq.
            (6).
            3.3.3. Selecting the résumé s for the Relevance Feedback
            Even though the Relevance Feedback described in
            Rocchio (1971) consists of choosing a number of top-retrieved
            documents, we test whether the Relevance Feedback determined
            with other position-retrieved documents is useful to HRMs. More
            speciﬁcally, we use the Relevance Feedback of the documents
            retrieved from the following positions:
            •

            Top. This is the classic method which consists of taking the top
            ranked résumés to improve the following rankings. In the case

            11
            For instance, a term t can appear in VIrrelevant but not in VRelevant . Thus, for this
            same t the Term score VRelevant will be 0.01.
            12
            We experimented with other values: 0.25, 0.1 and 0.05. We observed that by
            decreasing the value the results were improved.

            where we ﬁnd non-relevant résumés among the top, it may be
            a way to determine which characteristics, although common,
            may not be required for the job or are not the ones searched
            by the HRM.
            • Bottom. This is the opposite of the classic method, as we select the résumés located at the end of
            rankings. We infer that
            ﬁnding a relevant résumé with a low ranking can provide more
            useful feedback than detecting an irrelevant résumé at the top.
            Furthermore, this may be interesting for the Human Resources
            domain, as leaving a relevant résumé at the bottom would set
            aside the objectives of the rankings.
            • Top and Bottom (henceforth Both). For this position, we decided to merge the ideas from the ﬁrst two
            Relevance Feedback
            positions. More speciﬁcally, in this position we ask a recruiter
            whether some résumés from the top and the bottom are truly
            relevant.13 The goal is to reduce the weaknesses of the Bottom
            and Top positions; we may detect truly relevant and irrelevant
            documents ranked ﬁrst and also those that are interesting but
            mis-positioned at the end of a ranking.
            In addition to the different Relevance Feedback positions, we
            decided to test whether an iterative application of Relevance Feedback could improve the résumé rankings
            more quickly. Under noniterative conditions, once the Relevance Feedback has produced a
            new ranking the process ends. Nonetheless, for iterative conditions,
            once a new ranking is produced, it can be re-analyzed by an HRM
            in a new Relevance Feedback process.

            4. Data
            For this article, we used a set of 171 job postings which
            were processed (recruitment and selection) by a French human
            resources enterprise between November 2008 and March 2014.
            These job postings come from different professional domains (e.g.,
            chemistry, communications, physics and biotechnology) and position levels (e.g., laboratory researcher,
            intern, project manager and
            engineer). These 171 job postings were chosen because they contain at least 20 unique résumés in French; at
            least 5 of them are

            13
            Half of the résumés for the Relevance Feedback are from the top. The other half
            belong to the ranking’s bottom.

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            relevant, and 5 are irrelevant.14 In total, the corpus contains 14,144
            French résumés divided among these 171 job postings.
            All the job postings are composed of applications, and each application contains the documents associated
            with the recruitment
            and selection process. It is important to note that not all the documents located inside the applications
            corresponded to résumés;
            we could ﬁnd motivation and recommendation letters, diplomas,
            interview minutes and social network invitations as well. To obtain only the French résumés, we made use of
            a résumé detector. The résumé detector is a linear Support Vector Machine (SVM)
            developed previously in Cabrera-Diego et al. (2015). Furthermore,
            all the résumés were lower cased and lemmatized; for lemmatizing the documents, we used Freeling 3 (Padró &
            Stanilovsky, 2012).
            Stop-words, punctuation marks and numbers were deleted. In addition, all duplicated résumés within the same
            job posting were
            deleted.15 See Cabrera-Diego et al. (2015) in order to learn more
            about this pre-processing task.
            According to the HRMs with whom we worked, the system employed to manage the applications allowed them to
            organize each
            applicant into one of the following selection phases: Unread, Analyzed, Contacted, Interviewed and Hired.
            The phases were assigned
            to each applicant depending on the last point to which they arrived. For this article, we grouped four of
            these phases into two
            different classes: relevant and irrelevant.
            The ﬁrst class, relevant, corresponds to the phases Contacted,
            Interviewed and Hired. It represents the applicants who after reading their résumés were approached by a
            recruiter. The second
            class, irrelevant, contains only the résumés that remained in the
            Analyzed phase, i.e., the applicants that were not approached by
            an HRM after reading their résumés.
            With respect to the applications that remained in the Unread
            phase, these were discarded from the analysis since we cannot infer whether they were relevant or irrelevant
            for the job. Furthermore, most of these applications were not read because the selection process ended as
            they were received.
            There are two reasons to classify four of ﬁve phases into two
            classes. The ﬁrst is that to determine whether an applicant will
            be hired implies the analysis of elements that are not present in
            a résumé (e.g., interview results, expected salary, job location and
            withdrawal). The second one is that we do not want to replace humans with an automaton in the selection
            process. Instead, we want
            to assist humans during the most diﬃcult part of the selection process, which is in discerning relevant and
            irrelevant applicants. And
            this can be achieved by ordering applicant’s résumés in terms of
            how relevant are for the job posting.
            It is important to note that some applications from the corpus, although impossible to trace, started as
            Direct contact. This
            means that an HRM found, usually on the Internet or job seekers databases, the résumé of a person who
            fulﬁlled the person
            speciﬁcation and decided to contact this person directly. Thus, for
            some job postings the relevant résumés can accurately reﬂect the
            searched proﬁle. This action can affect the number of relevant applicants for a job posting, which in some
            cases can be equal or
            greater than the number of irrelevant applicants. However, this
            characteristic from the corpus should be seen as normal, since for
            an HRM to make direct contact is a way to speed up the recruitment and selection processes.

            14
            All the résumés must be either relevant or irrelevant, but each job cannot have
            less than 5 per class.
            15
            There were job postings in which the same applicant sent their own résumé
            multiple times. Thus, to avoid a bias, we deleted the duplicated résumés with a set
            of heuristics developed in Cabrera-Diego et al. (2015). Among the heuristics used,
            we can highlight the selection of the most recent résumé in the application folder
            or the detection of the exact same applicant e-mail.

            97

            Furthermore, it should be indicated that we do not combine applications from different job postings, even if
            they belong to similar job positions. The reason is that each job posting is linked
            to a job offered by a speciﬁc enterprise, in a particular date and
            with a set of desired characteristics. In other words, each job posting might attract different job seekers
            despite describing a very
            similar job position; aspects like years of experience, spoken languages, mobility, relocation and salary
            can affect how the job market reacts. This variability makes impossible to determine whether
            a candidate from one job posting would participate in another one
            or whether a candidate would be considered equally relevant.
            To conclude with this section, after a manual search, we arrived
            to link 60 of the 171 job postings with their respective job offer.
            With these 60 job postings we created a baseline that will be described in Section 5.
            4.1. Data representation
            We decided to represent each résumé from the corpus as a set
            of n-grams in a Vector Space Model (VSM) (Salton, Wong, & Yang,
            1975). To be speciﬁc, for each résumé we extracted its set of unigrams, bigrams and trigrams. Every set of
            n-grams was saved as
            a vector, one per résumé. The vectors’ component weights (W) are
            the relative frequency of each n-gram which could be multiplied
            by a weight modiﬁer (); we present W in Eq. (8).

            W (• ) = F(• ) · (• )

            (8)

            where
            is an n-gram and F is the relative frequency calculated
            with respect to each résumé. The weight modiﬁer  can be one of
            the following:
            •

            •

            •

             = 1. In this case, we represent the data only by the relative
            frequency of each n-gram.
             = IDF(• ). Each n-gram (•) is weighted with respect to
            a Term-Frequency Inverse-Document Frequency (TF-IDF) SpärckJones (1972).16

            Once the résumés of a job posting have been ranked for the
            ﬁrst time, either with AIRP or MIRP, and a vocabulary scoring has
            been set, a new  for Eq. (6) can be used:
            •

             = Tc (• ). In this case each n-gram (•) is modiﬁed by its re-

            spective Term Score Tc (see Eq. (7)); where c is the class (relevant or irrelevant) that will affect
            uniquely.
            •  = IDF (• ) · Tc (• ). It is similar to the previous , however, it
            can be modiﬁed by IDF in the case, the original representation
            made use of the weight too.
            In all the cases, these last two  do not affect permanently the
            weights of the terms, they are only locally used each time Eq. (6) is
            called.
            4.2. Data for the Relevance Feedback
            Although the ideal experimentation would consist in applying
            our methods and asking HRM for Relevance Feedback on real time,
            the fact is that this task would be very expensive. Moreover, the
            HRM would have to do this task besides their normal work duties
            and it would be hard to get accurate results in cases where the
            person speciﬁcation evolved over time. Thus, we decided to simulate the Relevance Feedback.
            Regarding the Relevance Feedback in which an HRM indicates
            whether a résumé is relevant or irrelevant, we made use of the
            information available in the corpus. As we explained at the beginning of Section 4, every application and,
            therefore, every résumé
            16
            The IDF for each unigram, bigram and trigram was calculated using all the corpus described at the beginning
            of Section 4 (14,144 résumés).

            98

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            belongs to a real selection process. Thus, at a given moment, every
            résumé was analyzed by an HRM who considered whether it was
            from a relevant or irrelevant applicant. The information found in
            the corpus allows us to create a simulation that can be reproduced
            again if necessary.
            For the vocabulary scoring, we decided to explore three simulations, S1 , S2 and S3 , in which we select and
            weight differently
            the terms for the vocabulary scoring. Each simulation is composed
            of 100 n-grams in total, 50 describing the relevant résumés and
            50 the irrelevant ones. These simulations are different from the
            one used to determine whether a résumé is relevant or irrelevant,
            as the corpus does not contain this kind of information. However,
            they are based on information found in the corpus and in consequence reproducible. In the following
            subsection, we explain in
            detail how S1 , S2 and S3 were determined.
            4.2.1. Simulations for Vocabulary Scoring
            Consider V = {t1 , t2 , . . . , tv }, the vocabulary composed of the ngrams (t) that occur in at least 2
            résumés from the Relevance Feedback.17 The process to generate the three simulations is as follows:
            1. For each term t belonging to V, we calculate the squared probability of term t occurring in each possible
            class c, either relevant
            or irrelevant. This is done using Eq. (9):

            

            p2c (t ) =

            Dc (t )
            D(t )

            2

            (9)

            where Dc (t) is the number of résumés belonging to class c; D(t)
            is the number of résumés analyzed in the Relevance Feedback.
            The equation is an adaptation of Gini’s Coeﬃcient18 presented
            in Cossu (2015). For a set of classes C = {c1 , c2 , . . . , ck }, Gini’s
            Coeﬃcient has an interval between [1/k, 1], where 1/k means
            that a term appears in every class, while 1 indicates that the
            term belongs to one class (Torres-Moreno et al., 2012).
            2. Then, we calculate a factor (fc ) that takes into account the number of documents from class c where the
            n-gram appeared and
            the sum of the n-gram’s weights (W) inside these documents.
            The factor is presented in Eq. (10).

            fc (t ) = Dc (t ) ·

            

            Wc (t )

            (10)

            where t represents an n-gram, c is one of the two possible
            classes (relevant or irrelevant), fc is the factor for the class c,
            Dc is the number of documents of class c where t appears and
            W is the n-gram weight (see Eq. (8)).
            3. For each class c we sort the n-grams ﬁrst according to their
            squared probabilities p2c and then by their factor fc . If two or
            more n-grams share the same squared probabilities and factors,
            although this is unusual, we assign them different but consecutive locations in the sorted list.
            4. We select the ﬁrst 50 n-grams for each class, to which we calculate their Term Scores (Tc ) using Eq.
            (7).
            5. For the rest of n-grams, or those that did not occur in the
            résumés from the Relevance Feedback, we give them a Term
            Score of 0.01 as explained in Section 3.3.2.
            Simulation S1 consists of selecting and scoring the vocabulary
            according to the information found only in the résumés used for
            17

            Because we simulate the vocabulary scoring, to use terms that were seen only
            in one résumé may not be reliable but speculative. In fact, a one time-seen term,
            and in consequence its pertinence, may be no more than a coincidence which could
            change by increasing the number of documents analyzed.
            18
            Although Gini’s Coeﬃcient is frequently used in economics for wealth distribution, it has been used in other
            NLP works, e.g., Fang and Zhan (2015) and
            Cossu, Janod, Ferreira, Gaillard, and El-Bèze (2014). Gini’s Coeﬃcient in NLP has the
            objective of modifying the weight of an element in the data model by determining
            to which degree it represents a certain class or set of them (Torres-Moreno, El-Bèze,
            Bellot, & Béchet, 2012).

            Relevance Feedback. In other words, the résumés from the Relevance Feedback are used to calculate the
            squared probabilities and
            the factors of the n-grams. Next, for each class c, we calculate the
            Term Scores for the ﬁrst sorted 50 n-grams.
            For simulation S2 , we decided to recreate a scenario where the
            selection and sorting of the terms is done carelessly. Put differently, the terms that, in theory, represent
            relevant and irrelevant
            résumés are ignored and are not used in the Relevance Factor
            (Eq. (6)). To this end, we sort the n-grams using only the information from the Relevance Feedback, as we do
            for S1 , but the Term
            Score of the ﬁrst 50 n-gram of each class c is set to zero (Tc = 0).19
            For the rest of terms, the Term Score is the default one, i.e. 0.01.
            In simulation S3 , we try to model optimally the n-grams that
            would be chosen by an HRM in real life. To this end, we calculate
            the squared probabilities and factors fc based on the information
            in all the résumés from the job posting. However, we continue to
            sort and calculate the Term Scores for the terms that only occur in
            the résumés from Relevance Feedback. In summary, we can have
            high reliable squared probabilities and factors fc but we only affect
            the n-grams that would have been seen by an HRM during the Relevance Feedback.20

            5. Experimental and evaluative settings
            There are multiple experiments that can be done following
            different conﬁgurations, however, although we explored a large
            amount of possible combinations, due to space limitations we only
            present the experiments that could contribute the most to the
            state-of-the-art. The experiments realized are summarized in the
            following list:
            No Relevance Feedback: We apply our methods without using
            any kind of relevance feedback, and we compare them against
            a couple of baselines.
            • Relevance Feedback applied using
            – The Relevance Factor: We explore how different Relevance
            Feedback position (Top, Bottom and Both) affect the Relevance Factor. As well, we analyze whether the
            iterative
            application of the Relevance Factor can improve faster the
            ranking of résumés.
            – The Relevance Factor with Vocabulary Scoring: We analyze
            how the simulations of Vocabulary Scoring affect the rankings created by the Relevance Factor.
            •

            Two different baselines are used, the ﬁrst one consists of a system that generates a random ranking for each
            job posting. The second baseline resides in using the 60 job postings to which we arrived to link with their
            respective job offer and calculate the similarity résumés/job offer. More speciﬁcally, for each job posting,
            we apply Dice’s Coeﬃcient between its job offer and every element from its set of résumés. Job offers are
            pre-processed under
            the same parameters that the résumés, as explained in Section 4.
            Although a comparison with other methods or systems from the
            state-of-the-art would have been desired, to the extent of our

            19
            By making zero the Term Score of these n-grams, we affect their weight in the
            vector space model as explained in Section 4.1. This modiﬁcation has, in consequence, an effect in the
            Relevance Factor (Eq. (6)), where the résumés containing
            most of the terms representing a class, instead of being pushed up or pulled down,
            they will stay in the same position in the rank.
            20
            In simulation S3 is possible that after sorting the n-grams, the one placed in the
            ﬁrst place does not appear in the Relevance Feedback. Thus, as this n-gram could
            not have been seen by the HRM during the Relevance Feedback, we must consider
            another n-gram as the one in the ﬁrst place. This will be the ﬁrst term seen in
            the Relevance Feedback that has the best squared probability and factor fc . For the
            following terms the rules are the same.

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            knowledge, none of the systems or datasets have been released to
            the public.21
            In the case of the experiments with Relevance Feedback, we
            have restricted the feedback size to a range between 2 and 20 résumés. It should be noted that we never use
            more than 50% of the
            résumés for each job as feedback. In fact, the 171 job postings described in Section 4 were chosen because
            they had at least 20 résumés, from which at least 5 were from relevant applicants and 5
            from irrelevant ones. When we use more than 10 résumés for the
            Relevance Feedback, we always verify that there is at least twice
            the résumés for the job posting, with more than 1/4 of them being
            relevant and no less than 1/4 irrelevant. For example, to do a Relevance Feedback of 16 résumés, a job
            posting must have at least
            32 résumés in total, and no less than 8 must be relevant or irrelevant. If one job posting do not have these
            characteristics then it is
            discarded, for that size of Relevant Feedback, from the analysis. All
            these precautions are taken to avoid inﬂating the measurements
            for evaluation artiﬁcially.
            In the experiments related to the iterative application of Relevance Factor, we explore how rankings are
            affected when multiple and sequential Relevance Factor processes are done. In other
            words, we start by doing a Relevance Factor over 2 résumés. This
            process will rank the remaining résumés of the job posting in an
            improved way. After that, a new process of Relevance Feedback is
            done on which 2 new résumés are analyzed. The Relevance Factor is calculated again and the process is
            repeated until having revealed up to 20 résumés.
            It should be mentioned that the corpus had 171 job postings
            that fulﬁlled the characteristics used for the Relevance Feedback
            up to size 10. For a Relevance Feedback of size 20, there were only
            127 job postings with the established characteristics.
            All the calculations for AIRP and MIRP were parallelized using
            GNU Parallel (Tange, 2011), a shell tool created to run the same
            task multiple times but with different inputs. More speciﬁcally, the
            parallelization consists in assigning a CPU thread to each job posting. Therefore, multiple job postings can
            be run at the same time.
            We decided to evaluate each ranking of résumés using Average
            Precision (AP) (Buckley & Voorhees, 20 0 0). AP is an evaluation metric designed for rankings with two
            grades of relevance: relevant
            and irrelevant.22 Furthermore, AP determines, at the same time,
            the precision and the recall of a ranking in accordance to the position of its elements (Voorhees & Harman,
            2001). In order to have
            a good value of AP, i.e., close to 1, the relevant elements should
            be positioned at the top of a ranking, while those that are irrelevant should be located at the bottom of a
            ranking. In our case, a
            ranked résumé is considered to have the correct relevance when it
            is similarly marked in the corpus data (see Section 4).
            To evaluate the performance of the methods used to rank résumés, we calculate the Mean Average Precision
            (MAP) for each
            one (Buckley & Voorhees, 20 0 0). As the name indicates, the MAP
            consists of averaging all the AP values obtained using the same
            method.
            In order to verify whether the MAP values obtained for each
            tested method are signiﬁcantly different, we analyze the results using a one-way Repeated Measures Analysis
            of Variance (rANOVA).
            The assumptions of rANOVA, data normality and sphericity, are
            tested with the Shapiro-Wilk Test and the Mauchly’s Test, respec21
            The only exception could be LO-MATCH, which provided a service through a
            website during a time. However, the software, per se, was never available to download for testing purposes.
            22
            Apart from the AP, we can ﬁnd in the literature two other metrics specialized in
            the evaluation of rankings: Kendall’s tau and (Normalized) Discounted Cumulative
            Gain (Järvelin & Kekäläinen, 20 0 0). These metrics are used in rankings with multiple grades of relevance,
            e.g., very relevant, relevant, irrelevant and very irrelevant.
            However, our data set is only annotated with two grades of relevance, thus, AP is
            the most appropriate metric.

            99

            Table 2
            Summary of the statistical analysis done over the results presented in
            Fig. 3. The upper diagonal shows the p value of the results that were
            signiﬁcantly different. The lower diagonal shows the values of Cohen’s
            d effect size.
            AIRP
            AIRP
            AIRP IDF
            MIRP
            MIRP IDF
            Random

            0.230
            0.316

            AIRP IDF

            MIRP

            MIRP IDF

            Random

            0.017

            -

            -

            4.4 × 10−4
            1.2 × 10−3
            5.4 × 10−4
            1.5 × 10−4

            0.344

            0.309

            0.339

            tively. In both cases, the alpha to refute the null hypothesis is set
            to 0.05.
            The results from the rANOVA are considered to be signiﬁcantly
            different when the p value is less than 0.05. In the case we compare more than two methods, and the rANOVA
            show a signiﬁcant
            difference, we also make use of a post hoc test. More speciﬁcally,
            we utilize a Pairwise t-Test with α = 0.05 in order to determine
            which pairs of groups are signiﬁcantly different.
            For each pair of experiments showing a signiﬁcant difference,
            we calculated the effect size using Cohen’s d. Effect sizes are values
            that helps to quantify the difference between two analyzed groups.
            As thumb rule, effect size can be classiﬁed into small (d = 0.2),
            medium (d = 0.5) and large (d = 0.8) (Cohen, 1988, Page 20).
            The statistical analyses were performed using R (R Core
            Team, 2018).
            6. Results
            In this section, we present the results regarding the experiments deﬁned in Section 5. Every result
            presented in a graph includes its respective 95% conﬁdence interval.
            6.1. Experiments with No Relevance Feedback
            In Fig. 3 we present the results of AIRP and MIRP with and
            without the Inverse-Document Frequency (IDF). We also compare
            the results with respect to the random baseline.
            As it can be seen in Fig. 3, all the methods presented in this
            work surpass the value given by the random baseline. Nonetheless,
            AIRP and MIRP get similar MAP values.
            The corresponding rANOVA between the results presented in
            Fig. 3 indicates that there is a signiﬁcant difference between the
            results ( p value = 2.153 × 10−5 ). According to the post hoc test all
            the methods are signiﬁcantly different with respect to the random baseline (p value
            <
            0.001). Moreover, AIRP with IDF is significantly different to AIRP ( p value = 0.017). For the remaining
            pairs
            of methods, there is no statistical difference. The average effect
            size between our methods with respect to the random baseline is
            d = 0.327, which is medium-small. The effect size between AIRP
            and AIRP with IDF is d = 0.230. In Table 2, we present a summary
            of the results from the statistical test.
            In Fig. 4, we compare again our methods with respect to a random baseline but also with the one based on the
            similarity between job offers and résumés. This experiment was done uniquely
            over the corpus’ subset composed of 60 job posting for which we
            had found their respective job offers (see Section 4).
            As shown in Fig. 4, our methods rank the résumés better than
            methods using the similarity between job offers and résumés.
            Moreover, our methods work better on these 60 job postings than
            with the complete set of 171. The reasons for these results will be
            discussed in Section 7.
            The rANOVA performed on the results shown in Fig. 4 indicates that there was a signiﬁcant difference between
            the meth-

            100

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            Fig. 3. Results, in terms of the MAP, for the random baseline, AIRP and MIRP without applying any kind of
            Relevance Feedback.

            Fig. 4. Comparison of our methods and two baselines (random and similarity between job offer and résumés)
            for 60 job postings. The values are presented in terms of the
            MAP, and we did not use any kind of Relevance Feedback.

            Table 3
            Summary of the statistical analysis done over the results presented in Fig. 4, which correspond to the
            subset of 60 job postings. The upper diagonal shows the p value of the results that were signiﬁcantly
            different. The lower diagonal shows the values of Cohen’s d effect size.
            AIRP
            AIRP
            AIRP IDF
            MIRP
            MIRP IDF
            Job Offer/Résumé
            Random

            0.357
            0.800
            0.656

            AIRP IDF

            MIRP

            MIRP IDF

            Job Offer/Résumé

            Random

            0.045

            -

            -

            7.8 × 10−7
            1.5 × 10−9
            1.2 × 10−6
            4.1 × 10−9

            3.6 × 10−5
            7.8 × 10−6
            4.7 × 10−5
            1.1 × 10−5
            0.025

            1.012
            0.716

            0.784
            0.701

            ods ( p value = 3.270 × 10−7 ). In fact, and in accordance with post
            hoc test, the method based on the similarity of job offer/résumé is
            signiﬁcantly different than the random baseline and all our methods (p value
            <
            0.05). The effect size between the methods AIRP IDF,
            MIRP and MIRP IDF, and the job offer/résumé baseline is always
            d > 0.780, which correspond to large effect sizes. In Table 3, we
            present a summary of the results from the statistical test.

            0.977
            0.642

            0.392

            6.2. Experiments with Relevance Feedback
            In this part, we present the experiments run with Relevance
            Feedback (Section 3.3) and applied using two different methods,
            Relevance Factor and Vocabulary Scoring. It should be noted that
            as there is no signiﬁcant difference between AIRP and MIRP, we
            excluded from the following experiments MIRP. We decided to use

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            101

            Fig. 5. Results of AIRP IDF after the introduction of the Relevance Feedback using the Relevance Factor. We
            present, as well, the performance depending on the different
            positions from where we could obtain the résumés for the Relevance Feedback.
            Table 4
            Summary of the statistical analyses, done over the results, at 10 and 20 résumés, regarding the
            non-iterative Relevance Factor. The upper diagonal shows the p value of the
            results that were signiﬁcantly different. The lower diagonal shows the values of Cohen’s
            d effect size.
            10 résumés
            Top
            Top
            Bottom
            Both

            2.1 × 10
            0.532
            0.293

            20 résumés

            Bottom

            Both
            −10

            Top
            −4

            1.7 × 10
            2.3 × 10−5

            0.345

            uniquely the AIRP IDF because it showed a statistical difference
            with AIRP, moreover, in real cases the IDF could be of help in reducing the n-grams that are frequent but
            useless for HRM.
            6.2.1. Relevance Factor
            We present the results regarding the Relevance Factor and how
            the Relevance Feedback positions (Top, Bottom and Both) affected
            its performance. Furthermore, we veriﬁed whether the iterative application of the Relevance Feedback could
            improve the speed of résumé ranking. In each iterative step 2 résumés were analyzed until
            reveal up to 20 résumés. The results of these experiments are presented in Fig. 5.
            In Fig. 5, we see that the Relevance Feedback depends on where
            the résumés are obtained: Top, Bottom or Both positions. The Top
            position needs a smaller number of résumés to generate higher
            values of MAP than the Bottom position does.
            The rANOVA done with 10 and 20 résumés indicated a significant difference between the positions in the
            non-iterative process, p value = 2.45 × 10−12 and p value = 6.35 × 10−11 respectively. More speciﬁcally, the
            pairwise post hoc test revealed that
            there was always a signiﬁcant difference with 10 and 20 résumés for all the Relevance Feedback positions (p
            value
            <
            0.005). In
            Table 4, we present a summary of the statistical analyses and the
            effect sizes obtained. It should be noted that the effect sizes are
            between medium-small and medium. Similar results for Relevance
            Feedback positions were obtained with the rANOVA and post hoc
            test for the iterative process.
            It can be seen, in Fig. 5, that the iterative application of the Relevance Feedback does not bring any
            improvement with respect to

            Bottom
            5.8 × 10

            0.574
            0.484

            Both
            −9

            5.0 × 10−7
            1.4 × 10−3

            0.290

            the non-iterative application. There are some minimal variations,
            positive or negative, but in most cases the values are the same.
            In fact, we determined through a rANOVA that there is no signiﬁcant difference between the iterative and
            non-iterative application
            of the Relevance Feedback (p value > 0.05) for 10 and 20 résumés.
            We can say that both kinds of applications give comparable results.
            Thus, in the following experiments we use only the non-iterative
            process.
            6.3. Relevance Factor with Vocabulary Scoring
            For the Relevance Factor with Vocabulary Scoring, we made use
            of AIRP IDF with a non-iterative Relevance Feedback application.
            Vocabulary Scoring was done following simulations S1 , S2 and S3 ,
            as explained in Section 4.2. In Fig. 6, we present the results from
            these experiments.
            We see from Fig. 6 that the results in terms of the MAP depend
            on the simulation utilized for Vocabulary Scoring. On one hand, it
            is evident that simulation S3 , where we used the maximum quantity of information available to calculate the
            Term Scores, completely boosts the Relevance Factor and allows us reaching a MAP
            of 0.937 ± 0.014. On the other hand, simulations S1 and S2 do not
            improve the Relevance Factor. It can be seen in Fig. 6 that S1 , despite being conceived to boost the
            n-grams that represented the
            classes, relevant and irrelevant, reduces the performance of the
            Relevance Factor in comparison to its application without Vocabulary Scoring. For instance, using 20 résumés
            in the Relevance Feedback process without any Vocabulary Scoring results in the MAP
            being equal to 0.800 ± 0.030, while using simulation S1 results in

            102

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            Fig. 6. Results of AIRP IDF using a Relevance Feedback that was applied with the Relevance Factor and
            Vocabulary Scoring. The Vocabulary Scoring was obtained through
            three different simulation S1 , S2 and S3 .

            Table 5
            Summary of the statistical analyses, done over the results, at 10 and 20 résumés,
            regarding the application of Relevance Factor with three simulations of Vocabulary
            Scoring (S1 , S2 , S3 ). The upper diagonal shows the p value of the results that were
            signiﬁcantly different. The lower diagonal shows the values of Cohen’s d effect size.


            In the following subsections, we discuss the results obtained in
            Section 6. The discussion is divided based on the experiments.

            7.2. Relevance Feedback positions and the Relevance Factor

            10 résumés
            S1
            S1
            S2
            S3

            0.458
            0.978

            20 résumés

            S2

            S3

            1.2 × 10−8

            <
            2 × 10−16
            <
            2 × 10−16

            0.858

            S1
            0.749
            1.163

            S2

            S3

            6.2 × 10−14

            <
            2 × 10−16
            <
            2 × 10−16

            1.013

            a MAP value of 0.766 ± 0.031. In contrast, in S2 , where we do not
            consider the representative n-grams of each class, the MAP stayed
            stable as if the Vocabulary Scoring would have not been used. This
            outcome, will be discussed in Section 7.
            The rANOVA performed on the results showed there was a signiﬁcant difference between the simulations using
            10 and 20 résumés, in both cases p value = 2.2 × 10−16 . According to the pairwise post hoc test, at 10 and
            20 résumés, all the simulations were
            signiﬁcantly different. In Table 5, we present the results regarding
            p value and effect size.
            Regarding the effect size, at 10 résumés, between simulation S1
            and S2 Cohen’s d = 0.458, which is large-small; between S3 and,
            S1 and S2 , Cohen’s d was greater than 0.850, which it is a large effect size. Using 20 résumés, the effect
            size between S1 and S2 was
            large-medium effect size d = 0.749, for the rest of pairs, Cohen’s d
            was greater than 1, which correspond to a large effect size.

            7.1. AIRP, MIRP, IDF and baselines
            The signiﬁcant difference between our methods and the random baseline method means that our methods can be,
            by themselves, of help to HRMs. In other words, the Inter-Résumé Proximity, used through AIRP and MIRP, can
            rank correctly, to a certain
            degree, the résumés and proposes a better start point, than a random one, to HRMs during the selection
            process. As we observed
            in Section 6.1, there was no signiﬁcant difference between AIRP

            As we observed in Section 6.2.1, the Relevance Factor is affected
            by the place from where the résumés used for the Relevance Feedback were obtained. In fact, the most helpful
            position was the Top
            one while the Bottom position was the one that gave the lowest
            performance. The latter result indicates that at the end of the rankings we did not ﬁnd relevant résumés. In
            other words, we do not
            ﬁnd résumés that could help us determine what is sought by the
            HRM. As a consequence, it is diﬃcult to improve the results using
            only irrelevant résumés. Moreover, in order to see an improvement

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            with the Bottom position, it is necessary to increase the number of
            analyzed résumés. This means reaching the middle of the rankings,
            from the bottom, to increase the probability of ﬁnding relevant résumés.
            Despite the Both position results were less performing than
            those obtained with the Top position, it could be of interest to follow it in real life. The main reason is
            that it may verify that we did
            not leave someone relevant at the end of the résumé ranking. The
            second reason is that its behavior is not far from the behavior obtained with the Top position; although
            according to the statistical
            test, there is a signiﬁcant difference and the effect size is between
            small and medium-small.
            It is of interest to determine whether an asymmetric Both position is better than a symmetric one.
            Currently, the same number
            of résumés is analyzed from the top and the bottom of the résumé
            rankings. However, it may be better to analyze more résumés from
            the top of the rankings than from the bottom to improve the speed
            of our methods.
            It can be asked why the MAP decreases when using two résumés for Relevance Feedback for the Bottom and Both
            positions.
            The reason is that we increase the probability of ﬁnding only irrelevant résumés by looking for résumés at
            these positions. When
            we use only irrelevant résumés for the Relevance Factor (Eq. (6)),
            we can penalize relevant résumés based on their small similarities
            with the irrelevant ones. As mentioned previously, by increasing
            the number of analyzed résumés, we can increase the number of
            relevant résumés analyzed and reduce the effect of the irrelevant
            ones located at the end of the rankings.
            We did not found any signiﬁcant difference with respect to
            the iterative and non-iterative application of the Relevance Feedback. Moreover, we do not have a precise
            idea of why the iterative application did not improve the speed of résumé ranking. The
            best idea that we have is that the improvement is so small that
            the MAP cannot detect it. Put differently, the résumés just change
            ranking positions with other résumés of the same type (relevant
            or irrelevant) and this cannot be detected by the MAP. It is possible that the number of résumés used in
            each iteration, two, is
            not enough to provide visible improvement. We may need to determine with other experiments how many résumés
            are necessary
            in an iterative application of the Relevance Feedback to see real
            improvement.
            To improve the performance of the iterative application of the
            Relevance Feedback, we may need as well to take into account the
            history of how the résumés move within the rankings. If we ﬁnd
            that résumé rankings do not change greatly, it could mean that we
            arrived at a point where we cannot further improve the rankings
            with this method. Thus, we should change the method, for example, by using Vocabulary Scoring or looking for
            relevant résumés at
            the bottom, or even at a random position.
            7.3. Vocabulary Scoring
            The results obtained using Vocabulary Scoring and the Relevance Factor were surprising. We never expected to
            surpass a MAP
            of 0.9, as we did with S3 (MAP of 0.9372 ± 0.014). Furthermore, we
            were surprised by the results because Vocabulary Scoring only affects the model used in determining the
            Relevance Factor. Thus,
            the AIRP of one résumé r is modiﬁed only by the Relevance Factor
            (Eq. (6)) which determines how proximal résumé r is to the relevant and irrelevant ones using basically 100
            n-grams chosen by the
            HRM (50 terms per class).
            The poor performance of S1 and S2 , seen in Section 6.3, may
            be related to the quantity of data utilized to establish the Term
            Scores. Using only the information provided by documents from
            the Relevance Feedback is not enough to simulate correctly the
            knowledge that an HRM would have about the job posting and, in

            103

            consequence, to determine the Term Scores. It should be remembered that the simulations are based on the
            squared probabilities
            (Eq. (9)) and without enough information these values lack the reliability to correctly represent the
            classes. Although, we tried to
            increase the reliability by using only n-grams observed in at least
            two résumés, as explained in Section 4.2.1, this minimum might
            not be enough for these two simulations. The problem is solved
            when we make use of S3 , where we calculate the squared probabilities based on all the information
            available.
            To better understand how the simulations worked and affected
            the results, we present in the following lines a discussion of the
            simulations generated regarding a Project Manager job posting; this
            job posting is one of the 60 job postings linked manually to the job
            offer. In Fig. 7, we present an abstract of the job offer related to the
            job posting. In Table 6, we present an extract of Vocabulary Scoring using the three simulations, S1 , S2
            and S3 , for 20 résumés of
            Relevance Feedback.23 It should be remembered, that for obtaining
            the n-grams and the values presented in Table 6, we did not make
            use of the job offer at any moment, they are result from simulation
            S1 , S2 and S3 as explained in Section 4.2.1.
            We see from Table 6 that simulation S3 provides the best
            weights to the terms related to the job offer, even when the last
            one was not included in the analysis process. Nevertheless, S1 and
            S2 have trouble correctly weighting the terms of the job offer or at
            least placing them within the ﬁrst ﬁve positions; the reason is the
            lack of information.
            Additionally, although impossible to show due to their length,
            it should be mentioned that for simulation S3 , the n-grams of both
            classes always had a squared probability, p2c (t ), of 1. For simulations S1 and S2 the squared
            probabilities were always 1 regarding
            the relevant class, while they varied from 1 to 0.444 for the irrelevant class.
            In general, thanks to outputs like those presented in Table 6,
            it is possible to better understand which characteristics were the
            ones looked for or impacted the decision of HRM. With this kind
            of lists, psychologist can do a posteriori studies regarding the selection of candidates. Or, other HRMs can
            use this kind of output to
            explain to candidates why they were not selected for an interview.
            One interesting thing to note, as seen in Fig. 6, is that S2 is better than S1 despite the former did not
            contain the terms that were
            boosted in the latter. The reason for this discrepancy is related to
            the quality of the n-grams chosen for the simulations and how we
            determine the Term Scores. As seen in Table 6, the terms used for
            simulations S1 and S2 , especially those for the relevant résumés,
            are quite different from the terms found in S3 and in the job offer.
            They can be considered as “bad” in terms of representativeness.
            Thus, in S1 we gave these “bad” n-grams the power to reﬂect the
            classes, even though they do not truly represent them; the consequences are bad rankings. In S2 we deleted
            these “bad” terms,
            while the rest of terms represented the classes, although with poor
            Term Scores; the resulting rankings are affected negatively but not
            as much as in S1 .
            In the previous results, we can see that the terms chosen by
            HRM may have a crucial role in the performance of Vocabulary
            Scoring, and as a consequence on the performance of the Relevance Factor. In other words, to choose terms
            that do not correctly
            represent what an HRM wants and does not want can negatively
            impact the ranking of résumés.
            Related to this last point, we want to know how the Vocabulary
            Scoring is affected by the way the terms are sorted because it may
            not be an obvious task for an HRM to perform. In fact, an HRM
            23
            Simulations S1 and S2 sort in the same way the n-grams; their difference is that
            S2 gives a Term Score of 0 to the ﬁrst 50 n-grams. Simulation S3 makes use of all
            the information available in the job to sort the terms presented in the 20 résumés
            analyzed.

            104

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            Fig. 7. Summary of a Project Manager job offer. The job offer comes from one of the 60 job postings to which
            we found their respective job offers. The original job offer
            was in French; we translated it to English and summarized it.
            Table 6
            Squared probabilities, sum of weights, number of documents, factors and rank for a set of terms according to
            each Vocabulary Scoring simulation. All the n-grams, originally in French but translated to
            English, belong to the résumés linked to the job offer presented in Fig. 7. The job has in total 36 relevant
            résumés and 29 irrelevant ones.
            Simulations

            S1 and S2

            Class

            n-gram (t)

            pc (t)

             W(t)

            Dc (t)

            fc (t)

            Rank

            Irrelevant

            Project engineer
            Micro-techniques
            Investment
            SolidWorks Catia V5
            Supplier France
            Business
            Rail
            Planning
            Range
            Respect

            1
            1
            1
            1
            1
            1
            1
            1
            1
            1

            0.024
            0.022
            0.013
            0.019
            0.019
            0.040
            0.030
            0.024
            0.023
            0.024

            3
            2
            3
            2
            2
            7
            7
            8
            8
            7

            0.072
            0.045
            0.040
            0.039
            0.039
            0.285
            0.216
            0.196
            0.189
            0.174

            1
            2
            3
            4
            5
            1
            2
            3
            4
            5

            Responsible supplier
            Unit
            Renault project
            To orient
            Validation piece
            Rail
            Alstom transport
            Train
            TGV
            CAD software

            1
            1
            1
            1
            1
            1
            1
            1
            1
            1

            0.038
            0.026
            0.032
            0.024
            0.041
            0.023
            0.074
            0.076
            0.062
            0.048

            4
            4
            3
            4
            2
            22
            8
            7
            6
            5

            0.154
            0.106
            0.098
            0.096
            0.083
            5.098
            0.598
            0.532
            0.372
            0.241

            1
            2
            3
            4
            5
            1
            2
            3
            4
            5

            Relevant

            Irrelevant

            S3

            Relevant

            can ask how to determine whether one term better represents the
            relevant or irrelevant résumés than another one. Moreover, they
            can question whether to “incorrectly” sort one term would affect
            the resulting ranking at the same level as choosing a bad term. To
            answer these questions, instead of computing the Term Score with
            Eq. (7), we decided to assign a Term Score of 1 to the 50 more
            representative n-grams of each class. This is equivalent to saying
            that the order in which the n-grams are sorted has no importance.
            The results of setting the Term Scores equal to 1 using simulation S3 showed that at 10 résumés, we get a
            MAP of 0.913 ± 0.015;
            at 20 résumés, the MAP is 0.947 ± 0.012. The rANOVA between our
            method using Term Scores set to 1 and those computed with the
            5th root indicated there is no signiﬁcant difference at 10 and 20
            résumés ( p value = 1.7 × 10−3 and p value = 1.37 × 10−9 respectively). These outcomes do not mean that both
            methods are equivalent and as a consequence interchangeable, but that they perform
            very similarly.24 As well, the results obtained from using a Term
            Score of 1 may provide a hint that the success of Vocabulary Scoring is related more to the quality of the
            chosen n-grams and the
            weight difference we create with respect to the other terms, i.e.,
            those to which we set a Term Score of 0.01. In other words, to put
            the most representative n-gram at the 50th position of the Vocabulary Scoring does not affect the results as
            much as leaving it aside.
            One interesting thing we observed in ﬁve different job postings using S3 is that the top ranked n-grams from
            the relevant
            résumés appear in more documents than the top ranked n-grams
            from the irrelevant résumés. We see this behavior in column Dc (t)
            24
            The lack of signiﬁcant difference between two means does not express that they
            are equal. It indicates that we need more data to determine a signiﬁcant difference.
            However, the effect size of this difference may be very small and, in consequence,
            they would behave very similar in real conditions.

            of Table 6. If this is true for all the job postings, we could conﬁrm
            the ideas on which we based AIRP and MIRP: the résumés from
            relevant applicants have in common multiple terms while the irrelevant résumés usually present a great
            variety of terms that are
            not frequently shared. However, we must perform a deeper analysis to validate this hypothesis.
            Despite the interest to determine what would be the results using human judgments instead of simulations, it
            should be noted
            that this cannot be done without redoing the selection process.
            The main reason is the relation between the selection of applicants and the person speciﬁcation, a document
            that can evolve
            over time. In other words, the HRM who would redo the selection process may not have access to the previous
            person speciﬁcation. This may result in a different evaluation of résumés, especially those from the ﬁrst
            candidates who applied. However, we
            can imagine that in reality, humans would do a good job, even better than simulations, because they know a
            priori the person speciﬁcation.
            Although we did not test Vocabulary Scoring with a set of less
            than 50 n-grams, it may be possible to reduce this ﬁgure. In ﬁrst
            place, we should test whether a smaller Vocabulary Scoring with
            Term Scores set to 1, or determined by Eq. (7), have the same performance. If this is not the case, we may
            change Eq. (7). For example, a gradient closer to zero might help to give better results
            to the top 10 terms. Another option would be to further reduce
            the Term Score for the n-grams that do not appear in the Relevance Feedback. In previous experiments, not
            presented here, we
            observed that as we decreased the Term Scores of the unseen ngrams the results were boosted even more.
            Moreover, it could be of help to ﬁnd the n-grams or terms,
            and even their synonyms, that appear in the job offer and person speciﬁcation in order to improve or
            automate the generation

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            of Vocabulary Scorings. In other words, these n-grams or terms
            could be those that should be positioned at the top of the Vocabulary Scoring. To this end, we could make
            use of Human Resources lexica, ontologies and terminological extractors. However,
            the use of these resources may introduce some diﬃculties as terms
            may not correspond exactly to the n-grams used in the vector
            model.
            8. Conclusions and future work

            Credit authorship contribution statement
            Luis Adrián Cabrera-Diego: Conceptualization, Methodology,
            Software, Validation, Formal analysis, Investigation, Data curation,
            Writing - original draft, Writing - review & editing, Visualization. Marc El-Béze: Conceptualization,
            Methodology, Validation,
            Formal analysis, Investigation, Writing - review & editing, Supervision, Project administration, Funding
            acquisition. Juan-Manuel
            Torres-Moreno: Conceptualization, Methodology, Writing - review
            & editing, Supervision, Project administration, Funding acquisition. Barthélémy Durette: Conceptualization,
            Methodology, Formal
            analysis, Writing - review & editing, Supervision, Project administration.

            25
            We did not achieve the same results in the encrypted data set, as the résumés
            were encrypted without doing a deep pre-processing, like lemmatization or stop
            words deletion. Thus, the résumés contained a greater variety of terms and noisy
            words.

            106

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107

            Acknowledgments
        </corps>
        <conclusion>
            The massive access of the Internet has changed multiple aspects of our lives, and the way we ﬁnd and apply
            for a job offer
            is not an exception. Although the use of computers and the Internet has made easier to ﬁnd job offers and
            potential candidates
            to send their résumés or curricula vitae, it has negatively affected
            the performance of human resource managers during the selection
            process. Human resource managers have trouble to ﬁnd rapidly the
            candidates, among all who applied, that meet the job requirements
            and should be called for an interview.
            We presented two innovative methods for ranking résumés by
            relevance, making it easier for human resource managers to identify candidates with the desired
            characteristics. The methods here
            presented are innovative because they make use only of the résumés sent in response to a job offer. These
            methods contrast
            with state-of-the-art methods that usually compare résumés and
            job offers with proximity measures. Our methods are language independent and do not need semantic resources
            to work. Moreover,
            the methods presented here are statistically better than a random
            baseline or a baseline grounded on the similarity between résumés
            and a job offer.
            Moreover, we presented two different ways to apply Relevance
            Feedback in a résumé ranker. One method for applying Relevance
            Feedback works at a general level (Relevance Factor), while the
            other method works at a ﬁner lexical one (Vocabulary Scoring). Although the Relevance Factor helps to
            improve résumé rankings, we
            ﬁnd that it is its use along with Vocabulary Scoring that helps us
            to reach a Mean Average Precision of 0.937. Put differently, by using the Relevance Factor with Vocabulary
            Scoring we can correctly
            rank almost every résumé. As a consequence, we can reduce the
            time needed by human resource managers to ﬁnd the résumés of
            relevant applicants. It is important to note that the very good results obtained with Vocabulary Scoring
            reinforces the concept that
            relevant résumés share more characteristics with themselves than
            with irrelevant ones, as seen in our previous works.
            We believe that, within the résumés we can intrinsically ﬁnd a
            “facial composite” of the ideal candidate, and possibly the “facial
            composite” that represents the unqualiﬁed candidates. It may be
            these “facial composites” that enable us to rank résumés without
            the use of a job offer or semantic resources.
            We consider that methodologies based only on résumés and
            their vocabularies are the future of résumé rankers. The main reason to think this is that they are capable
            of offering excellent
            performance without being limited to one domain or language.
            Despite these methods were created to be used in a particular
            database, where it was impossible to have access to every job offer, we believe that it can be used in any
            database of résumés, only
            if these are separated by job postings. Furthermore, the methods
            here presented do not make use of any kind of semantic resources,
            which can make them easier to implement in under-resources languages.
            There are still things that must be studied with this kind of
            methods. In the ﬁrst place are the temporal aspects. We assumed
            in this article that all the résumés were present at the same time,
            but in real life this may not be true. On occasions, the process
            of recruitment and selection are done in parallel, i.e., once a résumé arrives to a human resource manager,
            it is analyzed. We have

            105

            to consider as well the evolution of the person speciﬁcation over
            time. In some cases, human resource managers are obliged to become more or less strict in order to ﬁlter the
            applicants. These
            changes, in consequence, will affect the human resource managers’
            perception regarding the relevance of applicants. Due to this effect,
            the way to apply our methods may need to change, and we should
            evaluate until which extent they remain valid. However, despite all,
            the proposed methods could be used to evaluate a posteriori the
            reasons why a group of candidates was chosen to do an interview.
            Moreover, other human resource managers or psychologists may
            ﬁnd useful the tool to determine whether human resource managers were affected by personality inferences,
            misspellings or any
            kind of discrimination.
            Another aspect to take into account is the way to match terms
            or concepts and n-grams. These representations are not the same,
            and this can infuse diﬃculty to some degree in the application of
            our methods. Put differently, a concept may be diﬃcult to represent with an n-gram. Finally, it should be
            analyzed the economics
            and whether human resource managers will adopt these methods
            to make their tasks easier.
            Regarding the scalability of the methods here presented, we do
            not observe any particular problem. As we indicated in Section 5,
            the methods were called using the program GNU Parallel, meaning
            that each job posting was analyzed using different CPU threads.
            This indicates that multiple job postings can be processed at the
            same time without any collision. Furthermore, it is possible to parallelize the similarity between résumés,
            i.e., to use several threads
            to calculate multiple Dice’s Coeﬃcient scores at the same time. The
            only aspect to take into consideration is that the vectors representing the résumés should be accessible to
            every thread. At the end,
            all the methods described in this work can be easily scaled and
            distributed in a cluster.
            In the future, we would like to use word embedding in order
            to calculate the proximity between résumés differently. It could
            also be useful for Vocabulary Scorings. In addition, we will work
            on the improvements described in the discussion. Since the methods developed here are language independent,
            it will be easy to
            test them on other languages than French. Although this last task
            can be diﬃcult to achieve due to the lack of a corpus of real selection processes. During the
            experimentation, we observed that
            our methods can keep a good performance when they are tested
            on an encrypted version of the data set here used.25 Therefore, we
            can rely on this clue that for other languages, the methods should
            work as well.
            In conclusion, we hope that our methods and results will attract
            new and deeper research in this domain.
        </conclusion>
        <discussion>
            and MIRP. This ﬁnding means that the distribution of Inter-Résumé
            Proximities is often symmetrical and does not contain outliers.
            We observed that between all our methods and the random
            baseline there was a statistical difference, however between our
            other methods, in general, there was not a signiﬁcant difference.
            Moreover, the rANOVA performed on the results presented over
            the subset of 60 job postings (Fig. 4) suggests that our methods
            are better than the method based on the similarity between job
            offers and résumés. We could see this, as evidence that résumés
            contain more information about the job requirements than the job
            offer does, at least without using semantic resources. This could
            also mean that the vocabulary used in the job offer and the résumés differs to a certain degree.
            It is interesting how in terms of MAP, our methods worked
            better over the 60 job postings to which we had access to the
            job offer than for the set of 171 job postings. One reason for this
            outcome might be that these 60 job postings had one particular
            characteristic: on average, the number of relevant résumés was
            2.2 times the number of irrelevant résumés. This contrasts with
            the average number of relevant résumés for the 171 job postings,
            which was 1.4 times the number of irrelevant résumés. Another
            explanation, is that this difference can be a signal that the “true”
            MAP, the one that would be obtained if we analyze the statistical
            population instead of a statistical sample, is located between 0.60
            and 0.73. Although these could be the main reasons, we do not
            leave aside the fact that there could be others, intrinsic or not, to
            these job postings. To ﬁnd these other reasons, we need to perform a deeper analysis of these job postings
            and validate whether
            the number of relevant résumés had an impact on the performance
            of AIRP and MIRP.
        </discussion>
        <biblio>
            This work was partially funded by the Agence National de la
            Recherche et de la Technologie (ANRT), France, through the CIFRE
            convention 2012/0293b and by the Consejo Nacional de Ciencia y
            Tecnología (CONACyT), Mexico, with the grant 327165.
            References
            Armstrong, M., & Taylor, S. (2014). Armstrong’s handbook of human resource management practice (13th). Kogan
            Page Publishers.
            Arthur, D. (2001). The employee recruitment and retention handbook. AMACOM.
            Barber, L. (2006). E-Recruitment developments. Institute for Employment Studies.
            Buckley, C., & Voorhees, E. M. (20 0 0). Evaluating evaluation measure stability. In
            Proceedings of the 23rd annual international ACM SIGIR conference on research and
            development in information retrieval (pp. 33–40). Athens, Greece: ACM. doi:10.
            1145/345508.345543.
            Cabrera-Diego, L. A. (2015). Automatic methods for assisted recruitment. Université
            d’Avignon et des Pays de Vaucluse Ph.D. thesis.
            Cabrera-Diego, L. A., Durette, B., Lafon, M., Torres-Moreno, J.-M., &
            El-Bèze, M. (2015). How can we measure the similarity between résumés
            of selected candidates for a job?. In Stahlbock, Robert, & Weiss, Gary M. (Eds.),
            Proceedings of the 11th international conference on data mining (DMIN’15)
            (pp. 99–106). Las Vegas, USA
            Chapman, D. S., & Webster, J. (2003). The use of technologies in the recruiting,
            screening, and selection processes for job candidates. International Journal of Selection and Assessment,
            11(2–3), 113–120. doi:10.1111/1468-2389.00234.
            Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd). Hillsdale,
            USA: Lawrence Earlbaum Associates.
            Cole, M. S., Feild, H. S., Giles, W. F., & Harris, S. G. (2009). Recruiters’ inferences of applicant
            personality based on résumé screening: Do paper people
            have a personality? Journal of Business and Psychology, 24(1), 5–18. doi:10.1007/
            s10869- 008- 9086- 9.
            Cossu, J.-V. (2015). Analyse de l’image de marque sur le Web 2.0. Avignon, France:
            Université d’Avignon et des Pays de Vaucluse Ph.D. thesis.
            Cossu, J.-V., Janod, K., Ferreira, E., Gaillard, J., & El-Bèze, M. (2014). LIA@RepLab
            2014: 10 methods for 3 tasks. In L. Cappellato, N. Ferro, M. Halvey, & W. Kraaij
            (Eds.), Working notes for 4th International Conference of the CLEF initiative
            (pp. 1458–1467). Sheﬃeld, UK
            Elkington, T. (2005). Bright future for online recruitment. Personnel Today, 9.
            Faliagka, E., Iliadis, L., Karydis, I., Rigou, M., Sioutas, S., Tsakalidis, A., & Tzimas, G. (2013). On-line
            consistent ranking on e-recruitment: Seeking the
            truth behind a well-formed CV. Artiﬁcial Intelligence Review, 1–14. doi:10.1007/
            s10462- 013- 9414- y.
            Faliagka, E., Kozanidis, L., Stamou, S., Tsakalidis, A., & Tzimas, G. (2011). A personality mining system
            for automated applicant ranking in online recruitment systems. In S. Auer, O. Díaz, & G. A. Papadopoulos
            (Eds.), Proceedings of the 11th
            international conference web engineering (ICWE 2011). In Lecture Notes in Computer Science: 6757 (pp.
            379–382). Paphos, Cyprus: Springer Berlin Heidelberg.
            doi:10.1007/978- 3- 642- 22233- 7_30.
            Fang, X., & Zhan, J. (2015). Sentiment analysis using product review data. Journal of
            Big Data, 2(1), 5. doi:10.1186/s40537-015-0015-2.
            García-Sánchez, F., Martínez-Béjar, R., Contreras, L., Fernández-Breis, J. T., &
            Castellanos-Nieves, D. (2006). An ontology-based intelligent system for recruitment. Expert Systems with
            Applications, 31(2), 248–263. doi:10.1016/j.eswa.2005.
            09.023.
            Guo, S., Alamudun, F., & Hammond, T. (2016). RésuMatcher: A personalized résuméjob matching system. Expert
            Systems with Applications, 60(Supplement C), 169–
            182. doi:10.1016/j.eswa.2016.04.013.
            Harzallah, M., Leclère, M., & Trichet, F. (2002). CommOnCV: Modelling the competencies underlying a
            curriculum vitae. In Proceedings of the 14th international
            conference on software engineering and knowledge engineering (SEKE’02) (pp. 65–
            71). Ischia Island, Italy: ACM. doi:10.1145/568760.568773.
            Hutterer, M. (2011). Enhancing a job recommender with implicit user feedback. Vienna,
            Austria: Fakultät für Informatik der Technischen Universität Wien Master’s
            thesis.
            Järvelin, K., & Kekäläinen, J. (20 0 0). IR evaluation methods for retrieving highly relevant documents. In
            Proceedings of the 23rd annual international ACM SIGIR conference on research and development in information
            retrieval (pp. 41–48). Athens,
            Greece: ACM. doi:10.1145/345508.345545.
            Kessler, R., Béchet, N., Roche, M., El-Bèze, M., & Torres-Moreno, J. M. (2008a). Automatic proﬁling system
            for ranking candidates answers in human resources.
            In R. Meersman, Z. Tari, & P. Herrero (Eds.), On the move to meaningful internet systems: OTM 2008
            Workshops. In Lecture Notes in Computer Science: 5333
            (pp. 625–634). Monterrey, Mexico: Springer Berlin Heidelberg. doi:10.1007/
            978- 3- 540- 88875- 8_86.
            Kessler, R., Béchet, N., Roche, M., Torres-Moreno, J.-M., & El-Bèze, M. (2012). A hybrid approach to
            managing job offers and candidates. Information Processing &
            Management, 48(6), 1124–1135. doi:10.1016/j.ipm.2012.03.002.
            Kessler, R., Béchet, N., Torres-Moreno, J.-M., Roche, M., & El-Bèze, M. (2009). Job
            offer management: How improve the ranking of candidates. In Foundations of
            intelligent systems: Proceedings of 18th international symposium on methodologies
            for intelligent systems (ISMIS 2009). In Lecture Notes in Computer Science: 5722

            (pp. 431–441). Prague, Czech Republic: Springer Berlin Heidelberg. doi:10.1007/
            978- 3- 642- 04125- 9_46.
            Kessler, R., Torres-Moreno, J. M., & El-Bèze, M. (2008b). E-Gen: Proﬁlage automatique de candidatures. In
            Actes de la 15ème conférence sur le Traitement Automatique des Langues Naturelles (TALN 2008) (pp. 370–379).
            Avignon, France
            Kmail, A. B., Maree, M., & Belkhatir, M. (2015). MatchingSem: Online recruitment
            system based on multiple semantic resources. In 12th international conference
            on fuzzy systems and knowledge discovery (FSKD 2015) (pp. 2654–2659). doi:10.
            1109/FSKD.2015.7382376.
            Looser, D., Ma, H., & Schewe, K.-D. (2013). Using formal concept analysis for ontology maintenance in human
            resource recruitment. In F. Ferrarotti, & G. Grossmann (Eds.), Proceedings of the ninth Asia-Paciﬁc
            conference on conceptual modelling: 143 (pp. 61–68). Adelaide, Australia: Australian Computer Society, Inc.
            Martin-Lacroux, C. (2017). “Without the spelling errors I would have shortlisted
            her...”:The impact of spelling errors on recruiters’ choice during the personnel
            selection process. International Journal of Selection and Assessment, 25(3), 276–
            283. doi:10.1111/ijsa.12179.
            Martinez-Gil, J., Paoletti, A. L., Rácz, G., Sali, A., & Schewe, K.-D. (2018). Accurate and
            eﬃcient proﬁle matching in knowledge bases. Data & Knowledge Engineering,
            117, 195–215. doi:10.1016/j.datak.2018.07.010.
            Martinez-Gil, J., Paoletti, A. L., & Schewe, K.-D. (2016). A smart approach for matching, learning and
            querying information from the human resources domain. In
            M. Ivanović, B. Thalheim, B. Catania, K.-D. Schewe, M. Kirikova, P. Šaloun, A. Dahanayake, T. Cerquitelli,
            E. Baralis, & P. Michiardi (Eds.), Proceedings of the new
            trends in databases and information systems: ADBIS 2016 short papers and workshops, BigDap, DCSA, DC (pp.
            157–167). Prague, Czech Republic: Springer International Publishing. doi:10.1007/978- 3- 319- 44066-8_17.
            Mason, R. L., Gunst, R. F., & Hess, J. L. (2003). Statistical design and analysis of experiments: With
            applications to engineering and science. Wiley Series in Probability
            and Statistics (2nd). Wiley-Interscience. doi:10.1002/0471458503.
            Menon, V. M., & Rahulnath, H. A. (2016). A novel approach to evaluate and rank candidates in a recruitment
            process by estimating emotional intelligence through
            social media data. In International conference on next generation intelligent systems (ICNGIS) (pp. 1–6).
            Kottayam, India: IEEE. doi:10.1109/ICNGIS.2016.7854061.
            Montuschi, P., Gatteschi, V., Lamberti, F., Sanna, A., & Demartini, C. (2014). Job recruitment and job
            seeking processes: How technology can help. IT Professional,
            16(5), 41–49. doi:10.1109/MITP.2013.62.
            Padró, L., & Stanilovsky, E. (2012). FreeLing 3. 0: Towards wider multilinguality.
            In N. Calzolari, K. Choukri, T. Declerck, M. U. Doğan, B. Maegaard, J. Mariani,
            A. Moreno, J. Odijk, & S. Piperidis (Eds.), Proceedings of the eight international
            conference on language resources and evaluation (LREC’12) (pp. 2473–2479). Istanbul, Turkey: ELRA.
            R Core Team (2018). R: A language and environment for statistical computing. R
            Foundation for Statistical Computing Vienna, Austria.
            Radevski, V., & Trichet, F. (2006). Ontology-based systems dedicated to human resources management: An
            application in e-Recruitment. In R. Meersman, Z. Tari,
            & P. Herrero (Eds.), On the move to meaningful internet systems 2006: OTM
            2006 Workshops. In Lecture Notes in Computer Science: 4278 (pp. 1068–1077).
            Montpellier, France: Springer Berlin Heidelberg. doi:10.1007/11915072_9.
            Rocchio, J. J. (1971). Relevance feedback in information retrieval. In G. Salton (Ed.),
            The SMART retrieval system: Experiments in automatic document processing. In Automatic Computation (pp.
            313–323). Englewood Cliffs, N.J., USA: Prentice-Hall.
            Salton, G., Wong, A., & Yang, C.-S. (1975). A vector space model for automatic indexing. Communications of
            the ACM, 18(11), 613–620. doi:10.1145/361219.361220.
            Sen, A., Das, A., Ghosh, K., & Ghosh, S. (2012). Screener: A system for extracting education related
            information from resumes using text based information extraction system. In Proceedings of 2012
            international on computer and software modeling (ICCSM 2012). In International proceedings of computer
            science & information
            technology: 54 (pp. 31–35). International Association of Computer Science and
            Information Technology Press (IACSIT Press). doi:10.7763/IPCSIT.2012.V54.06.
            Senthil Kumaran, V., & Sankar, A. (2012). Expert locator using concept linking. International Journal of
            Computational Systems Engineering, 1(1), 42–49. doi:10.1504/
            IJCSYSE.2012.044742.
            Senthil Kumaran, V., & Sankar, A. (2013). Towards an automated system for intelligent screening of
            candidates for recruitment using ontology mapping (EXPERT). International Journal of Metadata, Semantics and
            Ontologies, 8(1), 56–64.
            doi:10.1504/IJMSO.2013.054184.
            Singh, A., Rose, C., Visweswariah, K., Chenthamarakshan, V., & Kambhatla, N. (2010).
            PROSPECT: A system for screening candidates for recruitment. In Proceedings
            of the 19th ACM international conference on information and knowledge management (CIKM 2010) (pp. 659–668).
            Toronto, Canada: ACM. doi:10.1145/1871437.
            1871523.
            Spärck-Jones, K. (1972). A statistical interpretation of term speciﬁcity and its application in retrieval.
            Journal of Documentation, 28(1), 11–21. doi:10.1108/eb026526.
            Tange, O. (2011). GNU parallel - The command-line power tool. login: The USENIX
            Magazine, 36(1), 42–47.
            Thompson, M. A. (20 0 0). The global resume and CV guide. Chichester, New York: Wiley.
            Tinelli, E., Colucci, S., Donini, F. M., Di Sciascio, E., & Giannini, S. (2017). Embedding
            semantics in human resources management automation via SQL. Applied Intelligence, 46(4), 952–982.
            doi:10.1007/s10489- 016- 0868- x.
            Torres-Moreno, J.-M., El-Bèze, M., Bellot, P., & Béchet, F. (2012). Opinion detection as
            a topic classiﬁcation problem. In É. Gaussier, & F. Yvon (Eds.), Textual information
            access: Statistical models (pp. 337–368). Wiley-ISTE. doi:10.1002/9781118562796.
            ch9.

            L.A. Cabrera-Diego, M. El-Béze and J.-M. Torres-Moreno et al. / Expert Systems With Applications 123 (2019)
            91–107
            Trichet, F., Bourse, M., Leclère, M., & Morin, E. (2004). Human resource management
            and semantic web technologies. In Proceedings of information and communication technologies: From theory to
            applications (ICTTA’04) (pp. 641–642). Damascus, Syria: IEEE. doi:10.1109/ICTTA.2004.1307928.
            Voorhees, E. M., & Harman, D. (2001). Overview of TREC 2001. In Proceedings of the
            10th Text REtrieval Conference (TREC 2001) (pp. 1–15). Gaithersburg, Maryland,
            USA: National Institute of Standards and Technology (NIST).

            107

            Zaroor, A., Maree, M., & Sabha, M. (2017). A hybrid approach to conceptual
            classiﬁcation and ranking of resumes and their corresponding job posts. In
            I. Czarnowski, R. J. Howlett, & L. C. Jain (Eds.), Intelligent decision technologies
            2017: Proceedings of the 9th KES international conference on intelligent decision
            technologies (KES-IDT 2017) - part I (pp. 107–119). Vilamoura, Portugal: Springer
            International Publishing. doi:10.1007/978- 3- 319- 59421- 7_10.
        </biblio>
    </article>
    <article>
        <preamble>Dynamical_Models_Explaining_Socia_Balance_and_Evolution_of_Cooperation.pdf</preamble>
        <titre>Dynamical Models Explaining Social Balance and Evolution of Cooperation</titre>
        <auteur>
            Vincent Antonio Traag1*, Paul Van Dooren1, Patrick De Leenheer 2
            1 ICTEAM, Université catholiquede Louvain, Louvain-la-Neuve, Belgium, 2 Department of Mathematics,
            University of Florida, Gainesville, Florida, United States of America
            Citation: Traag VA, Van Dooren P, De Leenheer P (2013) Dynamical Models Explaining Social Balance and
            Evolution of Cooperation. PLoS ONE 8(4): e60063.
            doi:10.1371/journal.pone.0060063
            Editor: Renaud Lambiotte, University of Namur, Belgium
            Received September 28, 2012; Accepted February 25, 2013; Published April 25, 2013
            Copyright: ß 2013 Traag et al. This is an open-access article distributed under the terms of the Creative
            Commons Attribution License, which permits
            unrestricted use, distribution, and reproduction in any medium, provided the original author and source are
            credited.
            Funding: This research received support from a grant ‘‘Actions de recherche concertées – Large Graphs and
            Networks’’ of the ‘‘Communauté Française de
            Belgique’’ and from the Belgian Network DYSCO (Dynamical Systems, Control, and Optimization,
            http://sites.uclouvain.be/dysco/), funded by the Interuniversity
            Attraction Poles Programme, initiated by the Belgian State, Science Policy Office. PDL was provided a
            research fellowship by VLAC (Vlaams Academisch Centrum)
            and was hosted by Université catholique de Louvain as a visiting professor during sabbatical leave from the
            University of Florida, from which he received a Faculty
            Enhancement Opportunity. The funders had no role in study design, data collection and analysis, decision to
            publish, or preparation of the manuscript.
            Competing Interests: The authors have declared that no competing interests exist.
            * E-mail: Vincent.Traag@uclouvain.be
        </auteur>
        <abstract>
            Social networks with positive and negative links often split into two antagonistic factions. Examples of
            such a split abound:
            revolutionaries versus an old regime, Republicans versus Democrats, Axis versus Allies during the second
            world war, or the
            Western versus the Eastern bloc during the Cold War. Although this structure, known as social balance, is
            well understood, it
            is not clear how such factions emerge. An earlier model could explain the formation of such factions if
            reputations were
            assumed to be symmetric. We show this is not the case for non-symmetric reputations, and propose an
            alternative model
            which (almost) always leads to social balance, thereby explaining the tendency of social networks to split
            into two factions.
            In addition, the alternative model may lead to cooperation when faced with defectors, contrary to the
            earlier model. The
            difference between the two models may be understood in terms of the underlying gossiping mechanism: whereas
            the
            earlier model assumed that an individual adjusts his opinion about somebody by gossiping about that person
            with
            everybody in the network, we assume instead that the individual gossips with that person about everybody. It
            turns out
            that the alternative model is able to lead to cooperative behaviour, unlike the previous model.
            propose a second model that is based on a different underlying
            gossiping mechanism, and show that it generically does lead to
            social balance, even when reputations are not symmetric.
            Moreover, there is a natural connection between negative links
            and the evolution of cooperation: we consider positive links as
            indicating cooperation and negative links as defection. We will
            show that our alternative model is able to lead to cooperation,
            whereas the earlier model cannot.
        </abstract>
        <introduction>
            Why do two antagonistic factions emerge so frequently in social
            networks? This question was already looming in the 1940s, when
            Heider [1] examined triads of individuals in networks, and
            postulated that only balanced triads are stable. A triad is balanced
            when friends agree in their opinion of a third party, while foes
            disagree, see Fig. 1. The individuals in an unbalanced triad have
            an incentive to adjust their opinions so as to reduce the stress
            experienced in such a situation [2]. Once an adjustment is made,
            the triad becomes balanced, and the stress disappears.
            A decade later, Harary [3] showed that a complete social
            network splits in at most two factions if and only if all its triads are
            balanced, see also [4]. Such networks are called (socially) balanced
            as well. Since then, the focus of much of the research has been on
            detecting such factions in signed networks [5,6]. Many signed
            networks show evidence of social balance, although the split into
            factions might not be exact, that is, they are only nearly socially
            balanced [7–10].
            What has been lacking until fairly recently, are dynamical
            models that explain how social balance emerges. The purpose of
            this paper is to analyse two such models. One of these models,
            proposed first in [11], was proved to exhibit social balance in [12].
            However, this was done under a restrictive symmetry assumption
            for the reputation matrix. Here, we continue the analysis of this
            model and show that it generically does not lead to social balance
            when the symmetry assumption is dropped. In contrast, we
        </introduction>
        <corps>
            Dynamical Models Explaining Social Balance and
            Evolution of Cooperation


            PLOS ONE | www.plosone.org

            Earlier Model
            Certain discrete-time, stochastic dynamics have been investigated [13,14], but they exhibit so-called jammed
            states [15]: no
            change in the sign of a reputation improves the degree of social
            balance, as measured by the total number of balanced triads in the
            network. A surprisingly simple continuous-time model [11] was
            proved to converge to social balance for certain symmetric initial
            conditions [12]. The authors assume that the social network is
            described by a complete graph (everybody is connected to
            everybody), with weighted links representing reputations that
            change continuously in time. Let X denote the real-valued matrix
            of the reputations, so that Xij represents the opinion i has about j. It
            is positive whenever i considers j a friend, and negative if i thinks of
            j as an enemy. The network is balanced, if, up to a possible
            relabelling of the individuals, the sign structure of X takes one of
            two possible block forms:

            1

            April 2013 | Volume 8 | Issue 4 | e60063

            Dynamical Models Explaining Social Balance

            lim

            X ðtÞ

            t?1=l1 ð0Þ DX ðtÞDF

            ð1Þ

            Changes in the reputations are modelled as follows:
            X_ ~X 2 , or X_ ij ~

            X

            Xik Xkj ,

            ð4Þ

            Assuming that u1 has no zero entries, and up to a suitable
            permutation of its components, the latter limit takes one of the
            forms in Eq. 1. In other words, if the initial reputation matrix is
            symmetric and has a simple, positive eigenvalue, then the
            normalized reputation matrix becomes balanced in finite time.
            Our first main result is that this conclusion remains valid for
            normal initial conditions, i.e. for initial conditions that satisfy the
            equality X(0)XT (0) = XT (0)X(0), see SI Text S1, Theorem 2.
            Whereas the real eigenvalues behave similar to the symmetric
            case, the complex eigenvalues show circular behaviour, which
            results in small ‘‘bumps’’ in the dynamics as shown in Fig. 2 (see SI
            Fig. S1 for more detail). More precisely, if X(0) is normal and if
            l1(0) is a real, positive and simple eigenvalue which is larger than
            every other real eigenvalue (if any), then the solution X(t) of Eq. 2
            satisfies Eq. 4. Hence, once again, the normalized reputation
            matrix converges to a balanced state.
            Our second main result is that this conclusion does not carry
            over to the case where X(0) is not normal, see SI Text S1,
            Theorem 3. This general case is analysed by first transforming
            X(0) into its real Jordan-canonical form J(0): X(0) = TJ(0)T–1,
            where T consists of a basis of (the real and imaginary parts of)
            generalized eigenvectors of X(0). It can then be shown that the
            solution X(t) of Eq. 2 is given by TJ(t)T–1, where J(t) solves the
            matrix equation J_ ~J 2 , an equation which can still be solved
            explicitly. Hence, X(t) can still be determined. It turns out that if
            X(0) has a real, positive and simple eigenvalue l1(0) which is
            larger than every other real eigenvalue (if any), then the
            normalized reputation matrix satisfies:

            Figure 1. Social Balance. The two upper triads are balanced, while
            the two lower triads are unbalanced. According to the structure
            theorem [3], a complete graph can be split into (at most) two opposing
            factions, if and only if all triads are balanced. This is represented by the
            coloured matrix on the right, where blue indicates positive entries, and
            red negative entries.
            doi:10.1371/journal.pone.0060063.g001

            
            
            z {
            ðzÞor
            :
            { z

            ~u1 uT1 :

            ð2Þ

            k

            where X_ denotes the derivative with respect to time of the matrix
            X. The idea behind this model is that reputations are adjusted
            based on the outcome of a particular gossiping process. More
            specifically, suppose that Bob (individual i) wants to revise his
            opinion about John (individual j). Bob then asks everybody else in
            the network what they think of John. If one such opinion Xkj has
            the same sign as the opinion Bob has about his gossiping partner,
            i.e. as Xik, then Bob will increase his opinion about John. But if
            these opinions differ in sign, then Bob will decrease his opinion
            about John.
            The analysis for symmetric initial conditions X(0) = XT (0) was
            carried out in [12]: First, X(0) is diagonalized by an orthogonal
            transformation X(0) = UL(0)UT, where the columns of U are
            orthonormal eigenvectors u1,…,un of X(0) so that UUT = In, and
            L(0) is a diagonal matrix whose diagonal entries are the
            corresponding real eigenvalues l1(0) $ l2(0)$ ? $ ln(0) of X(0).
            Direct substitution of the matrix function UL(t)UT shows that it
            is the solution of Eq. 2 with initial condition X(0). Here, L(t) is
            a diagonal matrix, solving the uncoupled matrix equation
            : L~L2 with initial condition L(0). The diagonal entries of L(t)
            are obtained by integrating the scalar first order equations

            X ðtÞ
            u1 vT1
            ~
            ,
            t?1=l1 ð0Þ DX ðtÞDF
            Du1 vT1 DF
            lim

            ð5Þ

            where u1 and vT1 are left and right eigenvectors of X(0)
            respectively, that correspond to the eigenvalue l1(0). If we
            assume that none of the entries of u1 and v1 are zero, then we
            can always find a suitable permutation of the components of u1
            and v1 such that they have the following sign structure:
            1
            z
            BzC
            B C
            u1 ~B Cand vT1 ~ð z { D z { Þ
            @{A
            {
            0

            :

            l i ~l2i :
            li ðtÞ~

            
            li ð0Þ
            ½0,z?Þif l ð0Þƒ0
            , t [ 0,1 l ð0Þ iif l ð0Þw0
            ½ =i Þ i
            1{li ð0Þt

            Consequently, in general, the matrix limit in Eq. 5 has the sign
            structure:
            

            ð3Þ

            Hence, the solution X(t) blows up in finite time if and only if
            l1(0) .0. Moreover, if l1(0) .0 is a simple eigenvalue, then the
            solution X(t), normalized by its Frobenius norm, satisfies:

            PLOS ONE | www.plosone.org

            z
            {

            
            { z {
            D
            ,
            z { z

            as illustrated in Fig. 2. Clearly, this configuration doesn’t
            correspond to social balance any longer.

            2

            April 2013 | Volume 8 | Issue 4 | e60063

            Dynamical Models Explaining Social Balance

            Figure 2. The two models compared. The first row illustrates what happens generically for the model X_ ~X 2
            , while the second row displays the
            results for X_ ~XX T . Each row contains from left to right: (1) an illustration of the model; (2) the
            random initial state; (3) the dynamics of the model;
            and (4) the final state to which the dynamics converge. Blue indicates positive entries, and red negative
            entries. Although the first model converges
            to a rank one matrix, it is not socially balanced. The second model does converge generically to social
            balance. The small bumps in the dynamics for
            X_ ~X 2 are due to complex eigenvalues that show circular behaviour (see Fig. S1).
            doi:10.1371/journal.pone.0060063.g002

            and obtain

            Alternative Model
            Let us briefly reconsider the gossiping process underlying model
            X_ ~X 2 . In our example of Bob and John, the following happens.
            Bob asks others what they think of John. Bob takes into account
            what he thinks of the people he talks to, and adjusts his opinion of
            John accordingly. An alternative approach is to consider a type of
            homophily process [16–18]: people tend to befriend people who
            think alike. When Bob seeks to revise his opinion of John, he talks
            to John about everybody else (instead of talking to everybody else
            about John). For example, suppose that Bob likes Alice, but that
            John dislikes her. When Bob and John talk about Alice, they notice
            they have opposing views about her, and as a result the
            relationship between Bob and John deteriorates. On the other
            hand, should they share similar opinions about Alice, their
            relationship will improve. Thus, our alternative model for the
            update law of the reputations is:
            X_ ~XX T , or X_ ij ~

            X

            Xik Xjk :

            Z_ ~Z 2 zA0 AT0 :

            The latter matrix Riccati differential equation can be integrated, yielding the solution Z(t) explicitly,
            and hence S(t), as well as
            X(t), can be calculated.
            It turns out that if A0 ? 0, then X(t) always blows up in finite
            time. Moreover, using a perturbation argument, it can be shown
            there is a dense set of initial conditions X(0) such that the
            normalized solution of Eq. 6 converges to
            lim

            X ðt Þ

            t?t DX ðtÞD

            ~wwT ,

            ð8Þ

            F

            for some vector w, as t approaches the blow-up time t*, see SI Text
            S1, Theorem 5. If w has no zero entries, this implies that the
            normalized solution becomes balanced in finite time. Hence, the
            alternative model in Eq. 6 generically evolves to social balance, see
            Fig. 2.

            ð6Þ

            k

            Although there apparently is only a subtle difference in the
            gossiping processes underlying the models in Eq. 2 and 6, these
            models turn out to behave quite differently, as we discuss next.
            Our third main result is that for generic initial conditions, the
            normalized solution of system Eq. 6 converges to a socially
            balanced state in finite time. To show this, we decompose the
            solution X(t) into its symmetric and skew-symmetric parts:
            T
            X(t) = S(t)+ A(t), where S(t) = ST(t) and A(t) = –AT (t). Since X_ ~X_ ,
            the skew-symmetric part remains constant, and therefore
            A(t) = A(0) ; A0. The symmetric part then obeys the matrix
            Riccati differential equation S_ ~ðSzA0 ÞðS{A0 Þ. We introduce
            Z ðtÞ~e{A0 t SðtÞeA0 t to eliminate the linear terms in this equation,

            PLOS ONE | www.plosone.org

            ð7Þ

            Evolution of Cooperation
            Positive and negative links have a natural interpretation in the
            light of cooperation: positive links indicate cooperation and
            negative links indicate defection. There is then also a natural
            motivation for the alternative model in terms of cooperation.
            Again, suppose Bob wants to revise his opinion of John. For Bob it
            is important to know whether John is cooperative in order to
            determine whether he should cooperate with John or not. So,
            instead of asking Alice whether she has cooperated with John, Bob
            would like to know whether John has cooperated with her. In
            other words, Bob is not interested in Xkj but in Xjk, consistent with
            3

            April 2013 | Volume 8 | Issue 4 | e60063

            Dynamical Models Explaining Social Balance

            Eq. 6, illustrated in Fig. 2. This is also what is observed in studies
            on gossip: it often concerns what others did, not what one thinks of
            others [19,20].
            Indeed gossiping seems crucial in explaining the evolution of
            human cooperation through indirect reciprocity [21]. It has even
            been suggested that humans developed larger brains in order to
            gossip, so as to control the problem of cooperation through social
            interaction [22]. In general, the problem is that if defection allows
            individuals to gain more, why then do individuals cooperate? This
            is usually modelled in the form of a prisoner’s dilemma, in which
            each agent has the possibility to give his partner some benefit b at
            some cost c,b. So, if an agent’s partner cooperates (he gives the
            agent b) but the agent doesn’t cooperate (he doesn’t pay the cost c)
            his total payoff will be b. Considering the other possibilities results
            in the payoff matrix detailed in Fig. 3.
            Irrespective of the choice of the other player, it is better to defect
            in a single game. Suppose that the second player cooperates. Then
            if the first player cooperates he gains b – c, while if he defects he
            gains b, so defecting is preferable. Now suppose that the second
            player defects. The first player then has to pay c, but doesn’t have
            to pay anything when defecting. So indeed, in a single game, it is
            always better to defect, yet the payoff is higher if both cooperate,
            whence the dilemma.
            In reality, we do observe cooperation, and various mechanisms
            for explaining the evolution of cooperation have been suggested
            [23], such as kin selection [24,25], reciprocity [26] or group
            selection [27]. Humans have a tendency however to also cooperate
            in contexts beyond kin, group or repeated interactions. It is
            believed that some form of indirect reciprocity can explain the
            breadth of human cooperation [21]. Whereas in direct reciprocity
            the favour is returned by the interaction partner, in indirect
            reciprocity the favour is returned by somebody else, which usually
            involves some reputation. It has been theorized that such a
            mechanism could even form the basis of morality [28]. Additionally, reputation (and the fear of losing
            reputation) seems to play an
            important role in maintaining social norms [29–31].
            In general, the idea is the following: agents obtain some good
            reputation by helping others, and others help those with a good
            reputation. Initially a strategy known as image scoring was
            introduced [32]. Shortly after, it was argued that a different
            strategy, known as the standing strategy, should actually perform
            better [33], although experiments showed people tend to prefer
            the simpler image scoring strategy [34]. This led to more
            systematic studies of how different reputation schemes would
            perform [35–37]. Although much research has been done on
            indirect reciprocity, only few theoretical works actually study how
            gossiping shapes reputations [38,39]. Nonetheless, most studies

            (tacitly) assume that reputations are shaped through gossip.
            Additionally, it was observed experimentally that gossiping is an
            effective mechanism for promoting cooperation [40–42].
            Moreover, these reputations are usually considered as objective.
            That is, all agents know the reputation Xj of some agent j, and all
            agents have the same view of agent j. Private reputations–so that
            we have Xij, the reputation of j in the eyes of i–have usually been
            considered by allowing a part of the population to ‘‘observe’’ an
            interaction, and update the reputation accordingly. If too few
            agents are allowed to ‘‘observe’’ an interaction, the reputations Xij
            tend to become uncorrelated and incoherent. This makes
            reputation unreliable for deciding whether to cooperate or defect.
            The central question thus becomes how to model private
            reputations such that they remain coherent and reliable for
            deciding whether to cooperate or not.
            Dynamical models of social balance might provide an answer to
            this question. Although it allows to have private reputations–that is
            Xij–the dynamics could also lead to some coherence in the form of
            social balance. In addition, it models more explicitly the gossiping
            process, commonly suggested to be the foundation upon which
            reputations are forged.

            Simulation Results
            The reputations of the agents are determined by the dynamics
            of the two models. We call agents using X_ ~X 2 dynamics type A,
            and those using X_ ~XX T dynamics type B. We assume that agent
            i cooperates with j whenever Xij .0 and defects otherwise. Agents
            reproduce proportional to their fitness, determined by their payoff.
            Agents that do well (have a high payoff) have a higher chance of
            reproduction, and we are interested in knowing the probability
            that a certain type becomes fixated in the population (i.e. takes
            over the whole population), known as the fixation probability r.
            All simulations start off with an equal amount of agents, so if some
            type wins more often than his initial relative frequency, it indicates
            it has an evolutionary advantage. For the results presented here
            this comes down to r .1/2. More details on the simulations are
            provided in the Materials and Methods section at the end of the
            paper.
            The results are displayed in Fig. 4 using a normalized cost of c
            = 1 (the ratio b/c drives the evolutionary dynamics, see Materials
            and Methods and [23]). When directly competing against each
            other, type B has an evolutionary advantage (its fixation
            probability rB .1/2) compared to type A, already for relatively
            small benefits. When each type is playing against defectors (agents
            that always defect), type A seems unable to defeat defectors (rA
            ,1/2) for any b ,20, while type B performs quite well against
            them. When all three types are playing against each other results
            are similar (see SI Fig. S2). When varying the number of agents,
            the critical benefit b* at which type B starts to have an evolutionary
            advantage changes (i.e. where the fixation probability rB = 1/2).
            For b.b* agents using the model X_ ~XX T have a higher chance
            to become fixated, while for b,b* defectors tend to win. The
            inequality for type B to have an evolutionary advantage can be
            pﬃﬃ
            relatively accurately approximated by bwb  ~c n where c is
            estimated to be around c
            <
            1.7260.037 (95% confidence
            interval). Varying the intensity of selection does not alter the
            results qualitatively (see SI Fig. S3). Summarizing, type B is able to
            lead to cooperation and defeats type A. Based on these results, if a
            gossiping process evolved during the course of human history in
            order to maintain cooperation, the model X_ ~XX T seems more
            likely to have evolved than X_ ~X 2 . For smaller groups a smaller
            benefit is needed for the model X_ ~XX T to become fixated. This

            Figure 3. Prisoner’s Dilemma. Both players have the option to either
            Cooperate or Defect. Whenever an agent cooperates, it costs him c
            while his partners receives a benefit b.c, leading to the indicated
            payoffs.
            doi:10.1371/journal.pone.0060063.g003

            PLOS ONE | www.plosone.org

            4

            April 2013 | Volume 8 | Issue 4 | e60063

            Dynamical Models Explaining Social Balance

            Figure 4. Evolution of Cooperation. (A) The fixation probability (probability to be the sole surviving
            species) is higher for model X_ ~XX T than
            X_ ~X 2 . This implies that the model X_ ~XX T is more viable against defectors, and has an evolutionary
            advantage compared to X_ ~X 2 . (B) The point
            b* at which the model X_ ~XX T has an evolutionary advantage against defectors (i.e. the fixation
            probability r .1/2) depends on the number of
            pﬃﬃ
            agents n. The condition for the model X_ ~XX T to defeat defectors can be approximated by bwb  ~b n, with b
            <
            1.72.
            doi:10.1371/journal.pone.0060063.g004

            pﬃﬃ
            dependence seems to scale only as n, so that larger groups only
            need a marginally larger benefit in order to develop cooperation.

            formation and cooperation, and as such contributes to the study
            of indirect reciprocity.
        </corps>
        <conclusion>
            Materials and Methods

            To conclude, we have shown that the alternative model
            X_ ~XX T generically converges to social balance, whereas the
            model X_ ~X 2 did not. The current models exhibit several
            unrealistic features, we would like to address: (1) an all-to-all
            topology; (2) dynamics that blow-up in finite time; and (3)
            homogeneity of all agents. Although most of these issues can be
            addressed by specifying different dynamics, the resulting models
            are much more difficult to analyse, thereby limiting our
            understanding. Although the two models are somewhat simple,
            they are also tractable, and what we lose in truthfulness, we gain in
            deeper insights: in simplicity lies progress. Our current analysis
            offers a quite complete understanding, and we hope it provides a
            stepping stone to more realistic models, which we would like to
            analyse in the future.
            The difference between the two models can be understood in
            terms of gossiping: we assume that people who wish to revise their
            opinion about someone talk to that person about everybody else,
            while the earlier model assumed that people talk about that person
            to everybody else. Both gossiping and social balance are at the
            centre of many social phenomena [22,29,43,44], such as norm
            maintenance [30], stereotype formation [45] and social conflict
            [46]. For example, a classic work [29] on the established and
            outsiders found that gossiping was the fundamental driving force
            for the maintenance of the cohesive network of the established at
            the exclusion of the outsiders. Understanding how social balance
            may emerge might help to understand the intricacies of these
            social phenomena.
            Moreover, in light of the evolution of cooperation it appears
            that agents using X_ ~XX T dynamics perform well against
            defectors, and have an evolutionary advantage compared to
            agents using X_ ~X 2 dynamics. Contrary to other models of
            indirect reciprocity, not everybody might end up cooperating with
            everybody, and the population may split into two groups. This
            provides an interesting connection between social balance theory,
            gossiping and the evolution of cooperation. Our results improve
            our understanding of gossiping as a mechanism for group
            PLOS ONE | www.plosone.org

            In the simulations of the evolution of cooperation, the dynamics
            consist of two parts: (1) the interaction dynamics within each
            generation; and (2) the dynamics prescribing how the population
            evolves from generation to generation.

            Interaction Dynamics
            We include three possible types of agents in our simulations:
            Type A uses X_ ~X 2 dynamics,
            Type B uses X_ ~XX T dynamics, and
            Defectors have trivial reputation dynamics X_ ~0, with
            negative constant reputations.
            We can decompose the reputation matrix X(t) accordingly into
            three parts:
            0

            1
            X A ðt Þ
            B
            C
            X ðtÞ~@ XB ðtÞ A,
            X D ðt Þ
            where XA(t) are the reputations of all agents in the eyes of agents of
            type A, XB(t) for type B and XD(t) for defectors. The reputations
            XA(0) and XB(0) are initialized from a standard Gaussian
            distribution. The initial reputation for XD(0) will be set to a fixed
            negative value. To be clear, XD(0) is the reputation of all other
            agents in the eyes of defectors, which is negative initially. The
            initial reputation of the defectors themselves is of course not
            necessarily negative initially. For the results displayed here we
            have used XD(0) = 210, but results remain by and large the same
            when varying this parameter, as long as it remains sufficiently
            negative.
            Since we are dealing with continuous dynamics in this paper, we
            assume all agents are involved in infinitesimally short games at
            each time instance t. Each agent i may choose to either cooperate
            or defect with another agent j, and this decision may vary from one
            agent to the next. For agents of type A and type B the decision to

            5

            April 2013 | Volume 8 | Issue 4 | e60063

            Dynamical Models Explaining Social Balance

            respectively WA, WB and WD. In the next generation, the probability
            that agent i is of a specific type q can then be written as

            cooperate is based on the reputation: they defect whenever Xij(t) #
            0 and cooperate whenever Xij(t) .0. We define the cooperation
            matrix C(t) accordingly.
            
            0 if Xij ƒ0
            Cij ðtÞ~
            1 if Xij w0

            PrðTi ðgz1Þ~qÞ~Wq :
            This evolutionary mechanism can be seen as a Wright-Fisher
            process [47] with fitnesses ebPi ðgÞ . It is well known that this process
            converges faster than a Moran birth-death process, since it
            essentially takes n time steps in a Moran process to reproduce the
            effect of one time step in a Wright-Fisher process [47]. Because of
            the high computational costs (solving repeatedly a non-linear
            system of differential equations of size n2), this process is
            preferable.
            Higher b signifies higher selective pressure, and leads to a
            higher reproduction of those with a high payoff, and in the case
            that b R ‘ only those with the maximum payoff reproduce. On
            the other hand, for b R 0 this tends to the uniform distribution wi
            = 1/n, where payoffs no longer play any role. We have used b
            = 0.5 for the low selective pressure, b = 5 for the high selective
            pressure, reported in SI Fig. S3. For the results in the main text we
            have used b = 1.
            For an evolutionary neutral selection in where all Pi(g) = P are
            effectively the same, b has no effect, and wi = 1/n. Notice that if
            we rescale Pi(g) by 1/c so that the payoff effectively becomes

            Defectors will simply always defect. Whenever an agent i
            cooperates with j the latter receives a payoff of b at a cost of c to
            agent i. We integrate this payoff over all infinitesimally short
            games from time 0 to time t*, which can be represented as

            pðgÞ~

            ð
            1 t
            bC ðtÞT e{cC ðtÞedt,
            n 0

            where e = (1,…,1) the vector of all ones for a certain generation g.

            Evolutionary Dynamics
            We have simulated the evolution of cooperation for n
            = 10,20,…,60 agents, which stays constant throughout evolution.
            We consider four different schemes for initializing the first
            generation:
            pB(0)
            pD(0)
            pA(0)
            1. Type A vs Type B
            2. Type A vs Defectors
            3. Type B vs Defectors
            4. Type A,B and Defectors

            1/2
            1/2
            1/3

            1/2
            1/2
            1/3

            1/2
            1/2
            1/3

            ð
            T
            1
            1 t b
            Pi ðgÞ~
            C ðtÞ e{C ðtÞedt,
            c
            n 0 c
            and we rescale b by c, then the reproduction probabilities remain
            unchanged. Hence, only the ratio b/c effectively plays a role up to
            a rescaling of the intensity of selection. Since the point at which the
            evolution is neutral (i.e. r equals the initial proportional
            frequency), is independent of b, this point will only depend on
            the ratio b/c. So, we normalized the cost c = 1. To verify this, we
            also ran additional simulations with different costs, which indeed
            gave the same results.
            We stop the simulation whenever one of the types becomes
            fixated in the population. With fixation we mean that all other types
            have gone extinct, and only a single type remains. If no type has
            become fixated after 1,000 generations, we terminate the
            simulation and count as winner the most frequent type. This
            almost never happens, and the simulation usually stops after a
            relatively small number of generations.
            In total, we repeat this process 1,000 times for the results in the
            main text, and for the low (b = 0.5) and high (b = 5) selective
            pressure 100 times. This means that we run the evolutionary
            dynamics until one of the types has become fixated, and we record
            which type has ‘‘won’’. After that, we again start from the first
            generation, and run until fixation, and repeat this. Finally, we
            calculate how many rounds a type has ‘‘won’’ compared to the
            total number of rounds, which yields the fixation probability r.

            Here pA(0),pB(0) and pD(0) are respectively the proportion of
            agents of type A, type B and defectors in the first generation. We
            use the vector Ti(g) M {A, B, D} to denote the type of agent i in
            generation g, so that Ti(g) = A if agent i is a type A player, Ti(g) = B
            for a type B player, and Ti(g) = D for a defector. We are interested
            in estimating the probability that a single type takes over the whole
            population, known as the fixation probability rA, rB and rD for the
            three different types. If a type has no evolutionary advantage, it is
            said to be evolutionary neutral, and in that case its fixation
            probability is equal to its initial frequency, e.g. for type A
            rA = pA(0).
            We will keep the population constant at the initial n, and simply
            choose n new agents according to their payoff for the next
            generation. This can be thought of as choosing n times one of the n
            agents in the old generation for reproduction. Let wi denote the
            probability that an agent is selected for reproduction, which we
            define as
            ebPi ðgÞ
            wi ~ P bP ðgÞ :
            i
            ie
            Since we are only interested in the number of agents of a certain
            type, we can also gather all payoffs for the same type of agents, and
            write
            Wq ~

            X

            Supporting Information
            Figure S1 Phase portrait of system S12-S13. Circular
            orbits in the upper half plane (a .0) are traversed counter
            clockwise, whereas circular orbits in the lower half plane (a ,0) are
            traversed clockwise.
            (TIFF)

            wi ,

            i:Ti ðgÞ~q

            where q M {A, B, D} represents the type of agent. The probability
            to select a type A agent, a type B agent or a defector is then

            PLOS ONE | www.plosone.org

            Figure S2

            Results including type A, B and defectors.

            (TIFF)
            6

            April 2013 | Volume 8 | Issue 4 | e60063

            Dynamical Models Explaining Social Balance

            Figure S3 Results different intensities of selection.
        </conclusion>
        <discussion>
            The paper emphasizes that gossip dynamics shape reputation and cooperation. The success of the alternative
            model in simulations highlights how minor differences in interaction dynamics (direction of gossip) lead to
            significant macroscopic outcomes. The discussion acknowledges simplifications in the models (e.g., full
            connectivity, finite-time blowups, agent homogeneity) and calls for extensions toward more realistic
            networks and behaviors. This work bridges social psychology, network science, and evolutionary game theory.
        </discussion>
        <biblio>
            Author Contributions

            (TIFF)

            Conceived and designed the experiments: VAT. Performed the experiments: VAT. Analyzed the data: VAT PVD PDL.
            Contributed reagents/
            materials/analysis tools: VAT PVD PDL. Wrote the paper: VAT PVD
            PDL.

            Text S1 Proofs and details of statements in the main

            paper.
            (PDF)

            References
            25. Hamilton W (1964) The genetical evolution of social behaviour I. J Theor Biol 7:
            1–16.
            26. Axelrod R, Hamilton W (1985) The Evolution of Cooperation, volume 211.
            New York: Basic Books, 1390–1396 pp.
            27. Wilson DS (1975) A theory of group selection. Proc Natl Acad Sci U S A 72:
            143–146.
            28. Alexander RD (1987) The Biology of Moral Systems. New York: Aldine de
            Gruyter.
            29. Elias N, Scotson JL (1994) The Established and the Outsiders. London: SAGE
            Publications.
            30. Friedkin NE (2001) Norm formation in social influence networks. Soc Networks
            23: 167–189.
            31. Fehr E, Fischbacher U (2004) Third-party punishment and social norms.
            Evolution and Human Behavior 25: 63–87.
            32. Nowak MA, Sigmund K (1998) Evolution of indirect reciprocity by image
            scoring. Nature 393: 573–7.
            33. Leimar O, Hammerstein P (2001) Evolution of cooperation through indirect
            reciprocity. Proc Biol Sci 268: 745–753.
            34. Milinski M, Semmann D, Bakker TC, Krambeck HJ (2001) Cooperation
            through indirect reciprocity: image scoring or standing strategy? Proc Biol Sci
            268: 2495–2501.
            35. Ohtsuki H, Iwasa Y (2006) The leading eight: social norms that can maintain
            cooperation by indirect reciprocity. J Theor Biol 239: 435–44.
            36. Brandt H, Sigmund K (2004) The logic of reprobation: assessment and action
            rules for indirect reciprocation. J Theor Biol 231: 475–486.
            37. Ohtsuki H, Iwasa Y (2004) How should we define goodness?–reputation
            dynamics in indirect reciprocity. J Theor Biol 231: 107–120.
            38. Nakamaru M, Kawata M (2004) Evolution of rumours that discriminate lying
            defectors. Evol Ecol Res 6: 261–283.
            39. Traag VA, Van Dooren P, Nesterov Y (2011) Indirect reciprocity through
            gossiping can lead to cooperative clusters. In: IEEE Symposium on Artificial Life
            2011. Piscataway: IEEE, pp.154–161.
            40. Piazza J, Bering JM (2008) Concerns about reputation via gossip promote
            generous allocations in an economic game. Evolution and Human Behavior 29:
            172–178.
            41. Sommerfeld RD, Krambeck HJ, Milinski M (2008) Multiple gossip statements
            and their effect on reputation and trustworthiness. Proc Biol Sci 275: 2529–
            2536.
            42. Sommerfeld RD, Krambeck HJ, Semmann D, Milinski M (2007) Gossip as an
            alternative for direct observation in games of indirect reciprocity. Proc Natl Acad
            Sci U S A 104: 17435–17440.
            43. Gluckman M (1963) Gossip and Scandal. Curr Anthropol 4: 307–316.
            44. Foster EK (2004) Research on gossip: Taxonomy, methods, and future
            directions. Rev Gen Psychol 8: 78–99.
            45. Wert SR, Salovey P (2004) A social comparison account of gossip. Rev Gen
            Psychol 8: 122–137.
            46. Labianca G, Brass D, Gray B (1998) Social networks and perceptions of
            intergroup conflict: The role of negative relationships and third parties.
            Academy of Management journal 41: 55–67.
            47. Gillespie JH (2004) Population Genetics: A Concise Guide. Baltimore: The John
            Hopkins University Press.

            1. Heider F (1946) Attitudes and Cognitive Organization. J Psychol 21: 107–112.
            2. Bearman PS, Moody J (2004) Suicide and Friendships Among American
            Adolescents. Am J Public Health 94: 89–95.
            3. Harary F (1953) On the notion of balance of a signed graph. The Michigan
            Mathematical Journal 2: 143–146.
            4. Cartwright D, Harary F (1956) Structural balance: a generalization of Heider’s
            theory. Psychol Rev 63: 277–293.
            5. Doreian P, Mrvar A (1996) A partitioning approach to structural balance. Soc
            Networks 18: 149–168.
            6. Traag VA, Bruggeman J (2009) Community detection in networks with positive
            and negative links. Phys Rev E 80: 036115.
            7. Szell M, Lambiotte R, Thurner S (2010) Multirelational organization of largescale social networks in an
            online world. Proc Natl Acad Sci U S A 107: 13636–
            41.
            8. Leskovec J, Huttenlocher D, Kleinberg J (2010) Predicting positive and negative
            links in online social networks. In: WWW 2010.
            9. Facchetti G, Iacono G, Altafini C (2011) Computing global structural balance in
            large-scale signed social networks. Proc Natl Acad Sci U S A 108: 20953–20958.
            10. Kunegis J, Lommatzsch A, Bauckhage C (2009) The slashdot zoo. In:
            Proceedings of the 18th international conference on World wide web - WWW
            ’09. New York, New York, USA: ACM Press, p. 741.
            11. Kulakowski K, Gawronski P, Gronek P (2005) The Heider balance - a
            continuous approach. Int J Mod Phys C 16: 707–716.
            12. Marvel SA, Kleinberg J, Kleinberg RD, Strogatz SH (2011) Continuous-time
            model of structural balance. Proc Natl Acad Sci U S A 108: 1771–6.
            13. Antal T, Krapivsky PL, Redner S (2005) Dynamics of social balance on
            networks. Phys Rev E 72: 36121.
            14. Radicchi F, Vilone D, Yoon S, Meyer-Ortmanns H (2007) Social balance as a
            satisfiability problem of computer science. Phys Rev E 75: 026106.
            15. Marvel S, Strogatz S, Kleinberg J (2009) Energy Landscape of Social Balance.
            Phys Rev Lett 103: 198701.
            16. Mcpherson M, Smith-Lovin L, Cook JM (2001) Birds of a Feather: Homophily
            in Social Networks. Annu Rev Sociol 27: 415–444.
            17. Durrett R, Levin SA (2005) Can stable social groups be maintained by
            homophilous imitation alone? J Econ Behav Organ 57: 267–286.
            18. Fu F, Nowak MA, Christakis NA, Fowler JH (2012) The Evolution of
            Homophily. Scientific Reports 2.
            19. McAndrew FT, Bell EK, Garcia CM (2007) Who Do We Tell and Whom Do
            We Tell On? Gossip as a Strategy for Status Enhancement. J Appl Soc Psychol
            37: 1562–1577.
            20. Paine R (1967) What is Gossip About? An Alternative Hypothesis. Man 2: 278–
            285.
            21. Nowak MA, Sigmund K (2005) Evolution of indirect reciprocity. Nature 437:
            1291–1298.
            22. Dunbar RIM (1998) Grooming, Gossip, and the Evolution of Language.
            Cambridge: Harvard University Press, 242 pp.
            23. Nowak MA (2006) Five rules for the evolution of cooperation. Science (New
            York, NY) 314: 1560–3.
            24. Smith JM, Maynard Smith J (1982) Evolution and the Theory of Games.
            Cambridge: Cambridge University Press, 226 pp.

            PLOS ONE | www.plosone.org

            7

            April 2013 | Volume 8 | Issue 4 | e60063
        </biblio>
    </article>
    <article>
        <preamble>A_Benders_Decomposition_Approach_toCorrelation_Clustering.pdf</preamble>
        <titre>A Benders Decomposition Approach to Correlation Clustering</titre>
        <auteur>
            Margret Keuper
            University of Mannheim
            Baden Wurttemberg, Germany
            keuper@uni-mannheim.de

            Jovita Lukasik
            University of Mannheim
            Baden Wurttemberg, Germany
            jovita@informatik.uni-mannheim.de

            Maneesh Singh
            Verisk
            Jersey City, New Jersey, USA
            maneesh.singh@verisk.com

            Julian Yarkony
            Verisk
            Jersey City, New Jersey, USA
            julian.yarkony@verisk.com
        </auteur>
        <abstract>
            We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which
            we treat as an integer linear program (ILP). We
            reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a
            classic technique from operations research. Our Benders
            decomposition formulation has many subproblems, each associated with a node in
            the CC instance’s graph, which can be solved in parallel. Each Benders subproblem
            enforces the cycle inequalities corresponding to edges with negative (repulsive)
            weights attached to its corresponding node in the CC instance. We generate
            Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate
            optimization. Our Benders decomposition approach provides a promising new
            avenue to accelerate optimization for CC, and, in contrast to previous cutting plane
            approaches, theoretically allows for massive parallelization.
        </abstract>
        <introduction>
            Many computer vision tasks involve partitioning (clustering) a set of observations into unique entities.
            A powerful formulation for such tasks is that of (weighted) correlation clustering (CC). CC is defined
            on a sparse graph with real valued edge weights, where nodes correspond to observations and
            weighted edges describe the affinity between pairs of nodes.
            For example, in image segmentation (on superpixel graphs), nodes correspond to superpixels and
            edges indicate adjacency between superpixels. The weight of the edge between a pair of superpixels
            relates to the probability, as defined by a classifier, that the two superpixels belong to the same ground
            truth entity. This weight is positive, if the probability is greater than 12 and negative if it is less than
            12 .
            The magnitude of the weight is a function of the confidence of the classifier.
            The CC cost function sums up the weights of the edges separating connected components (referred
            to as entities) in a proposed partitioning of the graph. Optimization in CC partitions the graph into
            entities so as to minimize the CC cost. CC is appealing, since the optimal number of entities emerges
            naturally as a function of the edge weights, rather than requiring an additional search over some
            model order parameter describing the number of clusters (entities) [37].
            Optimization in CC is NP-hard for general graphs [5]. Previous methods for the optimization of CC
            problems such as described in Andres et al. [1, 2] and Nowozin and Jegelka [25] are based on linear
            programming with cutting planes. They do not scale easily to large CC problem instances and are not
            Preprint. Under review.

            easily parallelizable. The goal of this paper is to introduce an efficient mechanism for optimization in
            CC for domains, where massively parallel computation could be employed.
            In this paper we apply the classic Benders decomposition from operations research [10] to CC for
            computer vision. Benders decomposition is commonly applied in operations research to solve mixed
            integer linear programs (MILP) that have a special but common block structure. Benders decomposition
            partitions the variables in the MILP between a master problem and a set of subproblems. The
            block structure requires that no row of the constraint matrix of the MILP contains variables from
            more than one subproblem. Variables explicitly enforced to be integral lie only in the master problem.
            Optimization in Benders decomposition is achieved using a cutting plane algorithm. Optimization
            proceeds with the master problem solving optimization over its variables. The subsequent solution
            of the subproblems can be done in parallel and provides primal/dual solutions over their variables
            conditioned on the solution to the master problem. The dual solutions to the subproblems provide
            constraints to the master problem. Optimization continues until no further constraints are added to
            the master problem.
            Benders decomposition is an exact MILP programming solver, but can be intuitively understood as
            a coordinate descent procedure, iterating between the master problem and the subproblems. Here,
            solving the subproblems not only provides a solution for their variables, but also a lower bound in the
            form of a hyper-plane over the master problem’s variables. This lower bound is tight at the current
            solution to the master problem.
            Benders decomposition is accelerated using the seminal operations research technique of MagnantiWong Benders
            rows (MWR) [23]. MWR are generated by solving the Benders subproblems with an
            alternative (often random) objective under the hard constraint of optimality (possibly within a factor)
            regarding the original objective of the subproblem.
            Our contribution is the use of Benders decomposition with MWR to tackle optimization in CC. This
            allows for massive parallelization, in contrast to classic approaches to CC such as in Andres et al. [1].
        </introduction>
        <corps>
            Correlation clustering has been successfully applied to multiple problems in computer vision including
            image segmentation, multi-object tracking, instance segmentation and multi-person pose estimation.
            The classical work of Andres et al. [1] models image segmentation as CC, where nodes correspond to
            superpixels. Andres et al. [1] optimize CC using an integer linear programming (ILP) branch-and-cut
            strategy which precludes parallel execution. Kim et al. [21] extend CC to include higher-order cost
            terms over sets of nodes, which they solve using an approach similar to [1]. A parallel optimization
            scheme for complete, unweighted graphs has been proposed by Pan et al. [26]. This approach relies
            on random sampling and only provides optimality bounds.
            Yarkony et al. [37] tackle CC in the planar graph structured problems commonly found in computer
            vision. They introduce a column generation [16, 6] approach, where the pricing problem corresponds
            to finding the lowest reduced cost 2-colorable partition of the graph, via a reduction to minimum cost
            perfect matching [13, 29, 22]. This approach has been extended to hierarchical image segmentation
            in Yarkony and Fowlkes [35] and to specific cases of non-planar graphs in Yarkony [34], Zhang et al.
            [39], Andres et al. [3].
            Large CC problem instances such as defined in Keuper et al. [20, 19] and Beier et al. [9] are usually
            addressed by primal feasible heuristics [7, 8, 18, 20, 30]. Such approaches are highly relevant in
            practice whenever the optimal solution is out of reach, but they do not provide any guarantees on the
            quality of the solution.
            Tang et al. [31] tackles multi-object tracking using a formulation closely related to CC, where nodes
            correspond to detections of objects and edges are associated with probabilities of co-association.The
            work of Insafutdinov et al. [17] and Pishchulin et al. [27] build on Tang et al. [31] in order to formulate
            multi-person pose estimation using CC augmented with node labeling.
            Our work is derived from the classical work in operations research on Benders decomposition [10, 11,
            15]. Specifically, we are inspired by the fixed charge formulations of Cordeau et al. [12], which solves
            a mixed integer linear program over a set of fixed charge variables (opening links) and a larger set of
            fractional variables (flows of commodities from facilities to customers in a network) associated with
            2

            constraints. Benders decomposition reformulates optimization so as to use only the integer variables
            and converts the fractional variables into constraints. These constraints are referred to as Benders
            rows. Optimization is then tackled using a cutting plane approach. Optimization is accelerated by the
            use of MWR [23], which are more binding than the standard Benders rows.
            Benders decomposition has recently been introduced to computer vision (though not for CC), for the
            purpose of multi-person pose estimation [32, 33, 36]. In these works, multi-person pose estimation is
            modeled so as to admit efficient optimization, using column generation and Benders decomposition
            jointly. The application of Benders decomposition in our paper is distinct regarding the problem
            domain, the underlying integer program and the structure of the Benders subproblems.

            3

            Standard Correlation Clustering Formulation

            In this section, we review the standard optimization formulation for CC [1], which corresponds to a
            graph partitioning problem w.r.t. the graph G = (V, E). This problem is defined by the following
            binary edge labeling problem.
            Definition 1. Given a graph G = (V, E) with nodes v ∈ V and undirected edges (vi , vj ) ∈ E. A
            label xvi vj ∈ {0, 1} indicates with xvi vj = 1 that the nodes vi , vj are in separate components and is
            zero otherwise. Given the edge weight φvi vj ∈ R, the binary edge labeling problem is to find an edge
            label x = (xvi vj ) ∈ {0, 1}|E| , for which the total weight of the cut edges is minimized:
            min

            x∈{0,1}|E|

            s.t.

            X
            (vi ,vj )∈E −

            X

            X

            −φvi vj (1 − xvi vj ) +

            φ vi vj x vi vj

            (CC1 )

            (vi ,vj )∈E +

            xvi vj ≥ xvic vjc

            ∀c ∈ C,

            (1)

            (vi ,vj )∈Ec+

            where E − , E + denote the subsets of E, for which the weight φvi vj is negative and non-negative,
            respectively, C is the set of undirected cycles in E containing exactly one member of E − , (vic , vjc ) is
            the edge in E − associated with cycle c and Ec+ ⊆ E + associated with cycle c.
            Note that the graph G defined by E is very sparse for real problems [37]. Also we refer to an edge
            (vi , vj ) with xvi vj = 1 as a cut edge.
            The objective in Eq. (CC1 ) is to minimize the total weight of the cut edges. The constraints in Eq. (1)
            ensure that, within every cycle of G, the number of cut edges can not be exactly one. This enforces
            the labeling x to decompose G such that cut edges are exactly those edges that straddle distinct
            components. We refer to the constraints in Eq. (1) as cycle inequalities.
            Solving Eq. (CC1 ) is intractable due to the large number of cycle inequalities. Andres et al. [1]
            generates solutions by alternating between solving the ILP over a nascent set of constraints Cˆ
            (initialized empty) and adding new constraints from the set of currently violated cycle inequalities.
            Generating constraints corresponds to iterating over (vi , vj ) ∈ E − and identifying the shortest
            path between the nodes vi , vj in the graph with edges E \ (vi , vj ) and weights equal to x. If the
            ˆ The
            corresponding path has total weight less than xvi vj , the corresponding constraint is added to C.
            LP relaxation of Eq. (CC1 )-(1) can be solved instead of the ILP in each iteration until no violated
            cycle inequalities exist, after which the ILP must be solved in each iteration.
            We should note that earlier work in CC for computer vision did not require that cycle inequalities
            contain exactly one member of E − , which is on the right hand side of Eq. (1). It is established with
            Lemma(1) in Yarkony et al. [38], that the addition of cycle inequalities, that contain edges in E − , E +
            on the left hand side, right hand side of Eq. (1), respectively, do not tighten the ILP in Eq. (CC1 )-(1)
            or its LP relaxation.
            In this section, we reviewed the baseline approach for solving CC in the computer vision community.
            In the subsequent sections, we rely on the characterization of CC in Eq. (CC1 )-(1), though not on the
            specific solver of Andres et al. [1].
            3

            4

            Benders Decomposition for Correlation Clustering

            In this section, we introduce a novel approach to CC using Benders decomposition (referred to as
            BDCC). Our proposed decomposition is defined by a minimal vertex cover on E − with members
            S ⊂ V indexed by vs . Each s ∈ S is associated with a Benders subproblem and vs is referred to as
            the root of that Benders subproblem. Edges in E − are partitioned arbitrarily between the subproblems,
            such that each (vi , vj ) ∈ E − is associated with either the subproblem with root vi or the subproblem
            with root vj . Here, Es− is the subset of E − associated with subproblem s. The subproblem with root
            vs enforces the cycle inequalities Cs , where Cs is the subset of C containing edges in Es− . We use Es+
            to denote the subset of E + adjacent to vs .
            In this section, we assume that we are provided with S, which can be produced greedily or using an
            LP/ILP solver.
            Below, we rewrite Eq. (CC1 ) using an auxiliary function Q(φ, s, x). Here Q(φ, s, x) provides the
            cost to alter x to satisfy all cycle inequalities in Cs , by increasing/decreasing xvi vj for (vi , vj ) in
            E + /Es− , respectively. Below we describe the changes of the master’s problem edge labeling x, which
            is based on the edge labeling of each Benders subproblem xs = (xsvi vj ) ∈ {0, 1}|s| , where |s| is the
            number of edges in the subproblem s.
            X
            X
            X
            (CC1 )
            (CC2 ) :
            min
            −φvi vj (1 − xvi vj ) +
            φ vi vj x vi vj +
            Q(φ, s, x),
            x∈{0,1}|E|

            (vi ,vj )∈E −

            (vi ,vj )∈E +

            s∈S

            (CC2 )
            where Q(φ, s, x) is defined as follows.
            Q(φ, s, x)

            =

            min
            s

            x ∈{0,1}

            s.t.

            X
            |s|

            −φvi vj (1 − xsvi vj ) +

            (vi ,vj )∈Es−

            X

            X

            φvi vj xsvi vj

            (2)

            (vi ,vj )∈E +

            xvi vj + xsvi vj ≥ xvic vjc − (1 − xsvic vjc ) ∀c ∈ Cs .

            (vi ,vj )∈Ec+

            We now construct a solution x∗ = {x∗vi vj , (xs∗
            vi vj )s∈S } for which Eq. (CC2 ) is minimized and all
            cycle inequalities are satisfied. We start from a given solution x = {xvi vj , (xsvi vj )s∈S } and proceed
            as follows.
            M

            x∗vi vj = min(xvi vj , xsvi vj ) ∀(vi , vj ) ∈ Es− , s ∈ S
            M

            x∗vi vj = xvi vj + max xsvi vj
            s∈S

            ∀(vi , vj ) ∈ E + .

            (3)
            (4)

            The right hand side of Eq. (4) cannot exceed 1 at optimality because of the constraint in Eq. (2).
            Given the solution x∗vi vj , the optimizing solution to each Benders subproblem s is denoted xs∗
            vi vj and
            is defined as follows.
            
            1, if (vi , vj ) ∈ Es−
            xs∗
            =
            (5)
            vi vj
            0, otherwise.
            In Sec. A in the supplement, we show that the cost of {x∗vi vj , (xs∗
            vi vj )s∈S } is no greater than that of
            {xvi vj , (xsvi vj )s∈S }, with regard to the objective in Eq. (CC2 ) and that Q(φ, s, x∗ ) = 0 holds for
            all
            s ∈ S.
            It follows that there always exists an optimizing solution x to Eq. (CC2 ) such that Q(φ, s, x) = 0 for
            all s ∈ S.
            Observe, that there exists an optimal partition xs of the nodes of the graph , in Eq. (2), which is
            2-colorable. This is because any partition xs can be altered without increasing its cost, by merging
            connected components that are adjacent to one another, not including the root node vs . Note, that
            merging any pair of such components, does not increase the cost, since those components are not
            separated by negative weight edges in subproblem s and so the result is still a partition.
            Given this observation, we rewrite the optimization Eq. (CC2 ) regarding Q(φ, s, x), using the node
            labeling formulation of min-cut, with the notation below.
            4

            We indicate with mv = 1 that node v ∈ V is not in the component associated with the root of
            subproblem s and mv = 0 otherwise. To avoid extra notation mvs is replaced by 0. Let
            (
            1, for (vi , vj ) ∈ E + , if (vi , vj ) is cut in xs , but is not cut in x
            s
            f vi vj =
            (6)
            1, for (vi , vj ) ∈ Es− , if (vi , vj ) is not cut in xs , but is cut in x.
            Thus, the definition for the first/second case implies a penalty of φvi vj / - φvi vj , which is added to
            Q(φ, s, x). Note moreover that xsvi vj = fvsi vj for all (vi , vj ) ∈ E + and that xsvi vj = 1 − fvsi vj for
            all
            (vi , vj ) ∈ Es− .
            Below we write Q(φ, s, x) as primal/dual LP, with primal constraints associated with dual variables
            ψ, λ, which are noted in the primal. Given binary x, we need only enforce that f, m are non-negative
            to ensure that there exists an optimizing solution for f, m which is binary. This is a consequence of
            the optimization being totally unimodular, given that x is binary. Total unimodularity is a known
            property of the min-cut/max flow LP [14]. The primal subproblem is therefore given by the following.
            X
            X
            Q(φ, s, x) = smin
            φvi vj fvsi vj −
            φvs v fvss v
            (7)
            fv v ≥0
            i j
            (vi ,vj )∈E +
            mv ≥0

            (vs ,v)∈Es−

            λ−
            vi vj

            :

            mvi − mvj ≤ xvi vj + fvsi vj

            ∀(vi , vj ) ∈ (E + \ Es+ ),

            λ+
            vi vj

            :

            mvj − mvi ≤ xvi vj + fvsi vj

            ∀(vi , vj ) ∈ (E + \ Es+ ),

            ψv−

            :

            xvs v − fvss v ≤ mv

            ∀(vs , v) ∈ Es− ,

            ψv+

            :

            mv ≤ xvs v + fvss v

            ∀(vs , v) ∈ Es+ ,

            This yields to the corresponding dual subproblem.
            X
            +
            max −
            (λ−
            vi vj + λvi vj )xvi vj +
            λ≥0
            ψ≥0

            s.t.

            X

            ψv− xvs v −

            (vs ,v)∈Es−

            (vi ,vj )∈(E + \Es+ )

            ψv+i 1Es+ (vs , vi ) − ψv−i 1Es− (vs , vi )+
            X
            +
            (λ−
            vi vj − λvi vj ) +
            vj
            (vi ,vj )∈(E + \Es+ )

            X

            X

            ψv+ xvs v

            (8)

            (vs ,v)∈Es+

            −
            (λ+
            vj vi − λvj vi ) ≥ 0

            ∀vi ∈ V − vs

            vj
            (vj ,vi )∈(E + \Es+ )

            −φvs v − ψv− ≥ 0
            φvs v − ψv+ ≥ 0
            +
            φvi vj − (λ−
            vi v j + λ vi vj ) ≥ 0

            ∀(vs , v) ∈ Es−
            ∀(vs , v) ∈ Es+
            ∀(vi , vj ) ∈ (E + \ Es+ ).

            In Eq. (8) and subsequently 1Λ (x) denotes the binary indicator function for some set Λ, which returns
            one if (x ∈ Λ) and zero otherwise. We now consider the constraint that Q(φ, s, x) = 0. Note that
            any dual feasible solution for the dual problem (8) describes an affine function of x, which is a tight
            lower bound on Q(φ, s, x). We compact the terms λ, ψ into ω z , where ωvzi vj is associated with the
            xvi vj term.
            
            +
            −(λ−
            if (vi , vj ) ∈ (E + \ Es+ )
            
            vi vj + λvi vj ),
            
            
            
            
            −ψv+j ,
            if (vi , vj ) ∈ Es+
            ωvzi vj =
            
            ψv−j ,
            if (vi , vj ) ∈ Es−
            
            
            
            
            0,
            if (vi , vj ) ∈ (E − \ Es− ).
            We denote the set of all dual feasible solutions across s P
            ∈ S as Z, with z ∈ Z. Observe, that to
            enforce that Q(φ, s, x) = 0, it is sufficient to require that (vi ,vj )∈E xvi vj ωvzi vj ≤ 0, for all z ∈ Z.
            We formulate CC as optimization using Z below.
            X
            X
            (CC2 )
            (CC3 ) = min
            φ vi vj x vi vj −
            (1 − xvi vj )φvi vj
            (CC3 )
            x∈{0,1}|E|

            s.t.

            X

            (vi ,vj )∈E −

            (vi ,vj )∈E +

            xvi vj ωvzi vj ≤ 0

            ∀z ∈ Z

            (vi ,vj )∈E

            5

            Algorithm 1 Benders Decomposition for CC (BDCC)
            1: Ẑ = {}
            2: done_LP = False
            3: repeat
            4:
            x = Solve Eq. (CC3 ) over Ẑ enforcing integrality if and only if done_LP=True
            5:
            did_add = False
            6:
            for s ∈ S do
            7:
            if ∃(vi , vj ) ∈ Es− s.t. d(vi , vj )
            < xvi vj then
            8:
            z1 = Get Benders row via Eq (8).
            9:
            z2 = Get MWR via Sec. 5.
            10:
            Ẑ = Ẑ ∪ z1 ∪ z2
            11:
            did_add = True
            12:
            end if
            13:
            end for
            14:
            if did_add=False then
            15:
            done_LP = True
            16:
            end if
            17: until did_add=False AND xvi vj ∈ {0, 1} ∀(vi , vj ) ∈ E
            18: Return x

            4.1

            Cutting Plane Optimization

            Optimization in Eq. (CC3 ) is intractable since |Z| equals the number of dual feasible solutions across
            subproblems, which is infinite. Since we cannot consider the entire set Z, we use a cutting plane
            approach to construct a set Ẑ ⊂ Z, that is sufficient to solve Eq. (CC3 ) exactly. We initialize Ẑ as
            the empty set. We iterate between solving the LP relaxation of Eq. (CC3 ) over Ẑ (referred to as the
            master problem) and generating new Benders rows until no violated constraints exist.
            This ensures that no violated cycle inequalities exist but may not ensure that x is integral. To enforce
            integrality, we iterate between solving the ILP in Eq. (CC3 ) over Ẑ and adding Benders rows to Ẑ.
            By solving the LP relaxation first, we avoid unnecessary and expensive calls to the ILP solver.
            To generate Benders rows given x, we iterate over S and generate one Benders row using Eq. (8), if s
            is associated with a violated cycle inequality, which we determine as follows. Given s, x we iterate
            over (vi , vj ) ∈ Es− . We find the shortest path from vi to vj on graph G with edges E, with weights
            equal to the vector x. If the length of this path, denoted as d(vi , vj ), is strictly less than xvi vj ,
            then
            we have identified a violated cycle inequality associated with s.
            We describe our cutting plane approach in Alg. 1, with line by line description in Sec. B in the
            supplementary material. To accelerate optimization, we add MWR in addition to standard Benders
            rows, which we describe in the following Sec. 5.
            Prior to termination of Alg. 1, one can produce a feasible integer solution x∗ from any solution x,
            1
            provided by the master problem, as follows. First, for each (vi , vj ) ∈ E, set x∗∗
            vi vj = 1, if xvi vj > 2
            ∗
            and otherwise set x∗∗
            vi vj = 0. Second, for each (vi , vj ) ∈ E, set xvi vj = 1, if vi , vj are in separate
            ∗∗
            connected components of the solution described by x and otherwise set x∗vi vj = 0. The cost of the
            feasible integer solution x∗ provides an upper bound on the cost of the optimal solution. In Sec. C
            (supplementary material), we provide a more involved approach to produce feasible integer solutions.
            In this section, we characterized CC using Benders decomposition and provided a cutting plane
            algorithm to solve the corresponding optimization.

            5

            Magnanti-Wong Benders Rows

            We accelerate Benders decomposition (see Sec. 4) using the classic operations research technique of
            Magnanti-Wong Benders Rows (MWR) [23]. The Benders row, given in Eq. (8), provides a tight
            bound at x∗ , where x∗ is the master problem solution used to generate the Benders row. However,
            ideally, we want our Benders row to provide good lower bounds for a large set of x different from x∗ ,
            6

            Figure 1: Left: We plot the gap between the upper and lower bounds as a function of time for various
            values of τ on selected problem instances. We use red,green,blue for τ = [0.5, 0.99, .01] respectively,
            and black for not using Magnanti-Wong rows. We show both the computation time with and without
            exploiting parallelization of subproblems with dotted and solid lines, respectively. We use titles to
            indicate the approximate difficulty of the problem as ranked by input file size of 100 files.
            Right: We compare the benefits of parallelization and MWR across our data set. We scatter plot the
            total running time versus the total running time when solving each subproblem is done on its own
            CPU across problem instances. We use red to indicate τ = 0.5 and black to indicate that MWR are
            not used. We draw a line with slope=1 in magenta to better enable appreciation of the red and black
            points. NOTE: The time spent generating Benders rows, in a given iteration of BDCC when using
            parallel processing, is the maximum time spent to solve any sub-problem for that iteration.
            while being tight (or perhaps very active) at x∗ . To achieve this, we use a modified version of Eq. (8),
            where we replace the objective and add one additional constraint.
            We follow the tradition of the operations research literature and use a random negative valued vector
            (with unit norm) in place of the objective Eq. (8). This random vector is unique each time a Benders
            −1
            subproblem is solved. We experimented with using as an objective .0001+|φ
            , which encourages
            vi vj |
            the cutting of edges with large positive weight, but it works as well as the random negative objective.
            Here .0001 is a tiny positive number. It prevents the terms in the objective from becoming infinite.
            Below, we enforce the new Benders row to be active at x∗ , by requiring that the dual cost is within a
            tolerance τ ∈ (0, 1) of the optimum w.r.t. the objective in Eq. (8).
            X
            X
            X
            +
            τ Q(φ, s, x) ≤ −
            (λ−
            ψv− xvs v −
            ψv+ xvs v
            vi vj + λvi vj )xvi vj +
            (vs ,v)∈Es−

            (vi ,vj )∈(E + \Es+ )

            (vs ,v)∈Es+

            (9)
            Here, τ = 1 requires optimality w.r.t. the objective in Eq. (8), while τ = 0 ignores optimality. In our
            experiments, we found that τ = 12 provides strong performance.

            6

            Experiments: Image Segmentation

            In this section, we demonstrate the value of our algorithm BDCC on CC problem instances for image
            segmentation on the benchmark Berkeley Segmentation Data Set (BSDS) [24]. Our experiments
            demonstrate the following three findings. (1) BDCC solves CC instances for image segmentation; (2) BDCC
            successfully exploits parallelization; (3) the use of MWR dramatically accelerates
            optimization.
            To benchmark performance, we employ cost terms provided by the OPENGM2 dataset [4] for BSDS.
            This allows for a direct comparison of our results to the ones from Andres et al. [1]. We use the
            random unit norm negative valued objective when generating MWR. We use CPLEX to solve all
            linear and integer linear programming problems considered during the course of optimization. We use
            a maximum total CPU time of 600 seconds, for each problem instance (regardless of parallelization).
            7

            Table 1: We show the percentage of problems solved that have a duality gap of up to tolerance ,
            within a certain amount of time (10,50,100,300) seconds, with and without MWR/parallelization.
            We use par =1 to indicate the use of parallelization and par=0 otherwise. Here τ = 0 means that no
            MWR are generated.
            =0.1

            =1

            =10

            τ

            par

            10

            50

            100

            300

            0.5
            0
            0.5
            0

            0
            0
            1
            1

            0.149
            0.0106
            0.266
            0.0426

            0.372
            0.0532
            0.777
            0.0745

            0.585
            0.0745
            0.904
            0.0745

            0.894
            0.106
            0.968
            0.138

            τ

            par

            10

            50

            100

            300

            0.5
            0
            0.5
            0

            0
            0
            1
            1

            0.149
            0.0106
            0.319
            0.0532

            0.394
            0.0638
            0.819
            0.0745

            0.606
            0.0745
            0.947
            0.106

            0.904
            0.16
            0.979
            0.17

            τ

            par

            10

            50

            100

            300

            0.5
            0
            0.5
            0

            0
            0
            1
            1

            0.202
            0.0532
            0.447
            0.0638

            0.426
            0.0957
            0.936
            0.128

            0.628
            0.128
            0.979
            0.181

            0.915
            0.223
            0.989
            0.287

            We formulate the selection of S, as a minimum vertex cover problem, where for every edge (vi , vj ) ∈
            E − , at least one of vi , vj is in S. We solve for the minimum vertex cover exactly as an ILP. Given S,
            we assign edges in E − to a connected selected node in S arbitrarily. We found experimentally that
            solving for the minimum vertex cover consumed negligible CPU time for our dataset. We attribute
            this fact to the structure of our problem domain, since the minimum vertex cover is an NP-hard
            problem. For problem instances where solving for the minimum vertex cover exactly is difficult, the
            minimum vertex cover problem can be solved approximately or greedily.
            In Fig. 1 (left) we demonstrate the effectiveness of BDCC with various τ for different problem
            difficulties. We observe that the presence of MWR dramatically accelerates optimization. However,
            the exact value of τ does not effect the speed of optimization dramatically. We show performance
            with and without relying on parallel processing. Our parallel processing times assume that we
            have one CPU for each subproblem. For the problem instances in our application the number of
            subproblems is under one thousand, each of which are very easy to solve. The parallel and nonparallel time
            comparisons share only the time to solve the master problem. We observe large benefits
            of parallelization for all settings of τ . However, when MWR are not used, we observe diminished
            improvement, since the master problem consumes a larger proportion of total CPU time.
            In Fig. 1(right), we demonstrate the speed up induced by the use of parallelization. For most problem
            instances, the total CPU time required when using no MWR was prohibitively large, which is not the
            case when MWR are employed. Thus most problem instances solved without MWR terminated early.
            In Tab. 1, we consider the convergence of the bounds for τ = {0, 12 }; ( τ = 0 means that no MWR
            are generated). We consider a set of tolerances on convergence regarding the duality gap, which is
            the difference between the anytime solution (upper bound) and the lower bound on the objective. For
            each such tolerance , we compute the percentage of instances, for which the duality gap is less than
            , after various amounts of time. We observe that the performance of optimization without MWR,
            but exploiting parallelization performs worse than using MWR, but without paralleliziation. This
            demonstrates that, across the dataset, MWR are of greater importance than parallelization.
        </corps>
        <conclusion>
            We present a novel methodology for finding optimal correlation clustering in arbitrary graphs. Our
            method exploits the Benders decomposition to avoid the enumeration of a large number of cycle
            inequalities. This offers a new technique in the toolkit of linear programming relaxations, that we
            expect will find further use in the application of combinatorial optimization to problems in computer
            vision.
            The exploitation of results from the domain of operations research may lead to improved variants
            of BDCC. For example, one can intelligently select the subproblems to solve instead of solving all
            subproblems in each iteration. This strategy is referred to as partial pricing in the operations research
            literature. Similarly one can devote a minimum amount of time in each iteration to solve the master
            problem so as to enforce integrality on a subset of the variables of the master problem.
        </conclusion>
        <discussion>
            Aucune discussion trouvée.
        </discussion>
        <biblio>
            [1] B. Andres, J. H. Kappes, T. Beier, U. Kothe, and F. A. Hamprecht. Probabilistic image segmentation
            with closedness constraints. In Proceedings of the Fifth International Conference on Computer Vision
            (ICCV-11), pages 2611–2618, 2011.
            [2] B. Andres, T. Kroger, K. L. Briggman, W. Denk, N. Korogod, G. Knott, U. Kothe, and F. A. Hamprecht.
            Globally optimal closed-surface segmentation for connectomics. In Proceedings of the Twelveth
            International Conference on Computer Vision (ECCV-12), 2012.
            [3] B. Andres, J. Yarkony, B. S. Manjunath, S. Kirchhoff, E. Turetken, C. Fowlkes, and H. Pfister.
            Segmenting
            planar superpixel adjacency graphs w.r.t. non-planar superpixel affinity graphs. In Proceedings of the Ninth
            Conference on Energy Minimization in Computer Vision and Pattern Recognition (EMMCVPR-13), 2013.
            [4] B. Andres, T. Beier, and J. H. Kappes. Opengm2, 2014.
            [5] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Journal of Machine Learning, pages
            238–247, 2002.
            [6] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance. Branch-and-price:
            Column generation for solving huge integer programs. Operations Research, 46:316–329, 1996.
            [7] T. Beier, T. Kroeger, J. H. Kappes, U. Kothe, and F. A. Hamprecht. Cut, glue, & cut: A fast, approximate
            solver for multicut partitioning. In CVPR, 2014.
            [8] T. Beier, F. A. Hamprecht, and J. H. Kappes. Fusion moves for correlation clustering. In CVPR, 2015.
            [9] T. Beier, B. Andres, K. Ullrich, and F. A. Hamprecht. An efficient fusion move algorithm for the
            minimum cost lifted multicut problem. volume LNCS 9906, pages 715–730. Springer, 2016. doi:
            10.1007/978-3-319-46475-6_44.
            [10] J. F. Benders. Partitioning procedures for solving mixed-variables programming problems. Numerische
            mathematik, 4(1):238–252, 1962.
            [11] J. R. Birge. Decomposition and partitioning methods for multistage stochastic linear programs.
            Operations
            research, 33(5):989–1007, 1985.
            [12] J.-F. Cordeau, G. Stojković, F. Soumis, and J. Desrosiers. Benders decomposition for simultaneous
            aircraft
            routing and crew scheduling. Transportation science, 35(4):375–388, 2001.
            [13] M. E. Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):
            1776–1781, 1966.
            [14] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian journal of Mathematics, 8(3):
            399–404, 1956.
            [15] A. M. Geoffrion and G. W. Graves. Multicommodity distribution system design by benders decomposition.
            Management science, 20(5):822–844, 1974.
            [16] P. Gilmore and R. Gomory. A linear programming approach to the cutting-stock problem. Operations
            Research (volume 9), 1961.
            [17] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and B. Schiele. Deepercut: A deeper, stronger,
            and faster multi-person pose estimation model. In European Conference on Computer Vision, pages 34–50.
            Springer, 2016.
            [18] A. Kardoost and M. Keuper. Solving minimum cost lifted multicut problems by node agglomeration. In
            ACCV 2018, 14th Asian Conference on Computer Vision, Perth, Australia, 2018.
            [19] M. Keuper, B. Andres, and T. Brox. Motion trajectory segmentation via minimum cost multicuts. In ICCV,
            2015.

            9

            [20] M. Keuper, E. Levinkov, N. Bonneel, G. Lavoué, T. Brox, and B. Andres. Efficient decomposition of
            image and mesh graphs by lifted multicuts. In ICCV, 2015.
            [21] S. Kim, S. Nowozin, P. Kohli, and C. D. Yoo. Higher-order correlation clustering for image
            segmentation.
            In Advances in Neural Information Processing Systems,25, pages 1530–1538, 2011.
            [22] V. Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm.
            Mathematical Programming Computation, 1(1):43–67, 2009.
            [23] T. L. Magnanti and R. T. Wong. Accelerating benders decomposition: Algorithmic enhancement and
            model selection criteria. Operations research, 29(3):464–484, 1981.
            [24] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its
            application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings of
            the Eighth International Conference on Computer Vision (ICCV-01), pages 416–423, 2001.
            [25] S. Nowozin and S. Jegelka. Solution stability in linear programming relaxations: Graph partitioning and
            unsupervised learning. In Proceedings of the 26th Annual International Conference on Machine Learning,
            pages 769–776. ACM, 2009.
            [26] X. Pan, D. Papailiopoulos, S. Oymak, B. Recht, K. Ramchandran, and M. I. Jordan. Parallel correlation
            clustering on big graphs. In Proceedings of the 28th International Conference on Neural Information
            Processing Systems - Volume 1, NIPS’15, pages 82–90, Cambridge, MA, USA, 2015. MIT Press. URL
            http://dl.acm.org/citation.cfm?id=2969239.2969249.
            [27] L. Pishchulin, E. Insafutdinov, S. Tang, B. Andres, M. Andriluka, P. V. Gehler, and B. Schiele.
            Deepcut:
            Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
            on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.
            [28] C. Rother, V. Kolmogorov, V. Lempitsky, and M. Szummer. Optimizing binary mrfs via extended roof
            duality. In Computer Vision and Pattern Recognition, 2007. CVPR ’07. IEEE Conference on, pages 1–8,
            june 2007.
            [29] W.-K. Shih, S. Wu, and Y. Kuo. Unifying maximum cut and minimum cut of a planar graph. Computers,
            IEEE Transactions on, 39(5):694–697, May 1990.
            [30] P. Swoboda and B. Andres. A message passing algorithm for the minimum cost multicut problem. In
            CVPR, 2017.
            [31] S. Tang, B. Andres, M. Andriluka, and B. Schiele. Subgraph decomposition for multi-target tracking. In
            CVPR, 2015.
            [32] S. Wang, K. Kording, and J. Yarkony. Exploiting skeletal structure in computer vision annotation with
            benders decomposition. arXiv preprint arXiv:1709.04411, 2017.
            [33] S. Wang, A. Ihler, K. Kording, and J. Yarkony. Accelerating dynamic programs via nested benders
            decomposition with application to multi-person pose estimation. In Proceedings of the European Conference on
            Computer Vision (ECCV), pages 652–666, 2018.
            [34] J. Yarkony. Next generation multicuts for semi-planar graphs. In Proceedings of the Neural Information
            Processing Systems Optimization in Machine Learning Workshop (OPT-ML), 2015.
            [35] J. Yarkony and C. Fowlkes. Planar ultrametrics for image segmentation. In Neural Information Processing
            Systems, 2015.
            [36] J. Yarkony and S. Wang. Accelerating message passing for map with benders decomposition. arXiv
            preprint arXiv:1805.04958, 2018.
            [37] J. Yarkony, A. Ihler, and C. Fowlkes. Fast planar correlation clustering for image segmentation. In
            Proceedings of the 12th European Conference on Computer Vision(ECCV 2012), 2012.
            [38] J. Yarkony, T. Beier, P. Baldi, and F. A. Hamprecht. Parallel multicut segmentation via dual
            decomposition.
            In International Workshop on New Frontiers in Mining Complex Patterns, pages 56–68. Springer, 2014.
            [39] C. Zhang, F. Huber, M. Knop, and F. Hamprecht. Yeast cell detection and segmentation in bright field
            microscopy. In ISBI, 2014.

            10

            A

            APPENDIX: Q(φ, s, x∗ ) = 0 at Optimality

            In this section, we demonstrate that there exists an x∗ , that minimizes Eq. (CC2 ), for which Q(φ, s, x∗ )
            = 0.
            Given an arbitrary solution {xvi vj , (xsvi vj )s∈S } another solution {x∗vi vj , (xs∗
            vi vj )s∈S } is constructed, for which
            Q(φ, s, x∗ ) = 0 holds, without increasing the objective in Eq. (CC2 ). We write the updates below in terms
            of
            xs .
            M

            x∗vi vj = xvi vj + max xsvi vj
            s∈S

            ∀(vi , vj ) ∈ E +

            M

            x∗vi vj = xvi vj + xsvi vj − 1 ∀(vi , vj ) ∈ Es− , s ∈ S
            M

            ∀(vi , vj ) ∈ E +

            M

            ∀(vi , vj ) ∈ Es− , s ∈ S.

            xs∗
            vi vj = 0
            xs∗
            vi vj = 1

            (10)

            The updates in Eq. (10) are equivalent to the following updates using f s ,f s∗ . Here f s , f s∗ correspond
            to the
            optimizing solution for f in subproblem s, given x, x∗ respectively.
            x∗vi vj = xvi vj + max fvsi vj
            s∈S

            x∗vi vj = xvi vj − fvsi vj

            ∀(vi , vj ) ∈ E +

            ∀(vi , vj ) ∈ Es− , s ∈ S

            fvs∗
            = 0 ∀(vi , vj ) ∈ E +
            i vj

            (11)

            fvs∗
            = 0 ∀(vi , vj ) ∈ Es−
            i vj
            These updates in Eq. (10) and Eq. (11) preserve the feasibility of the primal LP in Eq. (7). Also notice,
            that
            since f s∗ is a zero valued vector for all s ∈ S, then Q(φ, s, x∗ ) = 0 for all s ∈ S.
            We now consider, the total change in Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E + , induced by Eq.
            (10),
            which is non-positive. The objective of the master problem increases by φvi vj maxs∈S xsvi vj , while the
            total
            P
            decrease in the objectives of the subproblems is φvi vj s∈S xsvi vj . Since the latter value is greater than
            the
            former value, the total change in problem (CC2 ) decreases more than it increases. Considering on the other
            hand
            the total change of Eq. (CC2 ) corresponding to edge (vi , vj ) ∈ E − , induced by Eq. (10), which is zero,
            yields
            in an increase of the objective of the master problem by −φvi vj (1 − xn
            vi vj ), while the objective of subproblem
            s decreases by −φvi vj (1 − xsvi vj ). This shows that the objective of Eq. (CC2 ) is minimized for x∗ .

            B

            Line by Line Description of BDCC

            We provide the line by line description of Alg. 1.
            • Line 1: Initialize the nascent set of Benders rows Ẑ to the empty set.
            • Line 2: Indicate that we have not solved the LP relaxation yet.
            • Line 3-17: Alternate between solving the master problem and generating Benders rows, until a feasible
            integral solution is produced.
            1. Line 4: Solve the master problem providing a solution x, which may not satisfy all cycle
            inequalities. We enforce integrality if we have finished solving the LP relaxation, which is
            indicated by done_lp=True.
            2. Line 5: Indicate that we have not yet added any Benders rows to this iteration.
            3. Line 6-13: Add Benders rows by iterating over subproblems and adding Benders rows corresponding to
            subproblems, associated with violated cycle inequalities.
            – Line 7: Check if there exists a violated cycle inequality associated with Es− . This is done
            by iterating over (vi , vj ) ∈ Es− and checking if the shortest path from vi to vj is less than
            xvi vj . This distance is defined on the graph’s edges E with weights equal to x.
            – Lines 8-10: Generate Benders rows associated with subproblem s and add them to nascent
            set Ẑ.
            – Line 11: Indicate that a Benders row was added this iteration.
            4. Lines 14-16: If no Benders rows were added to this iteration, we enforce integrality on x, when
            solving the master problem for the remainder of the algorithm.
            • Line 18 Return solution x.

            11

            C

            Generating Feasible Integer Solutions Prior to Convergence

            Prior to the termination of optimization, it is valuable to provide feasible integer solutions on demand.
            This is so
            that a practitioner can terminate optimization, when the gap between the objectives of the integral solution
            and
            the relaxation is small. In this section we consider the production of feasible integer solutions, given the
            current
            solution x∗ to the master problem, which may neither obey cycle inequalities or be integral. We refer to
            this
            procedure as rounding.
            Rounding is a coordinate descent approach defined on the graph G and its edges E with weights κ, determined
            using x∗ below.
            κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
            κvi vj = φvi vj x∗vi vj

            ∀(vi , vj ) ∈ E

            (12)

            −

            ∗

            Consider that x is integral and feasible (where feasibility indicates that x∗ satisfies all cycle
            inequalities). Let
            xs∗ define the boundaries in partition x∗ , of the connected component containing s. Here xs∗
            vi vj = 1 if exactly
            one of vi , vj is in the connected component containing s under cut x∗ . Observe, that Q(κ, s, x0s ) = 0,
            where
            s∗
            x0s
            as the solution to Eq. (7). Thus xs∗ is the minimizer of Eq. (7).
            vi vj = 1Es− (vi , vj ), is achieved using x
            s∗
            The union of the edges cut in x across s ∈ S is identical to x∗ . Note that when x∗ is integral and feasible
            then the solution produced below has cost equal to that of x∗ .
            M

            xs∗ = minimizer of Q(κ, s, x0s ) ∀s ∈ S
            M

            s∗
            x+
            vi vj = max xvi vj
            s∈S

            M

            s∗
            x+
            vi vj = xvi vj

            ∀(vi , vj ) ∈ E +

            (13)

            ∀(vi , vj ) ∈ Es− , s ∈ S

            The procedure of Eq. (13) can be used regardless of whether x∗ is integral or feasible. Note that if x∗ is
            close to
            integral and close to feasible, then Eq. (13) is biased to produce a solution that is similar to x∗ by
            design of κ.
            We now consider a serial version of Eq. (13), which may provide improved results. We construct a partition
            x+
            by iterating over s ∈ S, producing component partitions as in Eq. (13). We alter κ by allowing for the
            cutting of
            edges previously cut with cost zero. We formally describe this serial rounding procedure below in Alg. 2.

            Algorithm 2 Generating an Integral and Feasible Solution Given Infeasible and or Non-Integral Input
            x∗ )
            1: x+
            vi vj = 0 ∀(vi , vj ) ∈ E
            2: κvi vj = φvi vj x∗vi vj ∀(vi , vj ) ∈ E −
            3: κvi vj = φvi vj (1 − x∗vi vj ) ∀(vi , vj ) ∈ E +
            4: for s ∈ S do
            5:
            xs = minimizer for Q(κ, s, x0s ) given fixed κ, s.
            +
            s
            6:
            x+
            vi vj = max(xvi vj , xvi vj ) ∀(vi , vj ) ∈ E
            +
            7:
            κvi vj = κvi vj (1 − xvi vj ) ∀(vi , vj ) ∈ E
            8: end for
            9: Return x+
            • Line 1: Initialize x+ as the zero vector.
            • Line 2-3: Set κ according to Eq. (12)
            • Line 4-8: Iterate over s ∈ S to construct x+ by cutting edges cut in the subproblem.
            1. Line 5: Produce the lowest cost cut xs given altered edge weights κ for subproblem s.
            2. Line 6: Cut edges in x+ that are cut in xs .
            3. Line 7: Set φvi vj to zero for cut edges in x+ .
            • Line 9: Return the solution x+
            When solving for the fast minimizer of Q(κ, s, x0n ), we rely on the network flow solver of Rother et al.
            [28],
            though we do not exploit its capacity to tackle non-submodular problems.
        </biblio>
    </article>
    <article>
        <preamble>A_memetic_algorithm_for_community_detectionin_signed_networks.pdf</preamble>
        <titre>A Memetic Algorithm for Community Detection in Signed Networks</titre>
        <auteur>
            Corresponding author: Wu Yang (e-mail: yangwu@hrbeu.edu.cn).
        </auteur>
        <abstract>
            ABSTRACT Community discovery (i.e. community detection) in signed networks is a division of nodes,
            such that the edges in the communities are positive and the edges between the communities are negative.
            Davis and Harary have solved the problem of community detection when a signed graph is balanced or
            weakly balanced. When the signed network is unbalanced, community detection becomes very complex. In
            this paper, we propose a novel memetic algorithm (MA) called MACD-SN for community partition (i.e.
            community detection) in signed networks. Firstly, we present a novel initialization algorithm used in
            initialization of MACD-SN. This method can accelerate the convergence rate of MACD-SN algorithm.
            Next, in addition to using frequently-used variation operation (in this paper, variation and mutation are
            interchangeable), this paper presents a novel crossover operation and a novel variation operation, which
            contributes to increasing the correctness of the MACD-SN algorithm's operation result and reduces its
            running time. Lastly, this paper proposes a new local search algorithm, which may enable the algorithm's
            result to jump away the local best result with a certain probability and draw near the global best result
            quickly. For testing the performance of MACD-SN algorithm, we have done many experiments using five
            kinds of synthetic signed networks and five real-world signed networks. The test outcomes show that the
            proposed algorithm is valid and efficient for signed network cluster partition (i.e. community detection).
            INDEX TERMS genetic algorithm, social network, signed network, community detection
        </abstract>
        <introduction>
            Modern network science is an active field in understanding
            complex systems. Actually, a lot of complicated systems in
            various fields can be expressed by means of networks, for
            example, complex collaborative relationships [1], social
            systems [2], information systems [3], etc. In these networks,
            nodes (or vertices) represent individual participants, and
            edges (or links) represent relationships between participants.
            A great deal of research efforts have been done on complex
            networks, such as correlation clustering, dynamic network
            evolution. Generally speaking, identifying community
            partition is an important task in complex network analysis.
            Community structures exist in a lot of network systems,
            such as politics, economics, engineering, computer science,
            biology and so on. A comment on network community
            discovery can be found in Ref. [4]. The purpose of
            community discovery is to identify clusters with dense links
            within clusters and only sparse links between clusters [5].
            In theoretical research and practical activities, community
            discovery is of great significance. For instance, in the

            purchasing relations network between customers and online
            retailers’ products (such as www.taobao.com), identifying
            clusters of customers with similar interests can establish an
            effective recommendation system [4, 6].
            In human society, many relationships between people are
            signed, either positive or negative. Compared with traditional
            networks, the positive and negative edges of signed networks
            can more accurately describe cooperation (friendship/trust)
            relations and competition (hostility/distrust) relations. When
            two people have a relationship of trust, respect or love, the
            relationship can be regarded as a positive connection. But,
            the relationship with mistrust, disrespect or hatred can be
            considered as a negative connection. This network is called
            signed network [7], that is, the edge weight is greater than 0,
            indicating a positive relationship; the edge weight is less than
            0, indicating a negative relationship; and the edge weight is
            equal to 0, indicating that there is no relationship between
            these two individuals. Figure 1 shows a simple signed
            network. In the figure, the solid line edge represents a
            positive relationship, and the dotted line edge represents a

            VOLUME XX, 2017
        </introduction>
        <corps>
            1

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            negative relationship. Community discovery in signed
            networks is quite different from that in unsigned networks
            (that is, networks only contain positive connections). In
            unsigned networks, the community structure is defined as a
            group of nodes or vertices which have dense connections
            within groups and sparse connections between groups.
            Whereas for signed social networks, communities (i.e.
            clusters) are defined not only by the density of connections
            but also by the signs of connections. The connections should
            be densely positive and sparsely negative in a community
            while densely negative and sparsely positive between
            communities. Many negative connections exist in the
            communities and many positive connections exist between
            the communities make community detection more difficult.
            A strongly (or weakly) balanced signed network can be
            divided into two (or more) clusters, so that all connections
            within clusters are positive, and all connections between
            clusters are negative [8-10]. However, due to the existence of
            negative connections in clusters and positive connections
            between clusters, the real world signed networks are often
            unbalanced. Therefore, it is a great challenge to design an
            effective and efficient algorithm to discover the community
            structure in signed graphs.

            FIGURE 1. A schematic diagram of a simple signed graph.

            Unlike previous work, this paper proposes a new memetic
            algorithm to detect community structure in signed networks.
            In order to accelerate the convergence of algorithm (decrease
            the numbers of loop), a novel population initialization
            method of memetic algorithm is presented. Besides
            employing the frequently-used variation operation, a novel
            crossover operation (called randomized two-way crossover
            operation in this article) and a novel variation operation
            (called community variation operation in this article) are also
            proposed, which are capable of enhancing the accuracy of the
            result of the algorithm and speeding up the convergence
            speed of population. Randomized two-way crossover
            operation can preferable retain the hereditary properties of
            previous generation individuals, and community variation
            operation is capable of enhancing greatly the chromosomes
            set (i.e. chromosomes population) multiformity of MACDSN algorithm. Furthermore, this paper also presents a
            local
            solution space search subroutine to drive the optimal result of
            the offspring individuals of the MACD-SN method approach
            the global optimal result more quickly in the search region.
            This subroutine is capable of driving the MACD-SN jump
            away local best solution and attain global best solution with a
            specified odds. Many experimental results show that the

            proposed algorithm is effective and efficient for signed
            network community partition.
            The remaining sections of this thesis are arranged as
            follows: The work related to this study is illuminated in
            Section II. A few key concepts and background knowledge
            connected with this study are introduced in Section III.
            Section IV describes presented MACD-SN algorithm for
            community identification (i.e. community detection) in the
            signed network in detail. This section introduces the
            proposed MACD-SN algorithm's chromosome coding
            method, initialization algorithm of chromosomes set,
            computational formula of fitness used, tournament selection
            operator for chromosomes selection, crossover operator and
            mutation operators of genetic operation, local search function,
            etc. Section V shows the test results on synthetic and real
            signed networks. Section VI summarizes the whole paper.
            II.

            RELATED WORKS

            In recent decades, due to the emergence of a large number
            of community partition problems, scholars have proposed
            many algorithms to settle the community partition problems.
            Girvan and Newman presented a dividing method, which is
            called GN algorithm. In addition, Newman also put forward
            a method called FN based on GN algorithm, which uses
            modularity function. It is a kind of agglomeration algorithm
            [11]. In the FN algorithm, each node in the graph is initially
            located in a community with only one node. Afterwards, at
            every stage, the method continuously consolidates cluster
            pair with the largest modularity function increment.
            According to majorization of modularity function, Moore et
            al. [12] proposed a method named CNM to detect
            community structures in complex networks. In comparison
            to the FN method, CNM method can save computing time
            and is appropriate for discovering cluster partition in large
            scale graphs. In [13], Newman put forward a spectral
            method as well. The algorithm used a modularity matrix.
            In recent years, researchers have proposed many multiobjective majorization algorithms to solve community
            discovery problems. The multi-objective majorization
            algorithm finds the optimal solution of the task to be solved
            by majorizing multiple majorization functions at the same
            time. These majorization functions evaluate the discovered
            cluster structure from multiple viewpoints. In the literatures,
            several frequently cited multi-objective majorization
            algorithms for solving community discovery problems are
            listed as follows. Shi et al. [14] put forward a multi objective
            evolutionary algorithm called MOCD. In [15], the authors
            presented an algorithm named MOEA/D-Net, which is also a
            multi-objective evolutionary algorithm (MOEA). Liu et al.
            [69] proposed a COMpression based Multi-Objective
            Evolutionary Algorithm with Decomposition (ComMOEA/D) for community detection. In the prevenient
            literatures, there are also two other multi-objective
            evolutionary algorithms, which are called MODTLBO/D [16]
            and MODBSA/D [17], respectively. The author of [18]

            2

            VOLUME XX, 2017

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            developed a multi objective genetic algorithm called MOGANet.
            Although there are many excellent algorithms that can be
            used to detect communities in the unsigned network, these
            algorithms can not be directly applied to the signed network
            because of the existence of edge signs in the signed network.
            Hence, a large number of scholars have proposed a lot of
            community detection algorithms for signed networks after
            considering the edge signs of signed networks.
            Albayrak et al. [19] apply the spectral algorithm to the
            signed network, and present a spectral method on the basis of
            the signed Laplacian. They concluded that using signed
            Laplacian kernel to divide signed networks into two clusters
            is similar to the ratio cut in unsigned networks. Based on the
            spectral Laplacian, Tewari et al. [20] propose a community
            detection algorithm for signed networks, and defines the
            social imbalance (MOIs) on the basis of the l-cycles in signed
            networks. However, the spectral method is very time
            consuming.
            Arenas et al. [21] modified the modularity definition of the
            unsigned network and extended it to the signed network.
            Bruggeman et al. [22] modified an existing Potts model to
            include negative links, resulting in a method similar to signed
            graphs clustering (i.e. community detection). The author of
            [23] proposed a statistical probability model to identify the
            community partition of signed networks. Moura et al. [24]
            put forward a mixed integer programming model for
            clustering problems related to structural balance. Ismail et al.
            [25] presented a high-efficiency two stage algorithm to
            identify the community structures in signed social networks.
            The objective functions they used are to minimize frustration
            and maximize modularity. Dhillon et al. [26] presented a
            scalable and efficient clustering algorithm using balance
            normalized cut and a multilevel clustering algorithm. Pizzuti
            et al. [27] obtained the community structure of the signed
            network by maximizing the cluster modularity and
            minimizing the number of negative edges within
            communities and the number of positive edges between
            communities. Liu et al. [28] presented two novel
            evolutionary algorithms and conducted a large number of
            experiments to compare them. The experimental results show
            the effectiveness and efficiency of the two algorithms.
            Cheung et al. [29] presented a random walk algorithm for
            community detection of signed networks. Firstly, select a
            node (i.e. vertex) in the network that has not yet assigned a
            community label, and then use the node as the starting node
            to perform a specific number of random walks to determine
            the set of nodes it can reach along the edge path. Next, the
            authors propose a function to determine which vertices are in
            the same community as the starting vertex. At the same time,
            the function considers the distribution of positive connections
            and negative connections between the community and the
            rest of the network. From [29], it can see that the result
            matrix has block characteristics. By using the above function

            to segment the matrix, the matrix will be split into different
            block matrices representing different communities.
            Du et al. [30] proposed a multi objective discrete particle
            swarm optimization algorithm for multi-resolution signed
            network clustering. Firstly, the algorithm generates initial
            population information including location, speed, individual
            optimal solution and neighborhood information. Next, the
            new velocity and position of each particle are calculated, and
            a small disturbance is added to the new position; then, the
            disturbed position is evaluated, and the neighborhood
            information and optimal solution of the particle are updated
            according to the evaluation result.
            Jiang et al. [31] propose a multi objective evolutionary
            algorithm, which is based on similarity. In order to consider
            the sign characteristic of signed networks, the authors
            proposed two new indexes. One index is the signed similarity
            index according to the existing similarity index [32]. Another
            index is the signed tightness index based on the existing
            tightness index [32]. During the run of the algorithm, when
            the movement can increase the signed tightness of the cluster
            (i.e., community), the node will be moved to another cluster.
            However, if the movement can not increase the signed
            tightness of the cluster, the node will be independent as a
            new cluster. A special case is that multiple clusters have the
            same vertices during the run of the algorithm, so when two
            clusters have more than half of the same vertices, they will be
            merged into a new cluster. The author of [33] used the
            Signed Stochastic Block-Model in the process of community
            detection. Haseyama et al. [34] partition network video by
            constructing a weighted signed network and maximizing the
            local modularity.
            Recently, some scholars have done some very good works
            for the community detection of signed networks. For
            example, Attea et al. [48] proposed a new multi-objective
            signed community detection model and a new anti-frustration
            heuristic operator for the community detection of signed
            networks. Ma et al. [68] used the relationship between
            balancedness and spectrum space, and proposed a spectrum
            algorithm based on leading eigenvectors of signed networks
            to partition clusters, so as to maximize the balancedness. Zhu
            et al. [49] proposed a new evolution algorithm for
            community detection in imbalanced signed networks which
            can be modeled as an optimal partition problem. And the
            evolving mechanism of nodes is updated by its neighbors’
            information which leads to form optimal community
            structure. Yan et al. [70] proposed a new modularized
            convex nonnegative matrix factorization (NMF) model,
            which combined signed modularized information with
            convex NMF model to improve the accuracy of community
            detection in signed and unsigned networks. As for model
            selection, Yan et al. extended the modularity density to
            signed networks and employed the signed modularity density
            to determine the number of communities automatically.
            Most of the algorithms described in this section are based
            on global information to identify communities. So far, some

            2

            VOLUME XX, 2017

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            scholars have begun to use local information for community
            detection. The knowledge of using local information for
            community detection is beyond the scope of this paper,
            which is not discussed here.
            III. FUNDAMENTAL NOTIONS AND BACKGROUND
            KNOWLEDGE
            A. THE DEFINITION OF SIGNED NETWORK AND THE
            DEFINITION OF CLUSTER DETECTION OF SIGNED
            NETWORK

            A signed social network can be modeled as a graph G = (V,
            E), where V = ( , , …, ) is the set of nodes (or vertices),
            E = {( , ) | ,
            V i } is the set of edges (or links).
            We can represent graph G by an adjacency matrix A=( ) ,
            where
            = 1(or − 1) if we observe it is the positive (negative)
            relationship between and , and
            = 0 means that there
            is no edge between and . If
            = 1, nodes and are
            positive neighbors of each other; if
            = -1, nodes and
            are negative neighbors of each other. Given a node
            V,
            =
            =
            and
            ⋀
            =
            =

            are defined respectively as the positive degree and the
            =
            =
            negative degree of vi .
            and
            are
            the total positive degree and the total negative degree of the
            signed network, respectively. If for any i and j,
            > = 0, then
            G is an unsigned network. Here, we do not consider the
            direction of the edge between any two nodes, that is, G is an
            undirected graph in this paper.
            Let C = { , , ... , } be a set of communities in G, that
            is, ⊂ V for i = 1, 2, ... ,k. The problem of community
            detection in signed networks is accurately expressed in (1).
            t

            t

            ( )

            where p
            q, p, q = 1, 2, ... , k. The problem can be
            described as identifying the community partition that
            maximizes the sum of positive edges within communities
            and negative edges between communities.
            If all the positive edges in the signed network are in the
            communities, and all the negative edges are between the
            communities, the signed network is balanced; otherwise, the
            signed network is unbalanced.
            B. A SIGNED MODULARITY FUNCTION OF SIGNED
            GRAPHS

            In this section, a signed modularity function
            [35] of the
            signed networks will be described. We will use it later in
            this study. Its expression is as follows:

            =

            ( )

            where
            is the element of adjacency matrix A, and
            denote the communities to which nodes
            and
            belong,
            = 1, otherwise,
            = 0.
            respectively. If = ,
            To understand the meaning of ( ), ( ) and
            ,
            please refer to section III-A.
            IV.

            PROPOSED ALGORITHM

            Here, we put forward a memetic algorithm use in community
            discovery in signed networks, called MACD-SN. At first, we
            describe the representation of chromosomes in a population.
            Then, a novel population initialization algorithm is presented,
            which can significantly speed up the convergence rate of the
            MACD-SN algorithm. Next, a computational formula for
            assessing chromosomes within a population is introduced. At
            the same time, a selection operator for selecting parent
            chromosomes for subsequent genetic operations is described.
            Afterwards, we elaborate on a crossover operation (called
            randomized two-way crossover operation) and two mutation
            operations (called traditional mutation operation and
            community mutation operation respectively), which are used
            in MACD-SN algorithm. The novel crossover operation and
            novel community mutation operation proposed by us can
            significantly improve the accuracy of MACD-SN algorithm's
            result and reduce the running time of the algorithm. In the
            end, we present a local solution space search subroutine. This
            subroutine can make the result of MACD-SN algorithm jump
            out of the local optimal result with a certain probability and
            approach the global optimal result quickly. The rest of
            Section IV will elaborate on each theme. Figure 2 shows the
            flow diagram of MACD-SN algorithm.
            A. CHROMOSOME REPRESENTATION

            In the classical memetic algorithm, each candidate
            community division of network is denoted by a chromosome,
            also called a solution or an individual. A set of a certain
            number of chromosomes is known as a population, i.e.,
            population popu={ t , t , … , t }, where t is the jth
            chromosome within the chromosomes set and q is the
            number of chromosomes in the set. Chromosome coding
            methods frequently used in the literatures consist of locusbased coding method and string-based coding
            method. In
            order to obtain the corresponding community partition by
            decoding chromosome conveniently, we make use of stringbased coding method. The jth chromosome in the
            population
            of memetic algorithm can be represented as:
            t =[
            ]. Among them,
            is the kth gene (or
            component) of individual t , and the amount of vertices in
            the signed network is n. The value range of each gene is {1,
            2,..., n}. Genes denote the vertices in the signed network, and

            2

            VOLUME XX, 2017

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            the value of the kth gene denote the community to which
            vertex belongs. In this coding method, if vertices and
            are in the same community,
            =
            .

            algorithm. Before introducing the initialization algorithm, we
            first introduced a definition to be used in the algorithm.
            Definition 1. (Node Imbalance Degree (NID)). Let
            be a
            node in the signed network G, and c be a community in G,
            then the node imbalance degree NID( ,c) of node relative
            to community c is calculated as follows:
            NID( c)=
            ⋀(

            )

            ⋀

            ⋀(

            =

            )

            ⋀

            =

            +
            (3)

            Let
            and
            be two communities in G. Obviously,
            according to the definition of community structure of signed
            network, when NID( , )< NID( , ), the priority should
            be given to assigning node to community . Therefore, in
            the initialization process of the MACD-SN algorithm
            proposed by us, node k (equivalent to above) is assigned to
            the neighbor community which reduces its node imbalance
            degree (NID) to the minimum. Algorithm 1 describes the
            pseudocode of the initialization process of the presented
            MACD-SN algorithm.

            FIGURE 2. The flow diagram of MACD-SN Algorithm.

            The chromosome shown in Figure 3(b) is a string-based
            coding method of the signed network illustrated in Figure
            3(a). This signed graph consists of seven vertices. The
            number of vertices in the network is 1, 2, 3, 4, 5, 6 and 7
            respectively. In Figure 3(b), we can see that the gene values
            assigned to vertices 1, 2, 4 and 5 are all 1, and the gene
            values assigned to vertices 3, 6 and 7 are all 2. This shows
            that the network contains two clusters, in which vertices 1, 2,
            4 and 5 are in the same cluster, while vertices 3, 6 and 7 are
            in the other cluster. Figure 3(c) illustrates the cluster partition
            decoded by the chromosome in Figure 3(b).
            B. CREATION OF INITIAL CHROMOSOMES
            POPULATION

            In order to get an excellent memetic algorithm, it is very
            important to generate a good initial population. The reason is
            that the properties of the initial chromosomes will have an
            effect on the convergence rate and the quality of the final
            result of the method. Therefore, we propose an efficient
            population initialization algorithm to generate a good initial
            population and to reduce the convergence time of the whole

            Algorithm 1. Pseudocode of population creation function
            initialize() of MACD-SN method.
            Algorithm Parameters: chromosomes set size
            popu_size;
            Algorithm Input: A matrix G representing a signed
            graph;
            Algorithm Output: The initial set of chromosomes
            generated;
            1: for j=1 to popu_size do
            2:
            for k=1 to n do
            3:
            popu[j][k]= A random integer in the range of 1
            to n generated randomly; //popu[] is an array
            //of chromosomes population. popu[j][k]
            //represents the kth gene of the jth
            //chromosome.
            4:
            end for
            5:
            flag=1;
            6:
            while flag do
            7:
            flag=0;
            8:
            for k=1 to n do
            9:
            comms= The set of communities to which
            the neighbor nodes of node k belong;
            10:
            t=∞;
            11:
            m=popu[j][k];
            thh do
            12:
            for each community thh
            13:
            if NID(k, thh)
            <t then
            14:
            t=NID(k, thh);
            15:
            m= thh;
            16:
            end if
            17:
            end for
            t
            [ ] then
            18:
            if m
            19:
            popu[j][k]=m;
            20:
            flag=1;
            21:
            end if

            2

            VOLUME XX, 2017

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            FIGURE 3. (a)A signed network contains 7 vertices, the solid line edge represents a positive relationship,
            and the dotted line edge represents a negative
            relationship; (b)string-based coding method of a individual; (c)schematic diagram of the community structure
            of the network in (a) decoded by the
            individual in (b).

            22:
            end for
            23: end while
            24: end for
            25: return popu[];
            In the above algorithm, the for loop at lines 1 to 24 is used
            in producing the initial chromosomes set. The codes in lines
            2 to 4 generate an initial chromosome, each gene value of
            which is a random integer randomly generated in the range of
            1 to n. The while loop of lines 6 to 23 continuously optimizes
            the generated chromosomes, until in a while loop, each node
            cannot optimize its community label according to its
            neighbors community label. The for loop of lines 8 to 22
            optimizes chromosome by a single pass loop from 1 to n. In
            line 9, the community labels of the neighbor nodes of the
            current node k are stored in the set comms. The codes in lines
            10 to 17 is responsible for finding the community in comms
            that minimizes the node imbalance degree (NID) of node k,
            and storing it in m. The codes in lines 18 to 21 check whether
            the current community label of vertex k is m, if not, use the m
            value as the community label of vertex k, and set the value of
            the variable flag to 1 to continue the while loop. In the end,
            the codes in line 25 return the generated population of
            chromosomes.
            Each time the for loop of line 1 is executed, it needs to
            iterate the popu_size times. Each time the for loop of line 2 is
            executed, it needs to iterate n (n is the number of nodes in the
            signed network) times. A large number of experiments show
            that every time the while loop of line 6 is executed, it needs
            to iterate 8 times on average, no more than 13 times at most.
            Each time the for loop of line 8 is executed, it needs to iterate
            n times. It is assumed that the average degree of nodes in the
            network is d (d
            <
            < n). The time complexity of the statement
            in line 9 is O(d). In the worst case, the for loop of line 12
            needs to iterate n-1 times for each execution. Therefore, the
            time complexity of the initialization process of MACD-SN
            algorithm is O(popu_size
            ).

            D. SELECTION OPERATOR

            For selecting parent individuals for subsequent genetic
            operations, we should propose a good selection operator. In
            recent years, many methods have been developed as the
            selection algorithm of memetic algorithm (MA). Tournament
            selection algorithm is one of them. In order to make the low
            fitness individuals appear in the offspring chromosomes set,
            we need a method to control elitism. The tournament
            selection algorithm meets this requirement, so we chose it.
            One of the most attractive features of this algorithm is that
            chromosomes in the current population have the same
            probability of becoming the parent chromosomes of
            subsequent genetic operations. The flow chart of the
            algorithm is shown in Figure 4.

            C. FITNESS FUNCTION

            In the MACD-SN algorithm proposed in this paper, we use
            the signed modularity formula
            (Eq. 2) introduced in
            section III-B as the fitness function. According to [35], the
            larger the value of
            , the better the community partition
            obtained.

            FIGURE 4. The flow chart of tournament selection algorithm.

            2

            VOLUME XX, 2017

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            The time complexity of tournament selection algorithm is
            O( ) ( k is the number of parent chromosomes to be selected
            by the tournament selection algorithm.).
            E. CROSSOVER OPERATOR

            Crossover operator is also one of the genetic operators of
            MA algorithm. The crossover operator applies
            simultaneously to two parent chromosomes chosen by the
            selection operator, and reproduces new chromosomes
            through interchanging contents between the selected
            chromosomes. Hence, the chromosomes produced through
            crossover operator possess simultaneously the genetic
            properties of two chromosomes of the previous generation
            [36]. The execution process of crossover operator is:
            (1)choosing two chromosomes from the parent chromosomes
            set; (2)exchanging contents between them in the light of
            crossover operation rules to generate novel offspring
            chromosomes; (3)giving two novel chromosomes. First,
            randomly select a crossing location. Afterwards, according to
            the specified crossover probability, the corresponding
            contents of the two parent chromosomes around the location
            are interchanged. Traditional crossover operations are listed
            below: the uniform crossover, the one-point crossover, the
            one-way crossover, the two-point crossover, the two-way
            crossover, and so on.
            For taking full advantage of the community partition
            information of two individuals of the previous generation,
            enhance the accuracy of the offsprings produced through
            crossover operation and reduce the running time of algorithm,
            this paper presents a novel crossover operation, which is
            named randomized two-way crossover operation. The pseudo
            code of randomized two-way crossover operation is shown in
            algorithm 2.
            Algorithm 2. Pseudocode for function Crossover() that
            performs randomized two-way crossover operation.
            Algorithm Parameters: Two parent individuals (i.e.
            chromosomes) t and t selected by tournament
            selection algorithm;
            Algorithm Input: A matrix G representing a signed
            graph;
            Algorithm Output: The two generated individuals;
            1: for i=1 to 2 do
            2: for j=1 to n do
            t _s[j]=’U’; // t _s[j] = 'U' means t [j] has
            3:
            //not been touched yet, t _s[j] = 'V' means
            // t [j] has been touched.
            t _s[j]=’U’; // t _s[j] = 'U' means t [j] has
            4:
            //not been touched yet, t _s[j] = 'V' means
            // t [j] has been touched.
            [ ]=0;
            5:
            6:
            end for
            7: ct=1;
            8: repeat
            9:
            Generate a random number r between 0 and 1;
            10:
            if 0 ≤ r ≤ 0.5 then
            VOLUME XX, 2017

            11:

            Randomly choose an untouched component in
            t , assuming that the chosen component is
            t [k], that is, t _ s[k]='U';
            12:
            Locate all the component locations in t that
            have the same component value as the
            t [k], assuming that these components are
            t [ ], t [ ],..., t [ ];
            t ⋯
            13:
            if [ ] t
            [ ]
            [ ] 0
            then
            14:
            for each u { , , … , } do
            t _s[u]=’V’;
            15:
            16:
            end for
            17:
            goto 11;
            18:
            end if
            19:
            for each u { , , … , } do
            20:
            if [u]==0 then
            21:
            [u]=ct;
            22:
            end if
            t _s[u]=’V’;
            23:
            24:
            end for
            25:
            else
            26:
            Randomly choose an untouched component in
            t , assuming that the chosen component is
            t [k], that is, t _ s[k]='U';
            27:
            Locate all the component locations in t that
            have the same component value as the
            t [k], assuming that these components are
            t [ ], t [ ],..., t [ ];
            t ⋯
            28:
            if [ ] t
            [ ]
            [ ] 0
            then
            29:
            for each u { , , … , } do
            t _s[u]=’V’;
            30:
            31:
            end for
            32:
            goto 26;
            33:
            end if
            34:
            for each u { , , … , } do
            35:
            if [u]==0 then
            36:
            [u]=ct;
            37:
            end if
            t _s[u]=’V’;
            38:
            39:
            end for
            40:
            end if
            41:
            ct=ct+1;
            42: until all component locations of the descendant
            individual
            are written in;
            43:end for
            44:return the generated descendant individuals and ;
            Figure 5 shows an executive process of the crossover
            operator of the MACD-SN method. Figure 6 shows an
            example of community partition encoded by chromosomes
            t , t , and in Figure 5, respectively.
            In Figure 5, t and t are two parent individuals chosen
            by the tournament selection algorithm from the previous
            generation population. The steps of obtaining a descendant
            individual
            by the individuals t and t of the previous
            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            generation are listed below. Step one: Label all components
            of t and t as untouched, assign 0 to all components of
            the descendant individual . Step two: Generate a random
            number r. Suppose r = 0.7. Because 0.5
            <r
            ≤ 1, in individual
            t , an untouched component 4 is randomly chosen, and all
            components with the identical component value as
            component 4 in individual t , namely components 2, 4, 6
            and 9, are located. Afterwards, 1 is assigned to the counter ct,
            and the present ct value is written in the components 2, 4, 6
            and 9 of the descendant individual . Next, components 2, 4,
            6 and 9 within individual t are labeled as touched. Step
            three: Generate a random number r. Suppose r = 0.4.
            Because 0 ≤ r ≤ 0.5, in individual t , an untouched
            component 8 is randomly chosen, and all components with
            the identical component value as component 8 in individual
            t , namely components 8 and 9, are located. Afterwards, ct
            + 1 = 2 is assigned to ct, and write the current ct value into
            the component 8 of the descendant individual . Due to the
            prevenient operation of the algorithm has written component
            9 of , this algorithm step doesnt change it. Component 9 of
            the individual
            remains unchanged. Afterwards,
            components 8 and 9 in individual t are labeled as touched.
            Step four: Generate a random number r. Suppose r = 0.2.
            Because 0 ≤ r ≤ 0.5, in individual t , an untouched
            component 2 is randomly chosen, and all components with
            the identical component value as component 2 in individual
            t , namely components 2 and 6, are located. Because the
            components 2 and 6 of
            have been written in, the algorithm
            first labels components 2 and 6 of t as touched, and then
            randomly chooses another untouched component 5 in t .
            Next, the algorithm locates all components with the identical
            component value as component 5 in individual t , namely
            components 4 and 5. Afterwards, let ct = ct+1=3, and write
            the current ct value into the component 5 of the descendant
            individual
            . Due to the prevenient operation of the
            algorithm has written component 4 of
            , this operation
            doesnt change it. Then, components 4 and 5 in individual
            t are labeled as touched. Step five: Generate a random
            number r. Suppose r = 0.6. Because 0.5
            <r
            ≤ 1, in individual
            t , an untouched component 7 is randomly chosen, and all
            components with the identical component value as
            component 7 in individual t , namely components 1, 3 and
            7, are located. Afterwards, let ct = ct+1=4, and write the
            current ct value into the components 1, 3 and 7 of the
            descendant individual
            . Then, components 1, 3 and 7 in
            individual t are labeled as touched. At this time, the
            individual
            has been filled, so the work of producing
            descendant individual
            has been completed. We obtain
            descendant individual . The generative steps of the other
            descendant individual
            are analogous to that of . Figure
            5 (c) shows the generation process of descendant individual
            .
            If the chromosome contains m communities on average,
            the time complexity of crossover operation is O(m
            ). In
            the worst case, there are n communities in the parent

            FIGURE 5. An executive process of the crossover operator of the MACDSN method. In the figure, U represents
            that the relevant component has
            been untouched, and V represents that the relevant component has been
            touched. The figure in the rectangle represents the label of the cluster that
            contains the relevant node. The value 0 represents that the relevant
            component has not been written. (a)Previous generation individuals
            Ԅ
            Ԅ . (b)The producing process of individual
            . (c)The producing
            process of individual .

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            component stands for the cluster containing the node. In the
            cluster identification methods, the frequently-used mutation
            operation first stochastically chooses a vertex , after that
            stochastically chooses a neighbour vertex of ( and
            are not included in the identical cluster), and at last assigns
            to the component
            the cluster label (component value) of
            corresponding to vertex
            [37]. The time complexity of
            frequently-used mutation operation is O(1).
            Figure 7 shows an instance of using a traditional variation
            operation. As indicated in Figure 7, a chromosome H is
            selected first, and afterwards a vertex 5 on chromosome H is
            randomly chosen. The adjacent vertices of vertex 5 include
            vertices 1, 2, 3 and 4. Suppose that vertex 2 is randomly
            chosen. Thus, the component value of vertex 2 is assigned to
            the component corresponding to vertex 5.

            FIGURE 7. A schematic diagram of a traditional mutation operator.

            FIGURE 6. An example of community partition encoded by chromosomes
            Ԅ , Ԅ ,
            and
            in Figure 5, respectively. A solid line indicates a
            positive edge and a dotted line indicates a negative edge.

            chromosomes of crossover operation, then the time
            complexity of crossover operation is O( ).
            F. MUTATION OPERATOR

            In the MACD-SN method, in addition to the frequently-used
            variation operation (this paper calls it the traditional variation
            operation), we also put forward the other variation operation
            (this paper calls it the community variation operation). In this
            paper, mutation and variation are interchangeable.
            As indicated in Figure 3, in classical string-based coding
            method, every component denotes a node, and the value of

            Previous to describe community mutation operator in
            detail, the definition of community imbalance degree is first
            described.
            Definition 2. (Community Imbalance Degree (CID)). Let
            comm be a community in the signed network G, then the
            community imbalance degree CID (comm) of community
            comm is calculated as follows:
            CID (comm)=

            thh

            th(

            thh)

            (4)

            where
            is the number of nodes in community comm. The
            smaller the value of number of CID (comm), the more
            evident the cluster structure of cluster comm.

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            The community variation operation will be described in
            detail below. If the CID (comm) is higher than a parameter
            , we think that the vertices in comm can not constitute a
            signed cluster. At this point, we need to use community
            mutation operator to reallocate the vertices in comm to
            more suitable clusters. In algorithm 3, the operation steps of
            community mutation operator of the MACD-SN method are
            presented. Figure 8 is an explanatory drawing of the
            execution steps of community mutation operator of the
            MACD-SN method.
            Algorithm 3. Pseudocode for function
            CommunityMutation() that performs community mutation
            operator.
            Algorithm Parameters: parent individual H, a threshold
            parameter ;
            Algorithm Input: A matrix G representing a signed
            graph;
            Algorithm Output: improved individual ;
            1: comms= The set of clusters in H;
            2: k= ∞;
            3: m=0;
            4: for each cluster comm comms do
            5:
            if CID(comm)>k then //CID (comm) is the
            //community imbalance degree of community
            //comm.
            6:
            k=CID(comm);
            7:
            m=comm;
            8: end if
            9: end for
            10:if m 0 ⋀ CID(m)> then
            11: ns = All vertices in m;
            = ;
            12:
            13: while every vertex
            ns do
            14:
            Stochastically choose a positive neighbour
            of
            vertex
            (
            and
            are not included in the
            identical cluster);
            15:
            if there is satisfying the condition then
            16:
            [ ]= [ ];
            17:
            end if
            18: end while
            19:
            =
            value of H computed using formula (2);
            20: q =
            value of computed using formula (2);
            21: if q >
            then
            22:
            return ;
            23: end if
            24: end if
            25: return H;
            Figure 8(a) shows an example of a simple signed graph.
            Figure 8(b) illustrates an individual produced through the
            graph in Figure 8(a). This individual is composed of three
            clusters, i.e. cluster 1, 2 and 3. By equation (4), we may get
            CID (comm1) = = 2, CID (comm2) = = 2.5, and CID

            (comm3) = 1/3. Let = 2.1. Thus, cluster 2 is chosen for
            mutation. Among the neighbors of vertex
            , vertices
            and
            are its positive neighbors, and they are not in the
            same cluster as vertex
            . Since the cluster labels of
            vertices
            and
            are both 1, the cluster label of vertex
            are set as 1. Among the positive neighbors of vertex
            ,
            vertices ,
            and
            are not in the same community as
            vertex
            . Suppose vertex
            is selected randomly.
            Therefore, the community label 1 of vertex
            is assigned
            to vertex .
            The time complexity of community mutation operation is
            O(n).

            FIGURE 8. An instance of the execution process of community variation
            operation of the MACD-SN method.

            G. LOCAL SEARCH FUNCTION

            For memetic algorithm, the local search process is a crucial
            part. A good local search subroutine may significantly
            increase the accuracy of the ultimate result of the algorithm
            and reduce the time to find the optimal solution. Therefore,
            this paper presents a subroutine called LocalSeek(), which
            achieves the above goals well. Algorithm 4 describes the
            details of the subroutine LocalSeek().
            Algorithm 4. LocalSeek().
            Algorithm Parameters: best individual bestOffspring in
            the offspring individuals set, a threshold parameter ρ;
            Algorithm Input: A matrix G representing a signed
            graph;
            Algorithm Output: revised individual bestOffspring;

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            1: A stochastic permutation of the natural numbers in
            the range of 1 to n is generated and saved in a set
            called seq;
            2: for count=1 to n do
            3: i=seq[count];
            4: comms=∅;
            5: Put the clusters (i.e. communities) containing most
            positive neighbors of vertex i into the set
            comms(there may be multiple clusters that meet
            the condition);
            6: k= ∞;
            7: m=0;
            8: if comms ∅ then
            9:
            for each cluster comm comms do
            10:
            newComm= Put node i into cluster comm to
            form a new cluster;
            11:
            if CID (newComm)
            <k then
            //CID (newComm)
            //is the community imbalance
            //degree of community newComm.
            12:
            k=CID (newComm);
            13:
            m=comm;
            14:
            end if
            15:
            end for
            16: else
            17:
            Put the clusters containing negative
            neighbors of vertex i into the set comms;
            18:
            for each cluster thh comms do
            19:
            newComm= Put node i into cluster comm to
            form a new cluster;
            20:
            if CID(newComm)
            <k then
            //CID (newComm)
            //is the community imbalance
            //degree of community newComm.
            21:
            k=CID(newComm);
            22:
            m=comm;
            23:
            end if
            24:
            end for
            25: end if
            26: if m 0 then
            27: newDivision=Put the vertex i of bestOffspring into
            cluster m to produce a novel
            individual;
            28:
            =
            value
            of
            individual
            bestOffspring was
            t
            computed through formula (2);
            29:
            =
            value of individual newDivision was
            computed through formula (2);
            30:
            if
            t then
            31:
            bestOffspring =newDivision;
            32:
            else if random number r between 0-1 generated
            randomly
            <
            33:
            bestOffspring =newDivision;
            34:
            end if
            35: end if
            36:end for
            37:return bestOffspring;

            In algorithm 4, the codes in line 1 generate a random
            permutation of the natural numbers from 1 through n and
            put it in a set called seq. The iteration from 1 to n of lines 2
            to 36 attempt to assign each node in the network to another
            more appropriate cluster (i.e. community) in the order of
            the nodes specified in the set seq. Obviously, according to
            the definition of the signed community, we should first try
            to put the current node i into the community to which most
            of its positive neighbors belong. When node i has no
            positive neighbors, we should try to put node i into the
            community that one of its negative neighbors belongs to.
            Algorithm 4 embodies this idea. The codes in line 5 put the
            clusters containing the most positive neighbours of the
            current vertex i into the set comms. (there may be multiple
            clusters that meet the condition). The codes in lines 9 to 15
            deal with the case where node i has positive neighbors.
            Suppose there are p clusters in comms, which are
            thh , thh , … , thh . After node i is assigned to
            clusters thh , thh , … , thh , the new clusters
            thh ,
            thh , ... ,
            thh ,
            formed are
            respectively. The codes in lines 9 to 15 are responsible for
            finding the community thh
            so that the CID
            thh ) is the minimum value in CID (
            thh ),
            (
            thh ), ... , CID (
            thh ), that is, thh
            CID (
            min
            th( thh ∪ ) . Obviously, at this
            = arg
            thh

            thh

            point, thh is the most suitable community in the set
            comms to merge with node i. The codes in lines 17 to 24
            handle the case where node i has only negative neighbors.
            The codes in line 17 put the clusters containing the negative
            neighbours of vertex i into the set comms. Suppose there are
            q clusters in comms, which are thh , thh , … ,
            thh . After node i is assigned to clusters
            thh , thh , … , thh , the new clusters formed are
            thh ,
            thh , ... ,
            thh , respectively.
            The codes in lines 18 to 24 are responsible for finding the
            thh ) is the
            cluster thh so that the CID (
            thh ), CID
            minimum value in CID (
            thh ), ... , CID (
            thh ), that is, thh =
            (
            min
            th( thh ∪ ) . Obviously, at this point,
            arg
            thh

            thh

            thh is the most suitable cluster in the set comms to
            merge with node i. The codes in lines 26 to 35 are
            responsible for deciding whether node i can be placed in the
            cluster thh (i.e. cluster m) found. If possible, place node
            i in the cluster m. The codes in line 27 put the node i of the
            individual bestOffspring into the cluster m to form a new
            individual newDivision. The codes in line 28 use equation
            (2) to calculate the
            value for individual bestOffspring
            and assign the calculated
            value to variable t . The
            codes in line 29 use equation (2) to calculate the
            value
            for individual newDivision and assign the calculated
            value to variable
            . The codes in line 30 determines if
            is greater than t . If
            is greater than t , the codes
            in line 31 take the individual newDivision as the best
            individual bestOffspring in the offspring population. If

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            is not greater than
            t , the codes in line 32 randomly
            generate a real number r between 0 and 1, and determines
            whether it is less than the input parameter
            of the
            algorithm. If less than, the codes in line 33 take the
            individual newDivision as the best individual bestOffspring
            in the offspring population. The codes in line 37 return the
            improved individual bestOffspring.
            Our subroutine LocalSeek() has one outstanding merit.
            This merit is that it may accept a worse result within a
            certain odds. In the process of solving, this merit enables
            LocalSeek() to increase the accuracy of the result of the
            algorithm, and also helps the algorithm jump away from the
            local best solution. In algorithm 4, the conditional statement
            in line 32 implements this function. When the algorithms
            find the local optimal solution, some other algorithms will
            terminate the iteration. On account of some other
            algorithms can not find the global best solution by
            movement in a small area near the local best solution.
            Nevertheless, the LocalSeek() presented in this paper can
            do this with a specified odds, namely, it may adopt a result
            with a certain odds that is worse than the present result.
            This is helpful for the subroutine to jump out of the local
            best result and attain the global best result after a few
            moving operations.
            In algorithm 4, the time complexity of the statement in
            line 1 is O(n logn). Each time the for loop of line 2 is
            executed, it needs to iterate n times. It is assumed that the
            average degree of nodes in the network is d (d
            <
            < n). The
            time complexity of the statement in line 5 is O(d). In the
            worst case, the for loop of line 9 needs to iterate n-1 times
            for each execution. Similarly, in the worst case, the for loop
            of line 18 needs to iterate n-1 times for each execution. The
            time complexity of the statement in line 17 is O(d). Because
            d
            <
            < n, the time complexity of LocalSeek() is O ( ).
            Algorithm 5 gives the pseudo code of the main function
            of MACD-SN algorithm.
            Algorithm 5. MACD-SN method.
            Algorithm Parameters: population size popu_size,
            number of parent individuals selected by tournament
            selection algorithm k, crossover probability , mutation
            probability
            of traditional mutation operator, mutation
            probability
            of community mutation operator, a
            threshold parameter δ used in community mutation
            operator, a threshold parameter ρ used in the function
            LocalSeek(), amount of iterations without revision gt;
            Algorithm Input: A matrix G representing a signed
            graph;
            Algorithm Output: a cluster partition comms of the
            signed graph;
            1: popu= initialize(popu_size); //produce initial individual
            //population;
            2: repeat
            3:
            Use formula (2) to compute the fitness function
            value of every individual in the population popu;

            4:

            for i=1 to k do //Using tournament selection
            //algorithm to select parent individuals for
            //subsequent genetic operations.
            5:
            Randomly select k individuals from the
            population set popu;
            6:
            Using the selected k individuals to build a
            maximum heap;
            7:
            offs[i]= The top element of the maximum heap;
            //offs[] is a individuals set selected for
            //subsequent genetic operations.
            8:
            end for
            9:
            i=1;
            10: while i≤k-1 do
            11:
            if rand(0,1)≤
            then
            12:
            (offs[i], offs[ i+1])=Crossover(offs[i], offs[i+1]);
            //Perform a randomized two-way crossover
            //operation. The Crossover function returns
            //two individuals, offs[i] and offs[i + 1].
            13:
            end if
            14:
            i=i+2;
            15: end while
            16: for i=1 to k do
            17:
            if rand(0,1)≤
            then
            18:
            offs[i]=The traditional mutation operator is
            executed on the offs[i];
            19:
            end if
            20: end for
            21: for i=1 to k do
            22:
            if rand(0,1)≤
            then
            23:
            offs[i]=CommunityMutation(offs[i], ); //The
            //community mutation operator is executed
            //on the offs[i];
            24:
            end if
            25: end for
            26: bestOffspring= The individual with the maximum
            fitness function value in offs;
            27: bestOffspring=LocalSeek(bestOffspring, );
            t
            28: popu=popu
            bestOffspring;
            29: popu=The set of the first popu_size individuals
            with the largest fitness in the population popu;
            30:until no improved amount of iterations for the
            optimal individual in population popu > = gt;
            31:comms= Cluster partition of individual with the
            biggest fitness function value in the population
            popu;
            32:return comms;

            According to the analysis of the prevenient sections, in
            algorithm 5, the time complexity of the statement in line
            1 is O(popu_size
            ). The time complexity of the
            statement in line 3 is O(popu_size). The time complexity
            of the codes in lines 4 to 8 is O( ) （k is the number of
            parent chromosomes to be selected by the tournament
            selection algorithm.）. On average, the time complexity

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            of the codes in lines 10 to 15 is O(k m
            )（m is the
            average number of communities contained in a single
            chromosome. ） . The time complexity of the codes in
            lines 16 to 20 is O(k). The time complexity of the codes
            in lines 21 to 25 is O(k
            ). The time complexity of the
            statement in line 26 is O(k). The time complexity of the
            statement in line 27 is O( ). The time complexity of the
            ) ).
            statement in line 29 is O(popu_size log ( t ꀀ
            The time complexity of the statement in line 31 is
            O(popu_size+n). Through a large number of
            experiments, we can see that the average execution times
            of repeat statement in line 2 is about popu_size/3.
            Therefore, the time complexity of MACD-SN algorithm
            is O(popu_size
            m
            +popu_size
            ).
            V.

            EXPERIMENTAL RESULTS

            In our experiment, the MACD-SN algorithm proposed in
            this paper is verified in the synthetic and real-world signed
            networks. We also make comparisons with other four
            famous algorithms, namely, FEC[38], SSL[39], DM[40]
            and SISN [41]. We use normalized mutual information
            (NMI) [66] to assess the capability of the methods. Given
            that A and B are two partitions of a network, then NMI (A,
            B) is defined as follows.
            NMI(A, )=
            =

            =

            ㌳ th (

            =

            ㌳)

            th (

            =

            ㌳

            ㌳ ㌳

            )

            th (

            ㌳

            )

            link between communities is positive. pm- and pm+ are
            also known as the yawp parameters.
            In order to verify our method effectively, in our
            experiment, we produce five kinds of synthetic signed
            networks, which have different characteristics. These kinds
            of synthetic signed networks include two different types of
            networks: balanced networks and unbalanced networks.
            Among them, only networks 1 is balanced, and other kinds
            of networks are imbalanced.
            Networks 1: The generation model of this kind of
            networks is SG (6, 42, 42, h , 0, 0). Among them, in the
            interval [0, 1], h increases gradually, and the increment
            of each step is 0.1. Because the positive edges are within
            the clusters and the negative edges lie between the clusters,
            this kind of signed networks are balanced. We carry out
            five methods on 11 networks of this kind of networks. In
            Figure 9, we describe the results of the five algorithms.
            As shown in Figure 9, the MACD-SN and SISN can
            accurately discover all communities in the networks. This
            means that the two algorithms are not sensitive to the
            variation of the parameter h
            and they have good
            detection ability within the balanced signed graphs. The

            (5)

            where N is the number of nodes of the network, C is a
            equals to the number of nodes shared
            confusion matrix.
            in common by community i in partition A and by
            (or
            ) is the number of
            community j in partition B.
            clusters in partition A (or B), ㌳ (or ㌳ ) is the sum of
            elements of C in row i (or column j). NMI takes value
            between 0 and 1. The larger the value of number of NMI,
            the more evident the cluster structure obtained.
            In this paper, the source codes of FEC, SSL, DM and
            SISN are obtained from the original author. MACD-SN is
            implemented by C# 4.0 using Microsoft Visual Studio 2010.
            In the experiment, we set the parameters popu_size = 1000,
            k=500, = = = 0.3, =3.9, gt = 3, and ρ= 0.15.
            A. SYNTHETIC SIGNED NETWORKS

            By using the generation model in reference [38], the
            synthetic signed networks used in our experiment are
            produced. This model can be described as
            SG(c, n, k, h , pm-, pm+)
            where c is the number of communities, n is the number of
            vertices in each community, k is the average degree of the
            vertices, h is the probability that a link falls within a
            community, pm- is the probability that a link within
            communities is negative, and pm+ is the probability that a

            FIGURE 9. The output of MACD-SN algorithm and four comparison
            algorithms on Networks 1.

            SSL algorithm also has excellent detection ability, when
            h ≤ 0.8, it can correctly identify the communities.
            Networks 2: The generation model of this kind of
            networks is SG(6, 42, 42, t㌳5, pm-, 0). Among them, in the
            interval [0, 1], pm- increases gradually, and the increment
            of each step is 0.1. This kind of signed networks is
            imbalanced, but the yawp only consists in the clusters. In
            these signed networks, the positive edges do not exist
            between clusters. The greater the numerical value of pm-,
            the more minus edges the clusters contain.

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            The outputs of the five methods running on Networks 2
            are shown in Figure 10. As we can see, when pm- varies
            from 0 to 1, the numerical values of NMI of the MACD-SN
            algorithm and SSL algorithm are both 1. This result shows
            that MACD-SN and SSL can perfectly identify the
            community structures in this kind of signed networks. The
            performance of the other three algorithms is relatively poor.
            Networks 3: The generation model of this kind of
            networks is SG(6, 42, 42, t㌳5, 0, pm+). Among them, in the
            interval [0, 1], pm+ increases gradually, and the increment
            of each step is 0.1. In this kind of networks, noises only
            exists between clusters, and there are no negative edges in
            the clusters, so this kind of signed networks is imbalanced.
            The greater the numerical value of pm+, the more positive
            edges there are between clusters in the network.
            Figure 11 shows the outputs of the five methods on
            Networks 3. In Figure 11, we may observe that MACD-SN
            is capable of identifying the clusters in the networks well,
            but when pm+ > 0.1, the detection ability of SSL decreases.
            On the whole, the sorting results of the five algorithms by
            performance are MACD-SN, SSL, SISN, DM and FEC.

            in the networks; when pm- > 0.3, the performance of
            MACD-SN decreases. The SSL also has good accuracy, but
            the DM and FEC have very bad accuracy. Among the five
            algorithms, the performance of SISN is in the third place.
            Networks 5: The generation model of this kind of
            networks is SG(6, 42, 42, t㌳5, 0.5, pm+). Among them, in
            the interval [0, 1], pm+ increases gradually, and the
            increment of each step is 0.1. Due to some negative edges
            consist in the clusters and a number of positive edges
            consist between the clusters, this kind of signed networks is
            imbalanced. The greater the numerical value of pm+, the
            more positive edges there are between clusters in the
            network.
            Figure 13 shows the results of the five methods running
            on networks 5. We can see that in Figure 13, MACD-SN
            has the best accuracy among the five algorithms. SSL is in
            second place. The rest are in order: SISN, FEC, and DM. In
            order to further prove the high performance of our
            presented algorithm, in Chart 1, we show the average
            values of NMI. These average values are derived from the
            results of every method executing on the above five kinds

            FIGURE 10. The outputs of MACD-SN algorithm and four comparison
            algorithms on Networks 2.

            FIGURE 11. The outputs of MACD-SN algorithm and four comparison
            algorithms on Networks 3.

            Networks 4: The generation model of this kind of
            networks is SG(6, 42, 42, t㌳5 , pm-, 0.5). Among them, in
            the interval [0, 1], pm- increases gradually, and the
            increment of each step is 0.1. In this kind of networks, not
            only the negative edges exist within the clusters, but also
            the positive edges exist between the clusters, so this kind of
            signed networks is imbalanced. The greater the numerical
            value of pm-, the more minus edges the clusters contain.
            The outputs of the five methods running on Networks 4
            are shown in Figure 12. We can see that in Figure 12, when
            pm- ≤ 0.3, MACD-SN can perfectly identify communities

            of signed networks. In Chart 1, we may observe that our
            algorithm shows the best accuracy among the five
            comparison algorithms.
            B. REAL SIGNED NETWORKS

            About the real signed networks, in the experiment, we
            choose two real-world networks with true cluster partition
            and three real-world networks without true cluster partition
            to test our method. Two real signed networks that possess
            true cluster partition are Gahuku-Gama subtribes network

            VOLUME XX, 2017

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            9

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            Eastern Central Highlands of New Guinea. This network
            describes the political alliance and enmities among the 16
            Gahuku-Gama subtribes, which were distributed in a
            particular area and were engaged in warfare with one
            another in 1954. The positive and negative links of the
            network correspond to political arrangements with positive
            and negative ties, respectively.
            The results of our algorithm running on SPPN are
            shown in Figure 14. In Figure 14, we may observe that
            MACD-SN detects two clusters on SPPN. The red circle
            vertices are in one cluster, and the blue square vertices are
            in the other cluster. Since all the plus edges are in the
            clusters and all the minus edges are between clusters, SPPN
            is balanceable. The final results of our algorithm is in line
            with the true cluster partition of SPPN.

            FIGURE 12. The outputs of MACD-SN algorithm and four comparison
            algorithms on Networks 4.

            FIGURE 14. Results of the MACD-SN running on SPPN.

            The results of MACD-SN algorithm running on GGSN
            are shown in Figure 15. In Figure 15, the vertices with the
            identical color and shape are in the same cluster. There are
            three clusters in GGSN. In Figure 15, we can see that most
            positive edges are in the clusters, and all negative edges are
            between the clusters. This shows that GGSN is an
            imbalanced signed network. The outputs of MACD-SN
            algorithm is in line with the true cluster partition of the
            GGSN.

            FIGURE 13. The outputs of MACD-SN algorithm and four comparison
            algorithms on Networks 5.

            (GGSN) [43] and Slovene parliamentary party network
            (SPPN) [42], separately. Three real signed networks that
            lack true cluster partition are Slashdot network [44],
            Country network [45], and Epinions network [44].
            The SPPN shows the relationships among the Slovenian
            parliament's 10 political parties in 1994. The plus and
            minus edges separately represent the alike and unalike
            relationships between the political parties. The true cluster
            partition of the SPPN consists of two clusters. The GGSN
            was created based on Reads study on the cultures of the

            FIGURE 15. Results of the MACD-SN running on GGSN.

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            Chart 1. The average NMI values of the five algorithms.
            MACD-SN

            SSL

            SISN

            DM

            FEC

            Networks I

            1

            0.9805

            1

            0.8746

            0.7646

            Networks II

            1

            1

            0.8985

            0.5091

            0.5323

            Networks III

            1

            0.9014

            0.806

            0.6846

            0.114

            Networks IV

            0.9494

            0.8699

            0.3991

            0.1225

            0.0637

            Networks V

            0.9597

            0.8659

            0.7251

            0.1185

            0.2116

            The Country network was generated using the Correlates
            of War data set from 1996 to 1999 [45]. The vertices
            represent the nations, the plus edges denote the military
            leagues, and the minus edges represent the military
            antagonisms. In the tests, we remove insular vertices in the
            graph and all connected components except the largest
            connected components in the graph, and only retain the
            maximal connected component. This maximal component
            consists of 144 vertices and 1243 edges.
            MACD-SN algorithm puts all the vertices into eight
            clusters. The outcomes of the algorithm are presented in
            Figures 16 and 17. Figure 16 visually presents the cluster
            partition of the Country network. In Figure 16, the minor
            circles are used to represent the vertices, solid lines are used
            to represent the positive edges, and dashed lines are used to
            represent the negative edges. The vertices with the identical
            color are in the identical cluster and constitute the larger
            circle. Figure 17 shows an adjacency matrix rearranged
            based on cluster partition. In Figure 17, pink and blue dots
            represent positive and negative edges, severally. Eight
            clusters of vertices are divided by solid lines with a green
            color. The number on the right in Figure 17 is the number
            of the clusters, which have a one-to-one correspondence
            with the number near the big circles in Figure 16.

            In Figures 16 and 17, we can see that, (1)Among the
            eight clusters, clusters 2, 3, 4, 6 and 7 have dense positive
            edges. (2)Among the eight clusters, there are two clusters
            with relatively sparse positive edges, which are clusters 1
            and 8 respectively. (3)The cluster 5 is composed of many
            peripheral vertices, in which there are few edges. (4) The
            negative edges mainly exist between clusters. (5) There are
            no negative edges between the clusters 2 and 4, but there
            are a large amount of positive edges between one vertex of
            the cluster 2 and the vertices in the cluster 4. (6) There are

            FIGURE 17. Result of the MACD-SN running on Country network.

            FIGURE 16. Result of the MACD-SN running on Country network.

            few edges between clusters 6 and 7, but there are a large
            amount of positive edges between two vertices of the
            cluster 7 and the vertices in the cluster 6.
            We also compare the result of our algorithm with that of
            SSL. Figure 18 presents the output of the SSL on the
            Country signed graph. In Figure 18, we may observe that
            the running result of MACD-SN is significantly superior to
            that of SSL.
            We also run our algorithm MACD-SN on the Slashdot
            network and Epinions network [44]. Epicions is a famous
            network of consumers reviews. Users may believe or not
            believe other users reviews. Slashdot is a debate website
            where users can see other users as pals or foes. After all the
            insular vertices within the network are removed, 126828
            vertices are left in the Epinions network and 73099 in the
            Slashdot network. In our experiment, the two remaining

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            VI.

            FIGURE 18. Output of SSL running on the Country network.

            FIGURE 19. Result of the MACD-SN running on Slashdot network.

            networks were used. In the Epinions network, our algorithm
            identifies sixteen clusters. In Slashdot network, our
            algorithm identifies six clusters. Figure 19 shows an
            adjacency matrix of the Slashdot network rearranged based
            on cluster partition. In Figure 19, pink and blue dots
            represent positive and negative edges, respectively. Six
            clusters of vertices are divided by solid lines with a green
            color. According to the order from top left to bottom right
            on the main diagonal, the number of vertices in each cluster
            is 1741, 54398, 5922, 6413, 2746 and 1879, severally. In
            Figure 19, we may observe that the largest cluster with
            54398 vertices is loose, and the rest of clusters are dense.
            Among the six clusters, the clusters with 6413 vertices and
            1879 vertices include a large number of negative edges,
            while the clusters with 1741 vertices, 5922 vertices and
            2746 vertices include a large number of positive edges.
        </corps>
        <conclusion>
            In the research field of signed graphs, cluster structure is an
            important network feature. For the sake of better study and
            take advantage of the signed networks, it is crucial to
            discover their cluster partition. In this paper, we propose a
            memetic method named MACD-SN for cluster partitions in
            signed networks. The individual coding method of MACDSN algorithm adopts the well-known string-based coding
            method. In order to speed up the convergence, we proposed
            a new initialization algorithm for the cluster partitions of
            signed networks. The fitness function of MACD-SN
            algorithm uses the
            function presented in [35]. In order to
            select parent individuals for succedent genetic operations,
            we adopt a well-known operator (i.e. tournament selection
            operator), which provides chromosomes in the parent
            population identical probabilities to be chosen for
            subsequent genetic operators. In addition to the frequentlyused mutation operation, this paper also presents
            a novel
            crossover operation and a novel mutation operation. The
            novel randomized two-way crossover operation can
            preferable retain the hereditary properties of the previous
            generation individuals, and the novel community mutation
            operator may greatly enhance the population diversity.
            Moreover, this paper presents a novel local search
            subroutine, which may enhance the accuracy of the ultimate
            output of the MACD-SN and reduce its running time, and
            enable the algorithm to jump out of the local best solution
            with a specified odds and attain the global best solution. For
            verifying the detection ability of the proposed algorithm, a
            large number of tests have been executed on five kinds of
            synthetic signed graphs and five real signed graphs. Next,
            we compare the test outcomes with four well-known signed
            network cluster partition methods. The comparison
            outcomes show that the performance of MACD-SN method
            is better than the other four methods, which indicates that
            the method proposed in this paper is an excellent method to
            identify cluster partitions in signed networks. The
            disadvantage of MACD-SN algorithm is that it can't detect
            overlapping communities in signed networks. We will solve
            this problem in our future work.
        </conclusion>
        <discussion>
            Aucune discussion trouvée.
        </discussion>
        <biblio>
            [1]
            [2]
            [3]
            [4]
            [5]
            [6]

            Z. Xia, Z. Bu, Community detection based on a semantic network,
            Knowl.-Based Syst. 26 (2012) 30–39.
            P. Doreian, A. Mrvar, Partitioning signed social networks, Soc.
            Networks 31 (2009) 1–11.
            M.E.J. Newman, The structure of scientific collaboration networks,
            Proc. Natl. Acad. Sci. USA, vol. 9781400841356, pp. 221–226,
            2011.
            S. Fortunato, Community detection in graphs, Phys. Rep. 486 (3)
            (2010) 75–174.
            M. Girvan, M.E.J. Newman, Community structure in social and
            biological networks, Proc. Natl. Acad. Sci. USA 99 (12) (2002)
            7821–7826.
            K.P. Reddy, M. Kitsuregawa, P. Sreekanth, S.S. Rao, A graph based
            approach to extract a neighborhood customer community for

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            collaborative filtering, in: Databases in Networked Information
            Systems, Springer, Berlin, Heidelberg, 2002, pp. 188–200.
            [7] P. Doreian, A. Mrvar, A partitioning approach to structural balance,
            Social Networks 18 (2) (1996) 149–168.
            [8] B. Yang, X. Zhao, and X. Liu, ‘‘Bayesian approach to modeling and
            detecting communities in signed network,’’ in Proc. AAAI, Austin,
            TX, USA, 2015, pp. 1952–1958.
            [9] D. Cartwright and F. Harary, ‘‘Structural balance: A generalization
            of Heider’s theory,’’ Psychol. Rev., vol. 63, no. 5, pp. 277–293,
            1956.
            [10] J. A. Davis, ‘‘Clustering and structural balance in graphs,’’ Hum.
            Relations, vol. 20, no. 2, pp. 181–187, 1967.
            [11] M.E. Newman, Fast algorithm for detecting community structure in
            networks, Phys. Rev. E 69 (2004) 066133.
            [12] A. Clauset, M.E. Newman, C. Moore, Finding community structure
            in very large networks, Phys. Rev. E 70 (2004) 066111.
            [13] M.E.J. Newman, Modularity and community structure in networks,
            Proceedings of the National Academy of Sciences of the United
            States of America 103 (2006) 8577–8582.
            [14] C. Shi, Z. Yan, Y. Cai, B. Wu, Multi-objective community detection
            in complex networks, Appl. Soft Comput. 12 (2012) 850–859.
            [15] M. Gong, L. Ma, Q. Zhang, L. Jiao, Community detection in
            networks by using multiobjective evolutionary algorithm with
            decomposition, Phys. A Stat.Mech. Appl. 391 (15) (2012) 4050–
            4060.
            [16] D. Chen, F. Zou, R. Lu, L. Yu, Z. Li, J. Wang, Multi-objective
            optimization of community detection using discrete teachinglearning-based optimization with decomposition,
            Inf. Sci. 369 (2016)
            402–418.
            [17] F. Zou, D. Chen, S. Li, R. Lu, M. Lin, Community detection in
            complex networks: multi-objective discrete backtracking search
            optimization algorithm with decomposition, Appl. Soft Comput. 53
            (2017) 285–295.
            [18] C. Pizzuti, A multiobjective genetic algorithm to find communities in
            complex networks, IEEE Trans. Evol. Comput. 16 (2012) 418–430.
            [19] Jerome Kunegis, Stephan Schmidt, Andreas Lommatzsch, J urgen
            Lerner, Ernesto W. DeLuca, and Sahin Albayrak. Spectral analysis
            of signed graphs for clustering, prediction and visualization. In
            Proceedings of the SIAM International Conference on Data Mining,
            2010,559-570.
            [20] Kai-Yang Chiang, Cho-Jui Hsieh, Nagarajan, Inderjit S. Dhillon,
            Ambuj Tewari. Prediction and Clustering in Signed Networks: A
            Local to Global Perspective. Journal of Machine Learning Research,
            2014, 15: 1177-1213.
            [21] S. Gómez, P. Jensen and A. Arenas. Analysis of community
            structure in networks of correlated data. Phys. Rev. E, 80(1):
            016114, 2009.
            [22] V.A. Traag, Jeroen Bruggeman, Community detection in networks
            with positive and negative links, Phys. Rev. E 80 (3) (2009) 036115.
            [23] H.W. Shen, Community Structure: An Introduction, Springer, Berlin,
            Heidelberg, 2013.
            [24] R. Figueiredoa, G. Moura, Mixed integer programming
            formulations for clustering problems related to structural balance,
            Social Networks 35 (4) (2013) 639–651.
            [25] P. Anchuri, M.M. Ismail, Communities and balance in signed
            networks: A spectral approach, in: 2012 IEEE/ACM International
            Conference on Advances in Social Networks Analysis and Mining,
            IEEE Computer Society, 2012, pp. 235–242.
            [26] K.Y. Chiang, J.J. Whang, I.S. Dhillon, Scalable clustering of signed
            networks using balance normalized cut, in: Proceedings of the 21st
            ACM International Conference on Information and Knowledge
            Management, ACM, 2012, pp. 615–624.
            [27] A. Amelio, C. Pizzuti, Community mining in signed networks: a
            multiobjective approach, in: Proceedings of the 2013 IEEE/ACM
            International Conference on Advances in Social Networks Analysis
            and Mining, ACM, 2013, pp. 95–99.
            [28] Y. Li, J. Liu, C. Liu, A comparative analysis of evolutionary and
            memetic algorithms for community detection from signed networks,
            Soft Comput. 18 (2) (2014) 329–348.

            [29] B. Yang, W. Cheung, and J. Liu, ‘‘Community mining from signed
            social networks,’’ IEEE Trans. Knowl. Data Eng., vol. 19, no. 10,
            pp. 1333–1348, Oct. 2007.
            [30] Q. Cai, M. Gong, S. Ruan, Q. Miao, and H. Du, ‘‘Network
            structural balance based on evolutionary multiobjective optimization:
            A two-step approach,’’ IEEE Trans. Evol. Comput., vol. 19, no. 6,
            pp. 903–916, Dec. 2015.
            [31] C. Liu, J. Liu, and Z. Jiang, ‘‘A multiobjective evolutionary
            algorithm based on similarity for community detection from signed
            social networks,’’ IEEE Trans. Cybern., vol. 44, no. 12, pp. 2274–
            2287, Dec. 2014.
            [32] J. Huang, H. Sun, Y. Liu, Q. Song, and T. Weninger, ‘‘Towards
            online multiresolution community detection in large-scale
            networks,’’ PLoS ONE, vol. 6, no. 8, p. e23829, Aug. 2011.
            [33] J. Q. Jiang, ‘‘Stochastic block model and exploratory analysis in
            signed networks,’’ Phys. Rev. E, Stat. Phys. Plasmas Fluids Relat.
            Interdiscip. Top., vol. 91, no. 6, p. 062805, Jun. 2015.
            [34] R. Harakawa, T. Ogawa, and M. Haseyama, ‘‘Extracting
            hierarchical structure of web video groups based on sentimentaware signed network analysis,’’ IEEE Access,
            vol. 5, pp. 16963–
            16973, Aug. 2017.
            [35] S. Gómez, P. Jensen, and A. Arenas, “Analysis of community
            structure in networks of correlated data,” Phys. Rev. E., vol. 80, no.
            1, p. 016114, 2009.
            [36] D. Goldberg, Genetic Algorithms in Search, Optimization and
            Machine Learning, Addison-Wesley, Reading, MA, 1989.
            [37] C. Pizzuti, A multiobjective genetic algorithm to find communities in
            complex networks, IEEE Trans. Evol. Comput. 16 (3) (2012) 418–
            430.
            [38] B. Yang , W. Cheung , J. Liu , Community mining from signed
            social networks, IEEE Trans. Knowl. Data Eng. 19 (10) (2007)
            1333–1348 .
            [39] B. Yang , X. Liu , Y. Li , et al. , Stochastic blockmodeling and
            variational Bayes learning for signed network analysis, IEEE Trans.
            Knowl. Data Eng. 29 (9) (2017) 2026–2039 .
            [40] P. Doreian , A. Mrvar , A partitioning approach to structural balance,
            Soc. Netw. 18 (2) (1996) 14 9–16 8 .
            [41] X. Zhao , B. Yang , X. Liu , H. Chen , Statistical inference for
            community detection in signed networks, Phys. Rev. E 95 (4) (2017)
            042313 .
            [42] S. Kropivnik , A. Mrvar , An analysis of the slovene parliamentary
            parties network, Dev. Stat. Methodol. (1996) 209–216 .
            [43] K.E. Read , Cultures of the central highlands, new guinea,
            Southwest. J. Anthropol. 10 (1) (1954) 1–43 .
            [44] J. Leskovec , D. Huttenlocher , J. Kleinberg , Signed networks in
            social media, in: Proceedings of the SIGCHI Conference on Human
            Factors in Computing Systems, ACM, 2010, pp. 1361–1370 .
            [45] P. Doreian , A. Mrvar , Structural balance and signed international
            relations, J. Soc. Struct. 16 (2015) 1–49 .
            [46] Hua J.,Yu j., and Yang M., “Fast clustering for signed graphs based
            on random walk gap”, Social Networks, vol. 60, pp. 113-128, 2020.
            [47] Brusco M. and Doreian P., “Partitioning signed networks using
            relocation heuristics, tabu search, and variable neighborhood
            search”, Social Networks, vol. 56, pp. 70-80, 2019.
            [48] Attea B., Rada H., Abbas M., and Özdemir, S., “A new evolutionary
            multi-objective community mining algorithm for signed networks”,
            Applied Soft Computing Journal, vol. 85, 2019.
            [49] Zhu X., Ma Y., and Liu Z., “A novel evolutionary algorithm on
            communities detection in signed networks”, Physica A: Statistical
            Mechanics and its Applications, vol. 503, pp. 938-946, 2018.
            [50] Ping S., Liu D., Yang B., Zhu Y., Chen H., and Wang Z.,
            “Community Detection in Signed Networks Based on the Signed
            Stochastic Block Model and Exact ICL”, IEEE Access, vol. 7, 2019.
            [51] Chen J.,Liji U., Wang H., and Yan Z., “Community Mining in
            Signed Networks Based on Dynamic Mechanism”, IEEE Systems
            Journal, vol. 13, no. 1, 2019.
            [52] Yan C. and Chang Z., “Modularized convex nonnegative matrix
            factorization for community detection in signed and unsigned
            networks”, Physica A: Statistical Mechanics and its Applications,
            vol. 539, 2020.

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.

            This article has been accepted for publication in a future issue of this journal, but has not been fully
            edited. Content may change prior to final publication. Citation information: DOI
            10.1109/ACCESS.2020.3006108, IEEE Access
            Author Name: Preparation of Papers for IEEE Access (February 2017)

            [53] Liu X., Song W., Musial K., Zhao X., Zuo W. and Yang B., “Semisupervised stochastic blockmodel for
            structure analysis of signed
            networks”, Knowledge-Based Systems, 2020.
            [54] Liu D., Zhang Y., Liang R., Li B., and Xia Z., “Signed Network
            Community Mining Based on Fine-grained Signed Stochastic Block
            Model”, 2019 2nd International Conference on Artificial
            Intelligence and Big Data (ICAIBD), 2019.
            [55] Bonchi F., Galimberti E., Gionis A., Ordozgoiti B., and Ruffo G.,
            “Discovering Polarized Communities in Signed Networks”, CIKM
            '19: Proceedings of the 28th ACM International Conference on
            Information and Knowledge Management, Beijing, China, pp. 961970, 2019.
            [56] Hu B., Wang H., Yu X., Yuan W. and He T., “Sparse network
            embedding for community detection and sign prediction in signed
            social networks”, Journal of Ambient Intelligence and Humanized
            Computing, vol. 10, no. 1, pp. 175-186, 2019.
            [57] Chen J., Liu D., Hao F. and Wang H., “Community detection in
            dynamic signed network: an intimacy evolutionary clustering
            algorithm”, Journal of Ambient Intelligence and Humanized
            Computing, vol. 11, no. 2, pp. 891-900, 2020.
            [58] He X., Du H., Du W., and Feldman M., “A Community Structure in
            Fully Signed Static Networks”, Hsi-An Chiao Tung Ta
            Hsueh/Journal of Xi'an Jiaotong University, vol. 52, no. 2, pp. 4551, 2018.
            [59] Wang S., Hu G., Pan Z., Zhang J., and Li D., “A game-theoretic
            approach for community detection in signed networks”, IEICE
            Transactions on Fundamentals of Electronics, Communications and
            Computer Sciences, vol. E102A, no. 6, pp. 796-807, 2019.
            [60] Li Z., Chen J., Fu Y., Hu G., Pan Z., and Zhang L., “Community
            Detection Based on Regularized Semi-Nonnegative Matrix TriFactorization in Signed Networks”, Mobile Networks
            and
            Applications, vol. 23, no. 1, pp. 71-79, 2018.
            [61] Zhang Y., Liu Y., Ma X., and Song J., “Community detection in
            signed networks by relaxing modularity optimization with
            orthogonal and nonnegative constraints”, Neural Computing and
            Applications, 2019.
            [62] Hu B., Wang H., and Zheng Y., “Sign prediction and community
            detection in directed signed networks based on random walk theory”,
            International Journal of Embedded Systems, vol. 11, no. 2, pp. 200209, 2019.
            [63] Girdhar N., and Bharadwaj K., “Community Detection in Signed
            Social Networks Using Multiobjective Genetic Algorithm”, Journal
            of the Association for Information Science and Technology, vol 70,
            no. 8, pp. 788-804, 2019.
            [64] Zahedinejad E., Crawford D., Adolphs C., and Oberoi J., “Multiple
            Global Community Detection in Signed Graphs”, 4th Future
            Technologies Conference, FTC 2019, San Francisco, CA, United
            states, pp. 688-707, 2019.
            [65] Wu Y., Chao P., Ying W., He L., and Chen S., “A Conical Area
            Evolutionary Algorithm Based on Modularity Q for Community
            Detection from Signed Networks”, 20th IEEE International
            Conference on Computational Science and Engineering and 15th
            IEEE/IFIP International Conference on Embedded and Ubiquitous
            Computing, CSE and EUC 2017, Guangzhou, Guangdong, China,
            pp. 57-62, 2017.
            [66] Cai M., Shen B., Ma L., Jiao L., “Discrete particle swarm
            optimization for identifying community structures in signed social
            networks”, Neural Networks, vol. 58, pp. 4-13, 2014.
            [67] Ruby, Kaur I., “An advanced automated approach for community
            mining in signed social networks”, 2017 International Conference
            on Energy, Communication, Data Analytics and Soft Computing,
            ICECDS 2017, Chennai, India, pp. 665-670, 2017.
            [68] Y. Ma, X. Zhu, Q. Yu, “Clusters detection based leading
            eigenvector in signed networks”, Physica A: Statistical Mechanics
            and its Applications, vol. 523, pp. 1263-1275, 2019.
            [69] Z. Liu, Y. Ma, X. Wang, “A Compression-Based Multi-Objective
            Evolutionary Algorithm for Community Detection in Social
            Networks”, IEEE Access, vol. 8, 2020.
            [70] C. Yan, Z. Chang, “Modularized convex nonnegative matrix
            factorization for community detection in signed and unsigned
            networks”, Physica A: Statistical Mechanics and its Applications,
            vol. 539, 2020.

            Shiwei Che is currently a Ph.D. candidate in the
            Department of Computer Science and
            Technology, Harbin Engineering University. He
            received his M.E. degree in 2010 from the
            Department of Computer Science and
            Technology of Xinjiang University, Xinjiang,
            China. His main research interests include social
            networks and community detection.

            Wu Yang received a Ph.D. degree in Computer
            System Architecture Specialty of Computer
            Science and Technology School from Harbin
            Institute of Technology. He is currently a
            professor and doctoral supervisor of Harbin
            Engineering University. His main research
            interests include wireless sensor network, peerto-peer network and information security. He is
            a member of ACM and senior member of CCF.

            Wei Wang received a Ph.D. degree in Computer
            System Architecture Specialty of Computer
            Science and Technology School from Harbin
            Institute of Technology. He is currently an
            professor in Harbin Engineering University. His
            main research interests include social networks
            and community detection.

            VOLUME XX, 2017

            9

            This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see
            https://creativecommons.org/licenses/by/4.0/.
        </biblio>
    </article>
    <article>
        <preamble>An_Improved_Branch-and-Cut_Code_for_the_Maximum_Balanced_Subgraph_of_a_Signed_Graph.pdf</preamble>
        <titre>An Improved Branch-and-Cut Code for the Maximum Balanced Subgraph of a Signed Graph</titre>
        <auteur>
            Rosa Figueiredo (CIDMA, Department of Mathematics, University of Aveiro, Portugal),
            Yuri Frota (Department of Computer Science, Fluminense Federal University, Brazil)
        </auteur>
        <abstract>
            The Maximum Balanced Subgraph Problem (MBSP) is the problem of finding a
            subgraph of a signed graph that is balanced and maximizes the cardinality of its
            vertex set. We are interested in the exact solution of the problem: an improved
            version of a branch-and-cut algorithm is proposed. Extensive computational
            experiments are carried out on a set of instances from three applications previously discussed in the
            literature as well as on a set of random instances.
            Keywords: Balanced signed graph; Branch-and-cut; Portfolio analysis; Network matrix; Community structure.

            ∗ Corresponding author. Fax number: +351 234370066 Email: rosa.figueiredo@ua.pt
            Rosa Figueiredo is supported by FEDER founds through COMPETE-Operational Programme
            Factors of Competitiveness and by Portuguese founds through the CIDMA (University
            of Aveiro) and FCT, within project PEst-C/MAT/UI4106/2011 with COMPETE number
            FCOMP-01-0124-FEDER-022690.

            Preprint submitted to Elsevier

            December 17, 2013
        </abstract>
        <introduction>
            1. Introduction
            Let G = (V, E) be an undirected graph where V = {1, 2, . . . , n} is the set
            of vertices and E is the set of edges connecting pairs of vertices. Consider a
            function s : E → {+, −} that assigns a sign to each edge in E. An undirected
            graph G together with a function s is called a signed graph. An edge e ∈ E is
            called negative if s(e) = − and positive if s(e) = +.
            In the last decades, signed graphs have shown to be a very attractive discrete
            structure for social network researchers [1, 8, 9, 16, 21] and for researchers
            in other scientific areas, including portfolio analysis in risk management [14,
            15], biological systems [7, 15], efficient document classification [3], detection
            of embedded matrix structures [12] and community structure [17, 20]. The
            common element among all these applications is that all of them are defined
            in a collaborative vs. conflicting environment represented over a signed graph.
            We refer the reader to [22] for a bibliography of signed graphs. Is this work we
            consider the Maximum balanced subgraph problem (MBSP) defined next.
            Let G = (V, E, s) denote a signed graph and let E − and E + denote, respectively, the set of negative and
            positive edges in G. Also, for a vertex set S ⊆ V ,
            let E[S] = {(i, j) ∈ E | i, j ∈ S} denote the subset of edges induced by S. A
            signed graph G = (V, E, s) is balanced if its vertex set can be partitioned into
            sets W (possibly empty) and V \ W in such a way that E[W ] ∪ E[V \ W ] = E + .
            Given a signed graph G = (V, E, s), the MBSP is the problem of finding a
            subgraph H = (V 0 , E 0 , s) of G such that H is balanced and maximizes the
            cardinality of V 0 .
            The MBSP is known to be an NP-hard problem [6] although the problem
            of detecting balance in signed graphs can be solved in polynomial time [13]. In
            the literature, the MBSP has already been applied in the detection of embedded
            matrix structures [10, 11, 12], in portfolio analysis in risk management [10] and
            community structure [10].
            The problem of detecting a maximum embedded reflected network (DMERN)
            is reduced to the MBSP in [12]. Most of the existing solution approaches to the
            MBSP were in fact proposed for the solution of the DMERN problem. The
            literature proposes various heuristics for the solution of the DMERN problem
            (for references see [12]). Lately, Figueiredo et al. [11] developed the first exact solution approach for
            the MBSP: a branch-and-cut algorithm based on the
            signed graph reformulation from Gulpinar et al. [12] for the DMERN problem.
            Computational experiments were carried out over a set of instances found in
            the literature as a test set for the DMERN problem. Almost all these instances
            were solved to optimality in a few seconds showing that they were not appropriate for assessing the quality
            of a heuristic approach to the problem. Recently,
            Figueiredo et al. [10] introduced applications of the MBSP in other two different
            research areas: portfolio analysis in risk management and community structure.
            These authors also provided a new set of benchmark instances of the MBSP
            (including a set of difficult instances for the DMERN problem) and contributed
            to the efficient solution of the problem by developing a pre-processing routine,
            an efficient GRASP metaheuristic, and improved versions of a greedy heuristic
        </introduction>
        <corps>
            2

            proposed in [12].
            In this work we contribute to the efficient solution of the MBSP by developing
            an improved version of the branch-and-cut algorithm proposed by Figueiredo et
            al. [11]. We introduce a new branching rule to the problem based on the odd
            negative cycle inequalities. Moreover, we improve the cut generation component
            of the branch-and-cut algorithm by implementing new separation routines and
            by using a cut pool separation strategy.
            The remainder of the paper is structured as follows. The integer programming formulation and the
            branch-and-cut algorithm proposed in [11] to the
            MBSP are outlined in Section 2. The improved version of the branch-and-cut
            algorithm is described in Section 3. In Section 4, computational results are
            reported for random instances as well as for instances of the three applications
            previously mentioned. In Section 5 we present concluding remarks.
            We next give some notations and definitions to be used throughout the paper.
            For an edge set B ⊆ E, let G[B] denote the subgraph of G induced by B. A
            set K ⊆ V is called a clique if each pair of vertices in K is joined by an edge.
            A set I ⊆ V is called a stable set if no pair of vertices in I is joined by an edge.
            We represent a cycle by its vertex set C ⊆ V . In this text, a signed graph is
            allowed to have parallel edges but no loops. Also, we assume that parallel edges
            have always opposite signs.
            2. Integer programming formulation and branch-and-cut
            The integer programming formulation and the branch-and-cut algorithm introduced in [11] are described next.
            2.1. Integer programming formulation
            It is well known that a signed graph is balanced if and only if it does not
            contain a parallel edge or a cycle with an odd number of negative edges [5, 12,
            22]. Let C o (E) be the set of all odd negative cycles in G, i.e., cycles with no
            parallel edges and with an odd number of negative edges. Throughout this text,
            a cycle C ∈ C o (E) is called an odd negative cycle. The formulation uses binary
            decision variables y ∈ {0, 1}|V | defined in the following way. For all i ∈ V , yi
            is equal to 1 if vertex i ∈ V belongs to the balanced subgraph, and is equal
            to 0 otherwise.
            We use the vector notation y = (yi ), i ∈ V , and the notation
            P
            y(V 0 ) = i∈V 0 yi for V 0 ⊆ V . The formulation follows.
            Maximize y(V )

            (1)
            −

            subject to yi + yj ≤ 1,
            y(C) ≤ |C| − 1,
            yi ∈ {0, 1},

            +

            ∀ (i, j) ∈ E ∩ E ,

            (2)

            ∀ C ∈ C o (E),

            (3)

            ∀ i ∈ V.

            (4)

            Consider a parallel edge (i, j) ∈ E − ∩ E + . Constraints (2) ensure vertices i and
            j cannot belong together to the balanced subgraph. Constraints (3), called odd
            negative cycle inequalities, forbid cycles with an odd number of negative edges
            3

            in the subgraph described by variables y. These constraints force variables y
            to define a balanced subgraph. Finally, the objective function (1) looks for a
            maximum balanced subgraph. The formulation has n variables and, due to
            constraints (3), might have an exponential number of constraints. Let us refer
            to this formulation as Y (G, s). By changing the integrality constraints (4) in
            formulation Y (G, s) by the set of trivial inequalities 0 ≤ yi ≤ 1, i ∈ V , we
            obtain a linear relaxation to the MBSP.
            2.2. A branch-and-cut algorithm
            The branch-and-cut algorithm developed in [11] is based on formulation
            Y (G, s), uses a standard 0–1 branching rule and has three basic components:
            the initial formulation, the cut generation and the primal heuristic.
            Initial formulation. The initial formulation is defined as
            maximize y(V )
            subject to y(K) ≤ 1,

            ∀ K ∈ L,

            (5)

            ∀ C ∈ M ⊆ C o (E),

            (6)

            y(K) ≤ 2,

            ∀ K ∈ N,

            (7)

            0 ≤ yi ≤ 1,

            ∀ i ∈ V,

            (8)

            y(C) ≤ |C| − 1,

            where (5) are clique inequalities from the stable set problem [19] defined over a
            set of cliques L in G[E + ∩ E − ]; (6) is a subset of inequalities (3) defined over
            a set of odd negative cycles M ; (7) is a subset of inequalities from a family of
            negative clique inequalities introduced in [11] for the MBSP and defined over a
            set of cliques N in G[E − ]; (8) is the set of trivial inequalities. Greedy procedures
            described in [11] are used to generate sets L, M and N .
            Cut generation. After an LP has been solved in the branch-and-cut tree, the
            algorithm check if the solution is integer feasible. If this is not the case, the cut
            generation procedure is called and a set of separation routines is executed (a
            limit of 100 cuts per iteration is set). If no violated inequality is found or if a limit
            of 10 cut generations rounds is reached, the algorithm enter in the branching
            phase. The cut generation component described in [11] has two separation
            procedures. An exact separation procedure is used to generate violated odd
            negative cycle inequalities (3). This separation routine is based on a polynomial
            algorithm described in [4] to solve the separation problem for cut inequalities. A
            heuristic separation procedure defined in [11] is used to generate violated clique
            inequalities also introduced in [11].
            Primal heuristic and branching rule. A rounding primal heuristic is executed in [11] every time a fractional
            solution is found. Moreover, a standard 0–1
            branching rule is used with the same branching priority assigned to each variable
            and the branch-and-cut tree is investigated with the best-bound-first strategy.
            The authors reported they have also implemented a version of the branching

            4

            rule proposed in [2]. Although this branching rule has been successfully applied
            to solve the stable set problem, they obtained better results with the standard
            0–1 branching rule.
            3. An improved branch-and-cut code
            In this work, the following new routines were added to the branch-and-cut
            algorithm described in Section 2.
            Branching on the odd negative cycle inequalities. Our branching rule
            is based on the odd negative cycle inequalities (3). The intuition behind this
            cycle based branching is the attempt to generate more balanced enumerative
            trees. The standard 0–1 branching rule can be very asymmetrical producing
            unbalanced enumerative trees.
            Let ȳ ∈ R be the optimal fractional solution of a node in the search tree. Let
            C 0 ⊆ C o (E) be the subset of odd negative cycles such that each cycle C ∈ C 0
            satisfy the following conditions:
            • constraint (3) defined by C 0 is a binding one in the current formulation,
            • there exists a vertex i ∈ C 0 such that ȳi is fractional.
            The standard 0–1 branching rule is used whenever C 0 is an empty set. If it
            is not the case, let C̄ be the smallest cycle in C 0 . Split C̄ into the sets C̄ 1 and
            C̄ 2 such that C̄ = C̄ 1 ∪ C̄ 2 , C̄ 1 ∩ C̄ 2 = ∅ and y(C̄ 1 ) is fractional. We create
            three branches in the search tree:
            (i) y(C̄ 1 ) ≤ |C̄ 1 | − 1 and y(C̄ 2 ) = |C̄ 2 |;
            (ii) y(C̄ 1 ) = |C̄ 1 | and y(C̄ 2 ) ≤ |C̄ 2 | − 1;
            (iii) y(C̄ 1 ) ≤ |C̄ 1 | − 1 and y(C̄ 2 ) ≤ |C̄ 2 | − 1.
            Separation routines. In this work, we introduce two new separation procedures to the cut generation
            component of the branch-and-cut algorithm described in Section 2.
            The authors in [11] proved that lifted odd hole inequalities (from the stable
            set problem) defined over the set of parallel edges E + ∩ E − are valid inequalities for the MBSP. They have
            also proved that, if the support graph of these
            inequalities satisfy certain conditions they are facet defining inequalities to the
            problem. We implemented a separation procedure described in [18] to the lifted
            odd hole inequalities. Also, the authors indicated in [11] that a very similar lifting procedure could be
            applied to strengthen constraints (3). We implemented
            this lifting procedure to the odd negative cycle inequalities satisfying |C| ≤ 20.
            In both cases, a very small instance of the MBSP must be solved at each iteration of the lifting procedures.
            In our implementation, these small problems
            were solved by simple enumerative algorithms.

            5

            Moreover, we added a cut pool to the branch-and-cut code: any violated
            inequality included to the active formulation of a node in the branch-and-cut
            tree is also included to the cut pool. As we have mentioned in Section 2, after an
            LP has been solved in the branch-and-cut tree, we check if the solution is integer
            feasible. If this is not the case, the cut generation procedure is then called.
            Before running any separation routine from our cut generation procedure, we
            check if there are violated cuts in the cut pool. In positive case, no separation
            routine is called and the violated cuts (limited to 100 cuts) are immediately
            added to the active formulation.
            4. Computational experiments
            We implemented the improved branch-and-cut algorithm described in Section 3 using the formulation defined by
            (5)-(8). Both branch-and-cuts (BC), the
            previous one and the improved version, were implemented in C++ running on
            a Intel(R) Pentium(R) 4 CPU 3.06 GHz, equipped with 3 GB of RAM. We use
            Xpress-Optimizer 20.00.21 to implement the components of these enumerative
            algorithms. The maximum running time per instance was set at 3600 seconds.
            The same instance classes reported in [10] were tested here to allow for a better
            comparison of the performances of the improved BC and the BC algorithm proposed earlier. The class Random
            consists of 216 randomized instances divided
            into two groups: Group 1 without parallel edges and Group 2 with parallel
            edges. The class UNGA is composed of 63 instances derived from the community structure of networks
            representing voting on resolutions in the United
            Nations General Assembly. The class new DMERN consists of 316 signed graphs
            coming from a set of general mixed integer programs. Finally, the class Portifolio is composed by 850
            instances generated from market graphs. The entire
            benchmark is available for download in www.ic.uff.br/∼yuri/mbsp.html.
            We first investigate the behavior of the Random instances, the results obtained by the two methods are
            summarized in Table 1. This table exhibits, for
            both groups, average times per | V |, and percentage gaps per | V |, d (density
            of the graph) and the rates | E − | / | E + | and | E + ∩ E − |. Multicolumn
            Time, gives us average times (in seconds) spent to solve instances to optimality; the values in brackets
            show the number of instances solved to optimality
            (“-” means no instance was solved within the time limit). Multicolumn %Gap
            presents the average of percentage gaps calculated over the set of unsolved instances. The percentage gap of
            each instance is calculated between the best
            integer solution found and the final upper bound. For each group of instances,
            the first and the second lines present, respectively, the results obtained with the
            original and the improved code of the branch-and-cut algorithm. The results
            obtained with the improved version are slightly better: six more instances were
            solved to optimality and all the average gaps were reduced.
            In the second experiment, we analyze the performance of the Portifolio instances. Table 2 reports the
            obtained results. The first two columns give the
            number of vertices and a threshold value t used to generate the instances [10].
            The next three columns give the average time, the average of percentage gaps
            6

            (as defined in Table 1) and the number of evaluated nodes in the original BC
            tree, respectively. The last three columns give the same data for the improved
            BC. Algorithm improved BC solved 227 out of 850 instances within 1 hour of
            processing time, while the original BC managed to solve only 217 instances. The
            average gap for the original BC over the set of unsolved instances is 17.91%,
            while the same value for the improved version is 9.41%. Furthermore, Figure 1
            shows that the improved BC presents tighter gaps for almost the entire set of
            Portifolio instances than the original one.
            In the third experiment, we investigate the behavior of the UNGA instances.
            We notice that these instances are extremely easy to solve. No matter the
            number of vertices or the parameters used to compose the instance, both BC
            codes were always able to solve all of them in a few seconds and in the root
            of the branch-and-bound tree. So, we could not draw any conclusion from this
            class of instance.
            In our last experiment, both methods were applied to each one of the 316 new
            DMERN instances [10]. Table 3 shows the results for the instances remaining
            unsolved and the instances solved to optimality in more than one minute. The
            first three columns in this table give us information about the instances: the
            Netlib instance name, the number of vertices and the number of edges. The
            next three columns give the number of negative, positive and parallel edges,
            respectively. Similarly to the previous table, the next set of three columns gives
            us information about the solution obtained with the original BC code: the time,
            the percentage gap, and the total number of nodes in the branch-and-bound tree.
            The last three columns give the same data for the improved BC. From this set
            of instances, we can extract 25 instances not solved to optimality by the original
            BC code with average gap of 11.42% of unsolved instances, while the improved
            BC could not solve 21 instances but with a much tighter average gap of 4.85%.
            One can notice that the implementation of new separation routines and a new
            branching rule used in the improved BC led to a better performance and a high
            number of evaluated nodes within the time limit.
            5. Final remarks
            In this work, we proposed an improved branch-and-cut algorithm based on
            the integer programming formulation and the BC algorithm proposed in [11],
            together with a new branching rule based on the odd negative cycle inequalities
            and improved cutting plane routines and strategies. The instance classes reported in [10] were used to
            compare the performances of the improved BC and
            the original BC algorithm proposed in [11]. The results obtained by the new approach were superior to those
            given by the previously existing branch-and-cut.
            The new method solved 431 out of 1445 instances within 1 hour of processing
            time, while the original algorithm managed to solve only 410 instances. Moreover, as we saw in Section 4,
            considering only the set of unsolved instances, the
            average gap obtained with the improved BC was smaller than the average gap
            obtained with the original BC from [11].

            7

            8

            Group 2

            Group 1

            Instances
            200
            −
            −
            −
            −

            50
            0
            0
            0
            0

            100
            37.05
            26.62
            6.17
            4.84

            |V |
            150
            104.55
            92.09
            49.08
            44.07
            200
            153.42
            144.34
            111.83
            104.36

            .25
            75.48
            65.26
            33.48
            30.74

            d
            .50
            88.03
            81.27
            56.28
            50.92

            .75
            82.83
            76.36
            65.78
            61.97

            %Gap
            .50
            75.84
            67.16
            −
            −

            |E − |/|E + |
            1
            2
            86.01 80.31
            76.48 74.27
            −
            −
            −
            −

            Table 1: Results obtained on random instances in Group 1 (E − ∩ E + = ∅) and in Group 2 (E − ∩ E + 6= ∅).

            50
            24.22(27)
            10.63(27)
            2.41(27)
            2.37(27)

            Time
            |V |
            100
            150
            2578.00(3)
            −
            1728.33(9)
            −
            473.90(21)
            1277.67(9)
            323.33(21)
            910.78(9)

            (|E − ∩ E + |)/|E|
            .25
            .50
            .75
            −
            −
            −
            −
            −
            −
            68.69 42.22 21.35
            63.84 38.71 18.74

            Figure 1: Results obtained on portfolio instances.

            Instance
            |V |
            t
            330
            0.300
            0.325
            0.350
            0.375
            0.400
            360
            0.300
            0.325
            0.350
            0.375
            0.400
            390
            0.300
            0.325
            0.350
            0.375
            0.400
            420
            0.300
            0.325
            0.350
            0.375
            0.400
            450
            0.300
            0.325
            0.350
            0.375
            0.400
            480
            0.300
            0.325
            0.350
            0.375
            0.400
            510
            0.300
            0.325
            0.350
            0.375
            0.400

            Original BC
            Time
            %Gap
            25.00(2)
            10.66
            295.25(8)
            4.61
            13.00(10)
            1.50(10)
            1.00(10)
            1145.67(3)
            19.24
            170.75(4)
            4.05
            161.10(10)
            3.10(10)
            1.10(10)
            141.00(1)
            29.52
            255.50(4)
            17.15
            81.71(7)
            2.40
            4.30(10)
            1.30(10)
            30.56
            1062.50(2)
            13.63
            176.14(7)
            12.04
            192.10(10)
            7.40(10)
            35.86
            342.00(1)
            14.75
            444.00(8)
            2.40
            18.10(10)
            2.40(10)
            2065.00(1)
            42.69
            1746.33(2)
            27.53
            385.20(5)
            10.33
            43.22(9)
            1.20
            23.90(10)
            2809.00(1)
            49.59
            392.00(2)
            34.39
            47.00(3)
            12.36
            101.29(7)
            1.05
            6.60(10)
            (217)
            17.91

            Nodes
            890.70
            467.40
            13.60
            1.80
            1.00
            561.90
            611.90
            100.90
            2.20
            1.40
            498.80
            461.80
            372.80
            2.40
            1.10
            401.70
            432.30
            285.90
            131.70
            15.60
            313.70
            360.40
            241.70
            8.40
            1.30
            243.60
            321.40
            288.70
            105.30
            25.90
            199.50
            217.40
            242.30
            299.70
            4.00

            Improved BC
            Time
            %Gap
            183.33(3)
            4.56
            83.13(8)
            2.82
            21.30(10)
            1.80(10)
            1.00(10)
            195.67(3)
            6.48
            331.00(5)
            2.39
            129.90(10)
            3.90(10)
            1.20(10)
            650.50(2)
            10.74
            101.25(4)
            4.41
            29.14(7)
            1.84
            5.20(10)
            1.40(10)
            15.86
            1442.33(3)
            8.24
            116.29(7)
            3.98
            155.20(10)
            4.40(10)
            14.45
            124.00(1)
            5.24
            390.89(9)
            2.56
            24.00(10)
            2.70(10)
            740.00(1)
            30.20
            546.33(3)
            13.66
            218.80(5)
            3.43
            170.90(10)
            7.30(10)
            943.50(2)
            33.17
            459.00(2)
            19.92
            59.67(3)
            3.70
            670.89(9)
            0.53
            7.60(10)
            (227)
            9.41

            Table 2: Results obtained on portfolio instances.

            9

            Nodes
            933.50
            431.60
            34.80
            2.70
            1.00
            581.20
            914.20
            135.50
            4.40
            1.50
            472.30
            511.40
            551.30
            4.40
            1.70
            395.70
            548.30
            322.60
            201.10
            11.50
            330.40
            375.80
            248.20
            17.20
            1.00
            261.10
            298.10
            318.80
            83.40
            7.00
            182.60
            244.70
            315.70
            563.90
            4.40

            10

            Name
            danoint
            bienst1
            stein45
            disctom
            fc.60.20.1
            air05
            neos17
            p100x588
            air04
            r80x800
            nug08
            p50x864
            dsbmip
            n5-3
            neos21
            neos23
            n4-3
            dano3mip
            n8-3
            roll3000
            neos20
            p200x1188c
            p200x1188
            janos-us-ca–D-D-M-N-C-A-N-N
            pioro40–D-B-M-N-C-A-N-N
            n13-3
            n2-3
            qap10
            ns1688347
            ns25-pr3
            ns4-pr3
            ns60-pr3
            nu120-pr3
            nu25-pr3
            nu4-pr3
            nu60-pr3
            germany50–U-U-M-N-C-A-N-N
            protfold
            cap6000
            n7-3
            n9-3
            acc-1
            n3-3
            zib54–D-B-E-N-C-A-N-N
            n12-3
            neos818918
            germany50–D-B-M-N-C-A-N-N
            acc-2
            ta2–U-U-M-N-C-A-N-N
            n6-3
            berlin
            neos11
            ta2–D-B-M-N-C-A-N-N
            acc-6
            acc-5
            mkc
            mod011
            acc-3
            acc-4
            brasil
            p500x2988c
            p500x2988
            rentacar
            neos1
            seymour1
            seymour
            n370a
            manna81
            neos12

            m−
            497
            1981
            10701
            30000
            521
            30257
            117370
            625
            55592
            1026
            13952
            895
            2264
            5472
            37373
            22295
            7670
            14948
            6258
            25022
            10788
            1228
            1256
            5491
            5777
            7579
            7935
            35200
            24983
            1393
            1393
            1393
            1393
            1393
            1393
            1393
            1143
            30219
            10297
            12220
            16280
            30912
            18602
            6991
            12956
            6485
            6325
            43842
            2582
            14664
            2703
            33685
            9090
            55567
            54569
            3503
            8186
            49812
            52301
            3363
            3650
            3064
            7916
            41850
            604007
            604007
            15000
            72900
            302967

            m+
            903
            567
            0
            0
            530
            0
            0
            845
            0
            974
            0
            977
            1383
            5278
            0
            1092
            7671
            31003
            5398
            31630
            3851
            1742
            1714
            6160
            4466
            7146
            6921
            0
            10195
            2940
            2940
            2940
            2940
            2940
            2940
            2940
            2691
            58395
            0
            12256
            16900
            13683
            20255
            3034
            13540
            3195
            5907
            16827
            1834
            16564
            3927
            13440
            4367
            18571
            19697
            2793
            0
            22179
            22804
            4902
            3820
            4406
            8716
            36380
            0
            0
            0
            0
            17549

            m−+
            56
            0
            0
            0
            0
            0
            485
            0
            0
            0
            0
            0
            86
            0
            0
            0
            0
            555
            0
            4054
            0
            0
            0
            0
            0
            0
            0
            0
            1622
            0
            0
            0
            0
            0
            0
            0
            6726
            1063
            870
            0
            0
            0
            0
            0
            0
            450
            0
            0
            7896
            0
            0
            60
            0
            46
            46
            3
            0
            81
            81
            0
            0
            0
            37
            2640
            0
            0
            0
            0
            210

            Original BC
            Time
            %Gap
            289(1)
            360(1)
            2263(1)
            14.05
            181(1)
            33.73
            38(1)
            64(1)
            164.00
            727(1)
            75(1)
            116(1)
            70(1)
            66(1)
            274.67
            109(1)
            139(1)
            78.65
            93(1)
            693(1)
            524(1)
            0.59
            0.63
            233(1)
            101(1)
            201(1)
            234(1)
            228(1)
            18.29
            112(1)
            111(1)
            111(1)
            110(1)
            110(1)
            110(1)
            110(1)
            13(1)
            53.07
            111(1)
            1431(1)
            0.09
            52.77
            4.45
            236(1)
            1341(1)
            819(1)
            278(1)
            6.12
            21(1)
            0.94
            5.72
            380(1)
            11.09
            14.30
            329(1)
            401(1)
            223(1)
            242(1)
            0.85
            4.59
            1.22
            3043(1)
            8.81
            14.42
            14.42
            1320(1)
            439(1)
            10.38
            413.75(43)
            11.42

            Table 3: Results obtained on the new DMERN instances.

            Instance
            n
            m
            144
            1456
            184
            2548
            331
            10701
            399
            30000
            414
            1051
            426
            30257
            486
            117855
            688
            1470
            823
            55592
            880
            2000
            912
            13952
            914
            1872
            1003
            3733
            1012
            10750
            1085
            37373
            1120
            23387
            1178
            15341
            1227
            46506
            1300
            11656
            1300
            60706
            1320
            14639
            1388
            2970
            1388
            2970
            1643
            11651
            1649
            10243
            1661
            14725
            1752
            14856
            1820
            35200
            1866
            36800
            1878
            4333
            1878
            4333
            1878
            4333
            1878
            4333
            1878
            4333
            1878
            4333
            1878
            4333
            2088
            10560
            2112
            89677
            2174
            11167
            2278
            24476
            2280
            33180
            2286
            44595
            2303
            38857
            2347
            10025
            2358
            26496
            2400
            10130
            2438
            12232
            2520
            60669
            2578
            12312
            2686
            31228
            2704
            6630
            2706
            47185
            2837
            13457
            3047
            74184
            3052
            74312
            3127
            6299
            3240
            8186
            3249
            72072
            3285
            75186
            3364
            8265
            3488
            7470
            3488
            7470
            4294
            16669
            4732
            80870
            4794
            604007
            4794
            604007
            5150
            15000
            6480
            72900
            8317
            320726
            Nodes
            4349
            2523
            651
            68
            399
            94
            1
            71
            21
            223
            1
            53
            1
            1
            24
            8
            3
            36
            1
            13
            75
            479
            494
            1
            1
            1
            1
            1
            138
            91
            91
            91
            91
            91
            91
            91
            1
            3
            1
            3
            4
            11
            8
            1
            1
            17
            1
            29
            1
            1
            16
            19
            1
            14
            11
            1
            1
            1
            1
            9
            68
            59
            3
            3
            0
            0
            1
            1
            0
            154.49

            Improved BC
            Time
            %Gap
            Nodes
            164(1)
            3951
            2755(1)
            39710
            4.03
            508
            642(1)
            16
            172(1)
            399
            30.98
            95
            60(1)
            1
            62(1)
            71
            40.43
            27
            699(1)
            223
            29(1)
            1
            113(1)
            53
            56(1)
            1
            83(1)
            1
            783(1)
            3
            29(1)
            2
            167(1)
            1
            85.43
            43
            119(1)
            1
            169(1)
            2
            106(1)
            10
            0.59
            489
            0.63
            519
            213(1)
            1
            126(1)
            1
            215(1)
            1
            259(1)
            1
            424(1)
            3
            20.49
            129
            11(1)
            7
            10(1)
            7
            11(1)
            7
            10(1)
            7
            11(1)
            7
            10(1)
            7
            11(1)
            7
            89(1)
            1
            53.40
            4
            110(1)
            1
            1184(1)
            3
            1321(1)
            3
            2.86
            20
            2821(1)
            5
            211(1)
            1
            1049(1)
            1
            803(1)
            17
            260(1)
            1
            8.76
            23
            173(1)
            1
            2753(1)
            3
            0.94
            17
            5.84
            7
            464(1)
            1
            11.09
            10
            13.84
            11
            338(1)
            1
            431(1)
            1
            225(1)
            1
            241(1)
            1
            0.85
            9
            4.52
            70
            1.19
            62
            2380(1)
            2
            7.92
            2
            15.25
            0
            15.25
            0
            1322(1)
            1
            1173(1)
            1
            10.38
            0
            518.06(48)
            4.85
            675.26
        </corps>
        <conclusion>
            Aucune conclusion trouvée.
        </conclusion>
        <discussion>
            Aucune discussion trouvée.
        </discussion>
        <biblio>
            References
            [1] P. Abell and M. Ludwig. Structural balance: a dynamic perspective. Journal of Mathematical Sociology,
            33:129–155, 2009.
            [2] E. Balas and C.S. Yu. Finding a maximum clique in an arbitrary graph.
            SIAM Journal on Computing, 14:1054–1068, 1986.
            [3] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. In Proceedings
            of the 43rd annual IEEE symposium of foundations of computer science,
            pages 238–250, Vancouver, Canada, 2002.
            [4] F. Barahona and A.R. Mahjoub. On the cut polytope. Mathematical Programming, 36:157–173, 1986.
            [5] F. Barahona and A.R. Mahjoub. Facets of the balanced (acyclic) induced
            subgraph polytope. Mathematical Programming, 45:21–33, 1989.
            [6] J.J. Barthold. A good submatrix is hard to find. Operations Research
            Letters, 1:190–193, 1982.
            [7] B. DasGupta, G. A. Encisob, E. Sontag, and Y. Zhanga. Algorithmic and
            complexity results for decompositions of biological networks into monotone
            subsystems. BioSystems, 90:161–178, 2007.
            [8] P. Doreian and A. Mrvar. A partitioning approach to structural balance.
            Social Networks, 18:149–168, 1996.
            [9] P. Doreian and A. Mrvar. Partitioning signed social networks. Social Networks, 31:1–11, 2009.
            [10] R. Figueiredo and Y. Frota. The maximum balanced subgraph of a signed
            graph: applications and solution approaches. Paper submitted, 2012.
            [11] R. Figueiredo, M. Labbé, and C.C. de Souza. An exact approach to the
            problem of extracting an embedded network matrix. Computers & Operations Research, 38:1483–1492, 2011.
            [12] N. Gülpinar, G. Gutin, G. Mitra, and A. Zverovitch. Extracting pure network submatrices in linear
            programs using signed graphs. Discrete Applied
            Mathematics, 137:359–372, 2004.
            [13] F. Harary and J.A. Kabell. A simple algorithm to detect balance in signed
            graphs. Mathematical Social Sciences, 1:131–136, 1980.
            [14] F. Harary, M. Lim, and D. C. Wunsch. Signed graphs for portfolio analysis
            in risk management. IMA Journal of Management Mathematics, 13:1–10,
            2003.
            [15] F. Huffner, N. Betzler, and R. Niedermeier. Separator-based data reduction
            for signed graph balancing. Journal of Combinatorial Optimization, 20:335–
            360, 2010.
            11

            [16] T. Inohara. On conditions for a meeting not to reach a deadlock. Applied
            Mathematics and Computation, 90:1–9, 1998.
            [17] K.T. Macon, P.J. Mucha, and M.A. Porter. Community structure in the
            united nations general assembly. Physica A: Statistical Mechanics and its
            Applications, 391:343–361, 2012.
            [18] M. Padberg. On the facial structure of set packing polyhedra. Mathematical
            Programming, 5:199–215, 1973.
            [19] S. Rebennack. Encyclopedia of optimization. Springer, 2008.
            [20] V.A. Traag and J. Bruggeman. Community detection in networks with
            positive and negative links. Physical Review E, 80:036115, 2009.
            [21] B. Yang, W.K. Cheung, and J. Liu. Community mining from signed social networks. IEEE Transactions on
            Knowledge and Data Engineering,
            19:1333–1348, 2007.
            [22] T. Zaslavsky. A mathematical bibliography of signed and gain graphs and
            allied areas. Electronic Journal of Combinatorics DS8, 1998.
            12
        </biblio>
    </article>
    <article>
        <preamble>Conversational_Networks_for_Automatic_Online_Moderation.pdf</preamble>
        <titre>Conversational Networks for Automatic Online Moderation</titre>
        <auteur>
            Etienne Papegnies , Vincent Labatut , Richard Dufour, and Georges Linarès
        </auteur>
        <abstract>
            Abstract— Moderation of user-generated content in an online
            community is a challenge that has great socio-economic ramifications. However, the costs incurred by
            delegating this paper
            to human agents are high. For this reason, an automatic
            system able to detect abuse in user-generated content is of
            great interest. There are a number of ways to tackle this
            problem, but the most commonly seen in practice are word
            filtering or regular expression matching. The main limitations
            are their vulnerability to intentional obfuscation on the part of
            the users, and their context-insensitive nature. Moreover, they
            are language dependent and may require appropriate corpora
            for training. In this paper, we propose a system for automatic
            abuse detection that completely disregards message content.
            We first extract a conversational network from raw chat logs
            and characterize it through topological measures. We then use
            these as features to train a classifier on our abuse detection task.
            We thoroughly assess our system on a dataset of user comments
            originating from a French massively multiplayer online game.
            We identify the most appropriate network extraction parameters
            and discuss the discriminative power of our features, relatively
            to their topological and temporal nature. Our method reaches an
            F-measure of 83.89 when using the full feature set, improving on
            existing approaches. With a selection of the most discriminative
            features, we dramatically cut computing time while retaining the
            most of the performance (82.65).
            Index Terms— Classification algorithms, Information retrieval,
            Network theory (graphs), Social computing, Text analysis.
        </abstract>
        <introduction>
            I. I NTRODUCTION

            O

            NLINE communities have acquired an indisputable
            importance in today’s society. From modest beginnings
            as places to trade ideas around specific topics, they have grown
            into important focuses of attention for companies to advertize products or governments interested in
            monitoring public
            discourse. They also have a strong social effect, by heavily
            impacting public and interpersonal communications.
            However, the Internet grants a degree of anonymity, and
            because of that, online communities are often confronted
            with users exhibiting abusive behaviors. The notion of abuse
            varies depending on the community, but there is almost
            always a common core of rules stating that users should
            not personally attack others or discriminate them based on
            race, religion, or sexual orientation. It can also include more
            Manuscript received April 16, 2018; revised September 25, 2018; accepted
            December 6, 2018. Date of publication January 29, 2019; date of current
            version February 12, 2019. This work was supported in part by ProvenceAlpes-Côte-d’Azur region, France and
            in part by Nectar de Code Company.
            (Corresponding author: Etienne Papegnies.)
            E. Papegnies is with the Laboratoire Informatique d’Avignon, Avignon
            University, 84911 Avignon, France, and also with Nectar de Code, 13570
            Barbentane, France (e-mail: etienne.papegnies@univ-avignon.fr).
            V. Labatut, R. Dufour, and G. Linares are with the Laboratoire Informatique
            d’Avignon, Avignon University, 84911 Avignon, France.
            Digital Object Identifier 10.1109/TCSS.2018.2887240

            community-specific aspects, e.g., not posting advertisement
            or external URLs. For community maintainers, it is often
            necessary to act on abusive behaviors: if they do not, abusive
            users can poison the community, make important community members leave, and, in some countries, trigger
            legal
            issues [1], [2].
            When users break the community rules, sanctions can then
            be applied. This process, called moderation, is mainly done
            by humans. Since this manual work is expensive, companies
            have a vested interest in automating the process. In this
            paper, we consider the classification problem consisting in
            automatically determining if a user message is abusive or not.
            This task is at the core of automated moderation, and it is
            difficult for several reasons. First, the amount of noise in the
            content (typos, grammatical errors, uncommon abbreviations,
            out-of-vocabulary words...) of messages posted on the Internet
            is usually quite high. Furthermore, while some of this noise is
            unwittingly produced by fast typing or poor language skills, a
            good part of it is voluntarily introduced as a means to defeat
            automated badword checks, e.g. “Pls d1e you f8 ck”. Then,
            even with a noiseless message, it is sometimes necessary to
            perform advanced natural language analysis to detect abuse in
            a message. Here is a fictional example of a message containing
            no obvious indicators of abuse such as straight insults, while
            still being very abusive indeed: “Would you like to meet your
            maker? I can arrange that”. Finally, even advanced natural
            language processing approaches may not be able to detect
            abuse from a message only without looking at its context. This
            context can take various forms. For instance, in the case of a
            “Yo mama joke”, it is the continuation of the conversation. But
            it can also include external knowledge, which makes it harder
            to handle. Consider the following exchange, for example: A:
            “They’ve been discriminated against enough. Six millions of
            them were killed during the holocaust.” – B: “That didn’t
            actually happen”. The message from B has no abuse markers
            at all until one considers both the messages that came before
            and historical knowledge.
            To address these issues, we propose, as our main contribution in this paper, an approach that completely
            ignores
            the content of the messages and models conversations under
            the form of conversational graphs. By doing so, we aim to
            create a model that is not vulnerable to text-based obfuscation.
            We characterize these graphs through a number of topological
            measures which are then used as features, in order to train and
            test a classifier. Our second contribution is to apply our method
            to a corpus of chat logs originating from the community of
            the French massively multiplayer online game SpaceOrigin.1
            1 https://play.spaceorigin.fr/

            2329-924X © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
            See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

            PAPEGNIES et al.: CONVERSATIONAL NETWORKS FOR AUTOMATIC ONLINE MODERATION

            Our third contribution is to investigate the relative importance
            of the classification features, as well as the parameters of
            the graph extraction process, with regard to our classification
            task—the detection of abusive messages.
            This paper is a significantly extended version of our preliminary work started in [3]. In comparison, we
            propose and experiment with several variations of our network extraction method
            and vastly expand the array of features that we consider.
            We also adapt our approach to greatly increase the efficiency of
            our system with regard to necessary computational resources
            and make it more versatile to possible use cases.
            The rest of this paper is organized as follows. In Section II,
            we review related work on abuse detection and previous
            approaches dedicated to network extraction from various types
            of conversation logs. We describe the methods used throughout
            our pipeline in Section III, including the approach proposed
            to extract conversational networks, and the topological features that we compute to characterize them. In
            Section IV,
            we present our dataset, as well as the overall experimental
            setup for the classification task. We then provide a discussion
            and a qualitative study of the performance of our approach,
            with a focus on the contributions of the considered features.
            Because some of them are computed from information that
            is not yet available at the instant some messages are posted,
            we also examine the performances of the system-based only
            on information available at the time (i.e., as a prediction task).
            Finally, we summarize our contributions in Section V and
            present some perspectives for this paper.
        </introduction>
        <corps>
            II. R ELATED W ORK
            In this section, we first review general approaches related to
            the problem of abuse detection (Section II-A), and then focus
            on techniques that have been previously used to extract graphbased representations of conversation logs
            (Section II-B).
            A. Abuse Detection
            One can distinguish two main categories of works related
            to abuse detection: those using the content of the targeted
            messages only and those focusing on their context (user
            metadata, content of surrounding messages...). Some hybrid
            works also propose to combine both categories.
            1) Content-Based Approaches: The work initiated by
            Spertus in [4] constitutes a first attempt to create a classifier
            for hostile messages. Abusive messages often contain hostility,
            so this task is related to ours. However, the notion of abuse is
            more general, as it can take a nonhostile form. Spertus uses
            static rules to extract linguistic markers for each message:
            imperative statement, profanity, condescension, insult,
            politeness, and praise. These are then used as features in a
            binary classifier. This approach obtains good results, except
            in specific cases like hostility through sarcasm. However,
            manually defining all the linguistic rules related to an abusive
            message is a severe limitation and appears impossible,
            in practice. Also, its application to another language would
            require to transpose it to other grammar rules and idioms.
            Chen et al. [5] seek to detect offensive language in social
            media so that it can be filtered out to protect adolescents. Like

            39

            before, this task is more specific than ours, as using offensive
            language is just one type of abuse. Chen et al. [5] developed a
            system that uses lexical and syntactical features as well as user
            modeling, to predict the offensiveness value of a comment.
            They note that the presence of a word tagged as offensive in
            a message is not a definite indication that the message itself
            is offensive. For instance, while “you are stupid” is clearly
            offensive, “this is stupid xD” is not. They further show that
            lack of context can be somewhat mitigated by looking at word
            n-grams instead of unigrams (i.e., single words). The method
            relies on manually constituted language-dependent resources
            though, such as a lexicon of offensive terms, which also makes
            it difficult to transpose to another language.
            Dinakar et al. [6] use t f –i d f features, a static list of
            badwords, and of widely used sentences containing verbal
            abuse, to detect cyberbullying in Youtube comments. Bullying
            is mainly characterized by its persistent and repetitive nature,
            and it can, therefore, be considered as a very specific type of
            abuse. Like before, the proposed model shows good results
            except when sarcasm is used. It is worth noting that sarcasm
            can be considered as a form of natural language obfuscation
            that is especially hard to detect in written communications,
            because of the lack of inflection clues.
            Chavan and Shylaja [7] review machine learning (ML)
            approaches to detect cyberbullying in online social networks.
            They show that pronoun occurrences, usually neglected in text
            classification, are very important to detect online bullying.
            They use skip-gram features to mitigate the sentence-level
            context issues by taking into account distant words. These
            new features allow them to boost the accuracy of a classifier
            detecting bullying by 4% points. The approach is, however,
            still vulnerable to involuntary misspellings and word-level
            obfuscation. It uses a language-dependent list of badwords
            during preprocessing.
            In their recent article, Mubarak et al. [8] work on the
            detection of offensive language in Arabic media, by introducing the interesting possibility of dynamically
            generating
            and expanding a list of bad words. They extract a corpus of
            tweets that is divided into two classes (obscene/not obscene)
            based on static rules. Then, they perform a log odds ratio
            analysis to detect the words favoring documents from the
            obscene class. Such an approach could be very useful in an
            online classification setting, but inherently requires a dataset
            where the number of samples in the obscene class is large.
            Still, they show that a list of words dynamically generated
            using that method contains 60% of new obscene words, and
            the process can be iterated over. Relatively to our problem
            of interest, the main limitation of this paper is its focus on
            obscene words, which are just one specific type of abuse.
            Razavi et al. [9] focus on a wider spectrum of types of abuse
            than the previously cited works, which they call inflammatory
            comments. It ranges from impoliteness to insult, and includes
            rants and taunts. To detect them, they stack three levels of
            Naive Bayes classifier variants, fed with features related to
            the presence, frequency, and strength of offensive expressions.
            These are computed based on a manually constituted lexicon
            of offensive expressions and insults, which makes the method
            relatively corpus-specific. The resulting system shows high

            40

            IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 6, NO. 1, FEBRUARY 2019

            precision and has the useful characteristics of being updatable
            online. It is, however, vulnerable to the text-based obfuscation
            techniques we have previously mentioned.
            With recent developments in GPU architecture and hardware
            availability, more computationally expansive techniques have
            been used. Djuric et al. [10] detect hate speech in Twitter
            data. They adopt a two-step approach consisting in first
            learning a low-dimensional representation of the tweets, and
            then applying a classifier to discriminate them based on this
            representation. They note that jointly using message- and
            word embeddings instead of simple bag-of-words boosts the
            performance. Park and Fung [11] also work on tweets using
            neural networks, but they focus only on sexism- and racismrelated cases. They propose a two-step framework
            consisting
            in first training a convolutional neural network (CNN) to
            identify the absence/presence of abuse, and then performing
            a simple logistic regression to further discriminate between
            sexism and racism. Both of these approaches are inherently
            portable, however, they require a lot of data.
            Pavlopoulos et al. [12] develop an automatic moderation
            system for comments posted by users on websites. It is based
            on a recurrent neural network operating on word embeddings,
            with an attention mechanism. They apply it to two large
            corpus extracted from a Greek sports website and the English
            Wikipedia. The proposed system outperforms CNN and other
            more mainstream classifiers. However, it is worth noticing that
            these tasks are slightly different, as the the Greek corpus is
            annotated for general moderation, whereas the English one
            focuses on personal attacks.
            It is worth noting that all these ML-based approaches
            perform better when a large dataset is available for training. However, text-based approaches are usually
            language
            dependent, which means that models have to be trained on
            a dataset of the specific language. This is usually not an issue
            when classifying English messages because of the wealth of
            publicly available data but is problematic in our case, since
            our messages come from low-resource language communities.
            Content-based text classification usually makes for a good
            baseline. However, such methods have severe limitations. First,
            abuse can be spread over a succession of messages. Some
            messages can even reference a shared history between two
            users. Second, it is very common for users to voluntarily
            obfuscate message content to work around badwords detection.
            Indeed, abusers can bypass automatic systems by making
            the abusive content difficult to detect: for instance, they can
            intentionally modify the spelling of a forbidden word.
            Hosseini et al. [13] demonstrate such an attack against
            the Google Perspective API.2 Adversarial attacks based on
            word-level obfuscation are nothing new, and approaches exist
            to counter them. For instance, Lee and Ng [14] experiment
            with spam de-obfuscation using a hidden Markov model that
            incorporates lexical information. Such an approach yields good
            results for de-obfuscation, but it is computationally expensive and requires a dataset of obfuscated words
            for training.
            More recently, Rojas-Galeano [15] describes a more compact
            approach based on a dynamic programing sequence alignment
            2 https://www.perspectiveapi.com

            algorithm. It has a different set of limitations, the main one
            being that it does not allow for one character to be used as
            an obfuscated version of several distinct original characters (it
            uses a one-to-one character mapping).
            2) Context-Based Approaches: Because the reactions of
            other users to an abuse case are completely beyond the control
            of the abuser, some works consider the content of messages
            around the targeted message, instead of the content of the
            targeted message only.
            For instance, Yin et al. [16] use features derived from the
            sentences neighboring a given message to detect harassment
            on the Web. Harassment implies repetition and can be considered as a specific type of abuse. Their goal is
            to spot
            conversations going off-topic and use that as an indicator.
            Their combined content/context approach shows good results
            when used against multiparticipant chat logs. They also note
            that sentiment features seem to constitute mostly noise due to
            the high misspelling rate. This lack of discriminative power
            from sentiment features is something we have also noticed
            while experimenting with content-based techniques on our
            data in [17].
            Cheng et al. [18] do not try to perform automatic moderation. Instead, they conduct a comprehensive study of
            antisocial
            behavior in online discussion communities, and use its results
            to build user behavior models. We include this paper in our
            review, because it provides some insight into the devolution
            of abusive users over time in a community, regarding both the
            quality of their contributions and their reactions toward other
            members of the community. A critical result of this analysis
            is that instances of antisocial messages usually generate a
            bigger response from the community, compared to normal
            messages. In our own work, we build upon this observation and compare classification performances obtained
            when
            considering or ignoring messages published right after the
            classified message.
            Balci and Salah [19] take advantage of user features to
            detect abuse in the community of an online game. These
            features include information such as gender, number of friends,
            financial investment, avatars, and general rankings. The goal is
            to help human moderators dealing with abuse reports, and the
            approach yields sufficiently good results to achieve this. One
            important difference with our work is that in our case, the user
            data necessary to replicate this approach are not available. As a
            practical consideration the availability of that data will always
            depend on the type of the community.
            In [17], we tackle the same problem as in this paper,
            i.e., detect abuse in chat messages in the context of an
            online game. However, unlike the method proposed presently,
            we use a wide array of language features (bag-of-words,
            t f -i d f scores, sentiment scores…) as well as context features
            derived from the language models of other users. We also
            experiment with several advanced preprocessing approaches.
            This method allows us to reach a performance of 72.1% in
            terms of F-measure on our abusive message detection task.
            Of all the approaches of the literature described in this
            section, [17] as well as Balci and Salah’s [19] aim at solving
            the same problem as us. The others focus on tasks which are
            related to abuse detection, but still different, and generally

            PAPEGNIES et al.: CONVERSATIONAL NETWORKS FOR AUTOMATIC ONLINE MODERATION

            more specific, e.g., insult or cyberbullying detection. The
            work of Balci & Salah differs from ours in the way they
            solve the problem, as they focus on the users’ profiles and
            behaviors: these data are not available in our case, so we
            only use the published messages. Our previous work [17]
            is completely based on the textual content of the messages,
            whereas the one presented here ignores it, and relies only on
            a graph-based modeling of the conversation, which is completely new in this context. Another important
            methodological
            difference with the literature is that almost all content-based
            methods rely on manually constituted linguistic resources,
            which makes them difficult to transpose to another context
            (different languages or online community). By comparison,
            our present approach is completely language independent, as it
            does not use the textual content (apart from user names). The
            third difference is that almost all methods from the literature
            consider messages independently, when we use sequences of
            messages forming conversations. Finally, we use a classic
            classifier to determine if a message is abusive, which means
            that our approach requires much less training data than the
            deep learning methods that we mentioned earlier.
            B. Network Extraction From Conversation Logs
            Although a major part of the methods proposed to address
            the abuse detection problem focus on the content of the
            exchanged messages, it appears that a user with previous exposure to automatic moderation techniques can
            easily circumvent
            them [13]. To avoid this issue, a solution would be not to focus
            on the textual content, but rather on the interactions between
            the users through these messages. For instance, the number
            of respondents to a given message appears frequently as a
            classification feature in the literature, e.g., as in [18]. But
            graphs constitute a more natural paradigm to model such
            relational information, under the form of so-called conversational networks, which represent the flow of the
            conversation
            between users. Such networks have the advantage of including
            the mentioned feature (number of respondents), but also much
            more information regarding the way users interact. We adopted
            this approach in [3], which is the first attempt at using such
            graph-based conversation models to solve a general abuse
            detection problem. Our present work is an extension of this
            method, essentially on two aspects: we experiment with several
            variations of our graph-extraction process, and we consider
            much more graph-based features.
            This section reviews methods proposed in the literature for
            the extraction of conversational networks. We do not narrow
            it to the abuse detection context, as [3] would be the only
            one concerned. Even so, there are not many works dealing
            with the extraction of conversational networks. This may be
            due to the fact that the task can be far from trivial, depending
            on the nature of the available raw data: it is much harder
            for chat logs than for structured messages board or Web
            forums, for instance. In a multiparticipant chat log it is
            frequent to see multiple disjointed conversations overlapping.
            There is no fixed topic although some chatrooms have a
            general purpose. There is also no built-in mechanism to
            specify the message someone is responding to. Finally, in most

            41

            Internet relay chat (IRC) chat logs, there is no enforcement
            mechanism to ensure that users have only one nickname.
            Mutton [20] proposes a strategy to extract such a network
            from IRC chat logs. The goal is to build a tool to visualize
            user interactions in an IRC chat room over time. The author
            uses a simple set of rules based on direct referencing (i.e.,
            when a user addresses another one by using his nickname),
            as well as temporal proximity and temporal density of the
            messages. In our own work, we adapt and expend on some
            of these rules, whereas certain cannot be applied. Specifically,
            while in a regular IRC channel timestamps are indeed useful
            to determine intended recipients of a message, in our case they
            are basically irrelevant.
            Osesina et al. [21] build on the work of Mutton using
            response-time analysis, which assumes that both temporal
            proximity and the cyclical nature of conversations can be used
            to perform edge prediction. The authors also use the content
            of the communications to build a word network, and then
            assign edges between users based on the keywords they use
            and the presence of these keywords in word clusters. Finally,
            by combining these two approaches with direct addressing,
            they achieve impressive performance in edge prediction with
            regard to a manually extracted network, both in terms of edge
            existence and edge strength. It is worth noticing the significant
            computational requirement for large chat logs. Besides the
            targeted task itself, the main difference with our approach is
            that this one is strongly content-based.
            Gruzd and Haythornthwaite [22] push the usage of direct
            referencing further by developing methods of name discovery. The data they work on come from a bulletin
            board
            which shares some similarities with regular chat: linear stream
            of messages with possibly intertwined discussion threads.
            By comparing a network extracted through their name discovery method, to a chain network based on temporal
            proximity,
            they show that their approach is better suited to detect social
            network links. Useful takeaways of their method are: the use
            of neighboring words (for instance, Dr., Pr., and Jr, are often
            seen in proximity of person names, whereas Street and Ave
            are often near location names), capitalization, and the position
            of words within the document (e.g., their sample of posts
            often end with a user’s signature because a bulletin board does
            not have the ephemeral nature of chatrooms). However, these
            differences between the two media also makes this method
            unsuitable to our data.
            Çamtepe et al. [23] experiment on the detection of groups of
            users in chat logs, collected from three different chatrooms in
            the Undernet IRC network. They first build a matrix containing
            the numbers of messages posted by each user at each considered time step. It can be considered as a
            low-resolution view
            of the logs—it retains information about temporal proximity
            but looses sequential information. They then perform singular
            value decomposition on this matrix, in order to ease the identification of clusters of interacting users
            (i.e., conversations).
            They extract an approximation of the conversational network
            from this partition, by representing each cluster by a clique.
            They validate their approach by manually extracting the actual
            conversational network directly from the logs, and comparing
            their structures. The main difference with our situation is that

            42

            IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 6, NO. 1, FEBRUARY 2019

            the conversational graph is only seen as a way to validate the
            user group detection method: we want to use it as a model of
            the interactions.
            Forestier et al. [24] tackle the extraction of networks from
            online forums. While the structure of conversations is explicitly represented on certain platforms, this it
            not the case there:
            a thread is represented as a flat sequence of messages. This
            makes it challenging to determine the intended recipient of a
            message. The authors show that by using a combination of
            grammatical analysis and Levenshtein distance computation
            for substrings, they can often ascertain who talks to whom.
            The resulting network can then be used to analyse the role of
            users in the community. The main difference with our method
            is that we ignore the content of messages.
            Travassoli et al. [25] explore different methods to extract
            representative networks from group psychotherapy chat logs.
            One of them includes fuzzy referencing to mitigate effects
            of misspelled nicknames, and rules for representing one-toall messages. The bulk of the methods uses static
            patterns of
            exchanges to predict a receiver. Their system shows a good
            agreement score with a human annotator. It is worth noting,
            though, that these logs are substantially different from ours:
            the psychotherapy sessions have well defined boundaries and a
            limited number of participants. This prevent the transposition
            of this approach to our problem.
            Sinha and Rajasingh [26] use only direct referencing, but
            with the same fuzzy matching strategy as in [25], in order to
            extract a network representing the activity in the #ubuntu
            IRC support channel. This method manages to expose high
            level components of the Ubuntu social network, which in turn
            allows for the qualification of user behaviors into specific
            classes such as beginner or expert. This method of building
            user models can be very interesting when the data describing
            the users are scarce, as is the case on IRC where everyone
            can join and there is no requirement to register. While it does
            not allow for the direct classification of individual messages,
            the behavior information can be useful as a supporting feature
            in a text classification task.
            Anwar and Abulaish [27] build a framework allowing to
            query user groups and communities of interest, based on
            the data extracted from the computers of suspects during
            a criminal investigation. They use a social graph extraction
            method that relies both on the presence of communication
            between users and the overlap between the content of the
            messages they exchange, in order to assign weights to the
            edges of the network. They then experiment with various
            forms of community detection (i.e., graph partitioning) to
            identify groups of users in this network. However, this method
            assumes that the corpus contains a variety of topics allowing
            to discriminate the groups, which is not necessarily the case
            for us, since our logs are thematically dominated by the video
            game hosting the chatroom.
            An interesting task where conversational networks can
            be used is the detection of controversial discussions.
            Garimella et al. [28] show that the predefined types of
            interactions allowed by Twitter can be used to build networks
            that highlight the presence of polarized groups of users. They
            extract all tweets matching a given hashtag around the time

            a specific event happens, then detect an endorsement link
            thanks to Twitter’s retweet feature. The resulting graph is then
            partitioned and analyzed using a controversy measure. In our
            context it is difficult to adopt this approach, as endorsement
            information is not immediately available and would have to
            be inferred from message content.
            The methods proposed in the literature mainly rely on
            the content of the exchanged messages. By comparison, our
            method only focuses on the presence/absence of communication between the users, i.e., on the dynamics of the
            conversation and its structure. Some methods also rely on
            specific functionalities of the studied platforms (e.g., answers
            explicitly addressed to a user), which are absent from our own
            data.
            III. M ETHODS
            In this section, we describe the methods that we propose
            to compute the features later used in the classification task to
            separate abusive and nonabusive messages. We first present
            how we extract conversational networks from series of raw
            chat messages (Section III-A), before describing the topological measures that we use to characterize these
            networks
            (Section III-B).
            A. Network Extraction
            We extract networks representing conversations between
            users through a textual discussion channel. They take the
            form of weighted graphs, in which the vertices and edges
            represent the users and the communication between them,
            respectively. An edge weight corresponds to a score estimating
            the intensity of the communication between both connected
            users. We propose two variants of our method, allowing to
            extract undirected versus directed networks. In the latter case,
            the edge direction represents the information flow between the
            considered users. Note that each network is defined relatively
            to a targeted message, since the goal of this operation is to
            provide features used to classify the said message.
            The method that we use to extract the networks representing the conversations in which each message occurs
            has
            three steps that we describe in detail in this section. First,
            we identify the subset of messages that we will use to extract
            the network (Section III-A1). Second, we select as nodes a
            subset of users which are likely receivers of each individual
            message (Section III-A2). Third, we add edges and revise their
            weights depending on the potential receivers (Section III-A3).
            We describe and discuss the resulting conversational graphs in
            Section III-A4
            1) Context Period: Our first step is to determine which
            messages to use in order to extract the network. For this
            purpose, we define the context period, as a sequence of messages. Fig. 1 shows an example of context period,
            representing
            each message as a vertical rectangle. Note that time flows
            from left to right in Fig. 1. This sequence is centered on
            the targeted message (in red), and spans symmetrically before
            (left side) and after (right side) its occurrence. Put differently:
            we consider the same number of past and future messages.
            The networks extracted from the context period contain only

            PAPEGNIES et al.: CONVERSATIONAL NETWORKS FOR AUTOMATIC ONLINE MODERATION

            Fig. 1. Sequence of messages (represented by vertical rectangles) illustrating
            the various concepts used in our conversational network extraction process.
            Figure available at 10.6084/m9.figshare.7442273 under CC-BY license.

            the vertices representing the users which posted at least once
            on this channel, during this period.
            Besides the network extracted over the whole context period
            (before and after the targeted message), which we call the Full
            network, we also consider two additional networks. We split
            the period in the middle, right on the targeted message, and
            extract one network over the messages published in the first
            half (Past messages), called Before network, and one over the
            other half (Future messages), called After network. Both of
            those smaller networks also contain the targeted message. For
            a prediction task, i.e., when using only past information to
            classify the targeted message, one would only be able to use
            the Before network. However, in a more general setting, all
            three networks (Before, After, and Full) can be used.
            2) Sliding Window: In order to extract a network, we apply
            an iterative process, consisting in sliding a window over the
            whole context period, one message at a time, and updating
            the edges and their weights according to the process described
            next. The size of this sliding window is expressed in terms of
            number of messages, and it is fixed. It can be viewed as a
            focus on a part of the conversation taking place at some given
            time. It is shown as a thick black frame in Fig. 1. We call
            current message the last message of the window taken at a
            given time (represented in blue), and current author the author
            of the current message.
            The use of such a fixed-length sliding window is a methodological choice justified by four properties of the
            user interface
            of the considered discussion channel: 1) at any given time,
            the user can see only up to 10 preceding messages without
            scrolling; 2) when a user joins a channel, the server sends
            him only the last 20 messages posted on the channel; 3) it is
            impossible for a user to scroll back the history further than
            20 lines; and 4) the user interface masks join and part events by
            default, whereas in typical chat clients the arrival and departure
            of users are shown by default. Thus, at some given time,
            a user only has access to a limited knowledge regarding who
            is participating in the conversation. As explained later, we use
            this value of 20 messages as an upper bound, and experiment
            with different sliding window sizes.
            3) Weight Assignment: Our assumption is that the current
            message is destined to the authors of the other messages
            present in the considered sliding window. Based on this
            hypothesis, we update the edges and weights in the following
            way. We start by listing the authors of the messages currently
            present in the sliding window, and ordering them by their last

            43

            Fig. 2. Example of sliding window (left) and computation of the corresponding receivers’ scores (right).
            Each color represents a specific user. Each
            message in the window is filled with the color of its author (left), whereas
            the small squares represent direct references to users. a, b, and c columns
            represent different steps of the computation (right) (see text). Figure available
            at 10.6084/m9.figshare.7442273 under CC-BY license.

            posted message. Only the edges toward the users in that list
            will receive weight. This choice is also due to the user interface
            constraints: a priori, a user cannot reliably know which users
            will receive a given message. Furthermore, the data we have
            do not allow us to directly determine channel occupancy at
            the time a message is posted.
            Fig. 2 (left) displays an example of sliding window, in which
            the colors of the messages (vertical rectangles) represent their
            authors. So, in this specific case, four different users participate
            in the conversation. Ordered from latest to earliest, these are:
            blue (author of the current message, i.e., the rightmost in the
            window), green, orange, and red. This list of users is noted a
            in Fig. 2 (right). Obviously, a user is not writing to himself,
            so we remove the current author from the list, resulting in
            list b. The use of such an ordered list is justified by the
            assumption of temporal proximity, which appears commonly
            in the literature concerned with the extraction of conversational
            networks ( [20], [21], [23]). It states that the most recent a
            message, the most likely its author to be the recipient of the
            current message.
            The user interface allows us to explicitly mention users in
            a message by their name, and moreover the game prevents
            the users from changing their name: we need to take these
            properties into account. It is also a common assumption that
            the presence of direct referencing increases the likelihood that
            the referred person is the intended recipient of the message.
            To reflect this in our process, we move the users directly
            referenced in the current message at the top of the list. If some
            users are directly referenced although they have not posted any
            message in the considered window, they are simply inserted at
            the top of the list. In Fig. 2, direct references are represented as
            small colored squares located in the current message. There are
            two of them in our example, referring to the purple and cyan
            users. The former has one post in the window, so he is moved
            from the third to the first rank in the list. The latter did not post
            anything in the window, so he is inserted at the first position.
            This results in what we call the list of receivers, which appears
            as list c in Fig. 2.
            We now want to connect the current author to the receivers
            constituting our ordered list. Our choice to create or update
            edges toward all users in the window even in case of direct
            referencing is based on several considerations. First, directly
            referencing a user does not imply that he is part of the
            conversation or that the message is directed toward him: for
            instance, his name could just be mentioned as an object of the
            sentence. Second, there can be multiple direct references in

            44

            IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 6, NO. 1, FEBRUARY 2019

            a single message (as in our example). Third, in online public
            discourse, directly addressing someone does not mean he is
            the sole intended recipient of the message. For instance when
            discussing politics, a question directed toward someone can
            have as a secondary objective to have the target expose his
            stance on an issue to the other participants.
            We also want to adjust the strength of each of these connections depending on the rank of the concerned
            receivers: the
            higher the rank, the stronger the interaction. For this purpose,
            each receiver is assigned a score, which is a decreasing
            function of both his rank i in the list and of the length N
            of this list (as reflected by the number of + signs in Fig. 2).
            We propose three different scoring functions, defined so that
            the assigned weights sum to unity.
            1) Uniform: Each receiver gets the same weight, defined as
            1
            .
            (1)
            N
            2) Linear: The score decreases as a linear function of the
            rank
            N −i
            f L (i ) =  N
            .
            (2)
            j =1 j

            Fig. 3. Scores assigned by our three scoring functions fU , f L , and f R for
            a receiver list containing 10 users.

            fU (i ) =

            3) Recursive: The first receiver gets 60% of the total
            weight, and the rest of them share the remaining 40%
            using the same recursive 60%–40% split scheme
            
            0.6 × 0.4i−1 , if 1 ≤ i
            < N
            (3)
            f R (i ) =
            if i = N.
            0.4i−1 ,
            As an illustration, Fig. 3 displays the scores assigned
            by these three strategies for N = 10, as functions of the
            receiver’s rank. The Uniform strategy fU (in red line) assumes
            that the content of the communication is not really important,
            and that the goal of the current author is just to have the
            message seen by as much people as possible. It, therefore,
            places very little importance on temporal proximity or direct
            referencing. The Recursive approach FR (in blue) gives
            the most importance to direct referencing and temporal
            proximity, with scores dropping fast when the receiver is not
            directly referenced or the author of the immediately preceding
            message. Finally, the Linear approach f L (in green) also
            places the most importance on temporal proximity and direct
            referencing, but in a less contrasted way, since it assigns higher
            scores (compared to f R ) to receivers located at the bottom
            of the list. We later compare these three strategies during
            our experiments, in order to determine whether it is worth
            exploring more advanced scoring functions, or if the difference
            in performance is not significant enough to justify this.
            We can then update the graph by creating an edge between
            the current author and each user in the receiver list. We consider two possible approaches, leading to an
            undirected versus
            a directed network. In the latter case, the edge is directed
            from the current author toward the receiver, in order to model
            the communication flow. Each newly created edge is assigned
            a weight corresponding to the receiver’s score. If this edge
            already exists, we increase its current weight by the said score.
            Fig. 4 shows the result of this update based on our previous
            example from Fig. 2, for the extraction of an undirected

            Fig. 4.
            Update of the edges and weights of the conversational graph
            corresponding to our ongoing example. The first graph displays the state
            before the update, and each remaining one corresponds to one rank in the
            receiver list. Figure available at 10.6084/m9.figshare.7442273 under CC-BY
            license.

            network. The first graph represents the network before the
            update. It already contains some edges though, resulting from
            some previous processing. The remaining graphs of Fig. 3 represent the changes corresponding to the ranks
            appearing in
            the receiver list: first position (purple and cyan users), second
            position (green line), and third position (orange line).
            Red edges represent the edges being modified or created.
            If we were extracting a directed graph, then the new edges
            would be directed outward from the central blue vertex.
            Once the iterative process has been applied for the whole
            context period, we get what we call the Full network. As mentioned before, for testing matters we also
            process two lesser
            networks based on the same context: the Before and After
            networks are extracted using only the messages preceding and
            following the targeted message, respectively, as well as the
            targeted message itself.
            4) Extracted Networks: Fig. 5 shows a real-world example
            of the three conversational networks obtained by applying
            our extraction method to an abusive comment belonging to
            our dataset. They are obtained based on a context period of
            200 messages, a sliding window of 10 messages, and are
            undirected. The isolates (disconnected vertices) present in the
            Before and After networks correspond to users present in the
            context period, but active only after or before the targeted
            message, respectively. The red vertex corresponds to the author
            of the targeted message, which we call the targeted user. One
            can see that the users involved in the conversation, as well as
            the location of the targeted user in this conversation, undergo
            some dramatic changes after the abuse.
            Generally speaking, two vertices are connected in our networks if they are supposed to have a direct
            interaction. Thus,
            if only one conversation occurs during the considered context

            PAPEGNIES et al.: CONVERSATIONAL NETWORKS FOR AUTOMATIC ONLINE MODERATION

            Fig. 5. Example of the three types of conversational networks extracted
            for a given context period: Before (left), After (center), and Full (right).
            The author of the targeted message is represented in red dot. For readability reasons, weights and
            directions have been omitted. Figure available at
            10.6084/m9.figshare.7442273 under CC-BY license.

            period, we expect the network to be rather cliquish. It seems
            possible to have several communities, i.e., several loosely
            connected dense subgraphs, if certain users completely ignore
            some other ones, for some reason. However, the smoothing
            induced by our use of a sliding window is likely to hide
            this type of behavior, especially if the window is large. The
            presence of a community structure could also occur if several
            distinct conversations take place during the considered context
            period. However, this can happen only if the number of
            common users between the conversations is small compared to
            the network size (otherwise, the communities will be indistinguishable). Due to the relatively dense nature
            of the networks
            (when ignoring isolates), we think weights are likely to be an
            important information, allowing to separate accidental edges
            from relevant ones. The edge direction allows distinguishing
            unilateral and bilateral interactions, so it could help identify
            certain types of conversations with atypical structure (e.g., oneway communication).
            B. Features
            The classification features that we consider in this paper
            are all based on topological measures allowing to characterize
            graphs in various ways. We process all the features for each
            of the 3 types of networks (Before, After, and Full) described
            in Section III-A.
            We adopt an exploratory approach and consider a large
            range of topological measures, focusing on the most
            widespread in the literature. Some of these measures can
            optionally handle edge directions or edge weights: we
            consider all practically available variants, in order to assess
            how informative these aspects of the graph are relatively to
            our classification problem.
            One can distinguish topological measures in terms of scale
            and scope. The scale depends on the nature of the characterized entity: vertex, subgraph, or graph. In our
            case, we focus
            only on vertex- and graph-focused measures: the former allows
            focusing on the author of the targeted message, whereas the
            latter describes the whole conversation, but we do not have
            any subgraph to characterize. The scope corresponds to the
            nature of the information used to characterize the entity:
            microscopic (interconnection between a vertex and its direct
            neighborhood), mesoscopic (structure of a subgraph and its
            direct neighborhood), and macroscopic (structure of the whole
            graph).

            45

            In the rest of this section, we describe these measures
            briefly: first the vertex-focused ones (Section III-B1), then
            the graph-focused ones (Section III-B2). For each measure,
            we give a generic, graph-theoretical definition, before explaining how it can be interpreted in the context
            of our conversational networks.
            1) Vertex-Focused Topological Measures: These measures
            allows characterizing only a single vertex. We compute them
            all for the vertex corresponding to the author of the targeted
            message (represented in red in Fig. 5).
            a) Microscopic measures: We start with the measures
            which describe a vertex depending on its direct neighborhood.
            In our context, this amounts to characterizing the position
            of some user depending on its direct interlocutors. In the
            case of a conversation involving a very small number of
            persons, it is likely all of them interact directly, and so
            these measures can also help describing the conversation
            itself.
            The degree centrality is a normalized version of the standard
            degree [46], [50], which corresponds itself to the number
            of direct neighbors of the considered vertex. In a directed
            graph, one can distinguish an incoming and an outgoing degree
            centrality, focusing only on the incoming and outgoing edges
            of the vertex, respectively. In our case, it can be interpreted as
            the number of users that have exchanged (undirected version),
            received (outgoing), or sent (incoming) messages to the author,
            respectively. We use both undirected and directed variants of
            the degree centrality.
            The generalization of the degree to weighted networks is
            called the strength [47]. The strength centrality is based on
            the sum of the weights of the edges attached to the considered
            vertex. Like the degree, it is possible to use incoming and outgoing versions if the network is directed. In
            our conversational
            graph, compared to the degree, the strength takes into account
            the frequency of the interactions. This allows accounting for
            certain situations ignored by the degree centrality. For instance,
            a user can have a few interlocutors, but still be central if
            he exchanges a lot with them. We use both undirected and
            directed variants of the strength centrality.
            The local transitivity (or clustering coefficient) [48] corresponds to the proportion of edges between the
            considered vertex’s neighbors, relatively to what this number could be if all of
            them were interconnected. It ranges from 0 (no interneighbor
            edge at all) to 1 (the vertex and its neighborhood form a
            clique). In our context, a high transitivity indicates that the user
            belongs to a single conversation, in which most protagonists
            exchange messages. On the contrary, a low transitivity denotes
            some form of segmentation: either the user participates in
            several distinct conversations, or some of his interlocutors
            ignore each other. We use the unweighted original version and
            the weighted variant presented in [47].
            Burt’s constraint [49] measures how redundant the
            neighbors of the vertex of interest are. It is based on the idea
            that a vertex located at the interface between several independent groups holds a position of power. Burt’s
            constraints
            measure this level of independence through a nonlinear combination of the number of connections between the
            neighbors.
            A high value indicates how embedded the vertex is in its

            46

            IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 6, NO. 1, FEBRUARY 2019

            neighborhood. In our case, this can help distinguishing users
            depending on the number of conversations they are involved
            in, if we suppose a conversation corresponds to a clique-like
            structure. We use both unweighted and weighted variants of
            Burt’s constraint.
            b) Macroscopic measures: The measures harnessing the
            entirety of the graph structure form the largest group. In our
            context, they allow characterizing the position of a vertex
            relatively to the whole context period (Full) or to one of its
            halves (Before and After).
            So-called spectral measures are based on the spectrum of the
            graph adjacency matrix, or of a related matrix. The eigenvector
            centrality [34] can be considered as a generalization of the
            degree, in which instead of just counting the neighbors, one
            also takes into account their own centrality: a central neighbor
            increases the centrality of the vertex of interest more than a
            peripheral one. Central vertices tend to be embedded in dense
            subgraphs. We use the (un)weighted and (un)directed variants
            of the measure (so: four variants in total).
            One limitation of the eigenvector centrality is that if the
            graph is directed and not strongly connected, certain vertices
            systematically get a zero centrality, whatever their position.
            Several modifications have been proposed to handle this
            situation. The hub and authority scores [35] are two complementary measures processed through the
            hyperlink-induced
            topic search algorithm. They solve the issue by splitting the
            centrality value into two parts: one for the incoming influence
            (authority), and the other for the outgoing one (hub). We use
            the (un)weighted directed variants of both hub and authority
            scores.
            The alpha centrality (or Katz centrality) [36], [51] solves
            the same problem by assigning a minimal positive centrality
            value to all vertices. In addition, it allows attenuating the
            influence of distant vertices during the computation. We use
            the (un)weighted directed variants of this measure. The power
            centrality [37] generalizes both the eigenvector and alpha
            centralities. In particular, it allows a negative attenuation.
            The implementation we use only works for unweighted
            directed (UD) graphs.
            The pagerank centrality [38] can be seen as a variant
            of the Katz centrality. One limitation of the later is that
            when a central vertex has many outgoing edges, all of them
            receive all its influence, as if they were its only recipient.
            The pagerank centrality includes a normalization allowing to
            model the dilution of this influence. We use the (un)weighted
            and (un)directed variants of this measure.
            Compared to the other spectral measures, the subgraph centrality [39] defines the notion of reachability
            based on closed
            walks rather than simple walks. Put differently, the other
            spectral measures consider that the vertex of interest influences
            (resp. is influenced by) some other vertex if a walk exists to
            go to (resp. come from) this vertex. The subgraph centrality
            requires both, and it uses an attenuation coefficient to give less
            importance to longer walks. The implementation we use only
            deals with undirected unweighted graphs.
            In our conversational graph, we expect that a user participating a lot in the conversation will be central,
            and even more so
            if there are several conversations and he is participating in the

            main one. It is difficult to predict which ones of these slightly
            different spectral measures will be the most appropriate to our
            case, which is why we included all of the available ones.
            Another group of macroscopic measures is based on the
            notions of shortest path or geodesic distance (i.e., the length
            of the shortest path).
            The betweenness centrality [40] is related to the number of
            shortest paths going through the considered vertex. In communication networks such as ours, it can be
            interpreted as the
            level of control that the user of interest has over information
            transmission. We use the (un)weighted and/or (un)directed
            variants of this measure.
            The closeness centrality [41] is related to the reciprocal of
            the total geodesic distance between the vertex of interest and
            the other vertices. It is generally considered that it measures
            the efficiency of the vertex to spread a message over the graph,
            and its independence from the other vertices in terms of communication. The eccentricity [42] is related to
            the closeness
            centrality, but it is not a centrality measure. On the contrary,
            it quantifies how peripheral the vertex of interest is, by considering the distance to its farthest vertex.
            By comparison to
            the closeness centrality, there is no reciprocal involved, and it
            uses the maximum operator instead of the sum. In our case,
            both measures indicate how involved the considered user is in
            the conversation(s), as they directly depend on how directly
            connected he is to the other users. In particular, we expect
            important changes in the Before and After graphs to reflect a
            significant modification of the user’s role in the conversation.
            For the closeness centrality, we use the (un)weighted and
            (un)directed variants, but for the eccentricity, we only have
            access to the unweighted (un)directed variants.
            The last group of macroscopic measures is based on the
            notion of connectivity, i.e., whether or not a path exists
            between certain parts of the graph.
            An articulation point (or cut vertex) is a vertex whose
            removal makes the graph disconnected, i.e., split it into several
            separate components [42]. We define a binary nodal feature
            indicating if the vertex of interest is an articulation point (1)
            or not (0). It could help describing whether the targeted user
            is bridging two separate groups of users in the conversation,
            possibly indicating that he caused a topic shift or that some
            of the users have left the conversation.
            c) Mesoscopic measures: Mesoscopic measures rely on
            an intermediate structure to characterize a vertex. In our case,
            such a subgraph corresponds to a tightly knit group of users,
            and is likely to represent a conversation. So, this type of
            measure would allow characterizing the position of a vertex
            relatively to the various conversations taking place in the
            considered context period (provided there are several of them).
            The coreness score [43] is based on the notion of k-core,
            which is a maximal induced subgraph whose all vertices have
            a degree of at least k. The coreness score of a vertex is the
            k value of the k-core of maximal degree to which it belongs.
            In our context, the coreness score is related to the number of
            participants of the largest conversation involving the user of
            interest. We use an undirected version of the coreness score,
            as well as two variants focusing on incoming and outgoing
            edges in directed networks.

            PAPEGNIES et al.: CONVERSATIONAL NETWORKS FOR AUTOMATIC ONLINE MODERATION

            We also take advantage of the within-module degree and
            participation coefficient, a pair of complementary measures
            defined relatively to the community structure of the graph [44].
            We detect the community structure through the InfoMap
            method [52]. These measures aim at characterizing the position
            of a vertex at this intermediate level. The within-module
            degree (or internal intensity) assesses the internal connectivity.
            It evaluates how the degree of a vertex within his community
            relates to those of the other vertices from the same community.
            For us, it is an indicator of how involved the user is in his
            current conversation. The participation coefficient is concerned
            with the external connectivity: it is based on the number and
            quality of the connections that the vertex has outside of his
            own community. In our case, a high value could indicate either
            someone holding a mediation position, in the case of a single
            conversation involving several groups of users, or someone
            participating in several conversations. We use the original
            undirected variants of these measures, as well as the directed
            variants proposed in [53] to focus on incoming and outgoing
            edges.
            One limitation of the participation coefficient is that it
            mixes several aspects of the external connectivity: the number
            of external connections, the number of concerned external
            communities, and the distribution of these connections over
            these communities. To solve this issue, three measures were
            proposed in [45] to separately assess these three properties.
            They are, respectively, called external intensity, diversity, and
            heterogeneity. The available variants are all unweighted, but
            allow handling undirected, incoming, and outgoing edges.
            2) Graph-Focused Topological Measures: A simple way to
            obtain graph-focused measures is to consider a vertex-focused
            measure and compute some statistic over the vertex set of the
            graph. This is what we do for all of the 21 measures described
            in Section III-B, by averaging them over the whole graph. This
            also holds for all the variants (weighted and/or directed) of
            these measures. But there are also measures defined specifically for the graph scale: like before, we
            distinguish them
            based on their scope.
            a) Microscopic measures: First, we use very classic
            statistics describing the graph size: the vertex and edge counts.
            We also compute the density, which corresponds to the ratio
            of the number of existing edges to the number of edges in
            a complete graph containing the same number of vertices.
            In other words, the density corresponds to the proportion of
            existing edges, compared to the maximal possible number
            for the considered graph. In our context, these measures
            allow assessing the number of users considered in a context
            period (vertex count), and the general intensity of their
            communication during this period (edge count). The Density
            can be viewed as a normalized edge count that is more likely
            to be useful when comparing graphs of different sizes.
            The global transitivity (or global clustering coefficient [31]
            is the graph-focused counterpart of the Local Transitivity.
            It corresponds to the proportion of closed triads among
            connected ones, where a closed triad is a three-clique (i.e.,
            a triangle) and a connected triad is a subgraph of three vertices
            containing at least two edges. This proportion measures the
            prevalence of triadic closure in the graph. In our context,

            47

            it assesses how likely two users communicating with the
            same person are to directly exchange messages themselves.
            We only have access to the undirected unweighted version of
            this measure.
            The reciprocity [32] is defined only for directed graphs.
            It corresponds to the proportion of bilateral edges over all
            pairs of vertices. In our networks, a low reciprocity would
            indicate that certain users do not respond to others.
            The degree assortativity (or assortativity for short) [33]
            measures the homophily of the graph relatively to the vertex
            degree. The homophily is the tendency for vertices to be
            connected to other similar vertices (in this case: of similar
            degree). It is based on the correlation between the series constituted of all pairs of connected vertices.
            We use both directed
            and undirected variants of this measure. In our conversational
            networks, this measure could help detect situations where users
            do not participate to the conversation at the same level.
            b) Macroscopic measures: A number of macroscopic
            measures are connectivity-based. The weak component count
            corresponds to the number of maximally connected subgraphs.
            In such a subgraph, there is a path to connect any pair of
            vertices. For our conversational networks, this could correspond to a conversation, whose participant do not
            necessarily
            talk directly to each other. However, due to the use of
            a sliding window, we expect our graphs to be connected
            (i.e., only one weak component), even if by very weak edges.
            In this case, a conversation is more likely to correspond to
            other substructures based on more relaxed definitions, such as
            cliques or communities. For directed graphs, we also consider
            the strong component count: a strong component is similar to
            a weak one, except it is based on directed paths. We suppose
            that, in our networks, we are more likely to get several strong
            components, since users do not necessarily exchange in a
            bilateral way.
            The cohesion (or vertex connectivity) of a graph corresponds to the minimal number of vertices one needs to
            remove
            in order to make the graph disconnected (i.e., have several
            components) [29]. The adhesion (or edge connectivity) is
            similar, but for edges. In our conversational networks, these
            measures can be related to the level of participation to the
            considered conversation: the higher their values, and the higher
            this level. But high values can also denote the presence
            of several distinct conversations in the context period. Both
            measures are defined for directed networks.
            As mentioned before when describing the nodal measures,
            we check whether the targeted user is an articulation point.
            We also compute the articulation point count, i.e., the total
            number of articulation points in the graph. This measure is
            related to the Cohesion, since there are no articulation point
            if the Cohesion is larger than 1. The implementation we use
            handles only undirected graphs. In our context, the number of
            articulation points could be related to the presence of several
            conversations (articulation points corresponding to gateway
            users between them). It could also reflect situations where
            a conversation lasts a very long time, and some groups of
            users loose interest and get disconnected from the active users.
            Another possibility is the occurrence of a flood-type situation:
            a user sends a flurry of messages into the channel to kill a

            48

            IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 6, NO. 1, FEBRUARY 2019

            conversation, then leave, and a different group of users later
            takes possession of the channel to start its own conversation.
            We also use three distance-related measures. The first is
            the diameter, which corresponds to the largest distance found
            in the graph, i.e., the length of the longest shortest path.
            It also corresponds to the largest eccentricity over all vertices.
            We use (un)weighted and (un)directed variants. The second is
            the radius, which is the smallest eccentricity over all vertices.
            We use its undirected, incoming and outgoing variants. The
            third is the average distance, which is the average length
            of the shortest paths processed over all pairs of vertices.
            We use its unweighted (un)directed variants. In our networks,
            the distance is related to the separation between users, in terms
            of interaction. A large Diameter means that a user can be many
            intermediaries away from exchanging directly with another
            user. This could be caused, for instance, by the occurrence
            of several distinct conversations in the considered context
            period, or by a very long conversation loosing and gaining
            users through time. This observation also holds for the radius
            and average distance, which provide a slightly different perspectives on the same aspect of the graph.
            c) Mesoscopic measures: We process the total clique
            count in the network, where a clique is a complete induced
            subgraph. As mentioned eralier, this can be related to the
            number of conversations occurring in the context period, or to
            number of subgroups of users participating in the same conversation.
            Like before with vertex-focused measures, we use the
            InfoMap algorithm to detect the community structure [52].
            Based on this partition, we compute two measures: the community count and the modularity [30]. The latter
            assesses the
            quality of the detected community structure, i.e., how internally cohesive and externally disconnected the
            communities
            are. We use both weighted and unweighted variants of the
            Modularity.
            IV. E XPERIMENTS
            This section describes our experimental setup and results
            regarding the automatic detection of abusive messages in
            chat logs. In Section IV-A, we present our dataset and the
            general architecture of our classification system. Because we
            expect some of our features to be redundant, we conduct a
            correlation study of our feature set in Section IV-B. We present
            general results and the effect of our various graph extraction
            parameters in Section IV-C. In Section IV-D, we investigate
            the temporal aspects of the system –specifically what happens
            when we train our models based on the features extracted from
            only one of the three graphs (Before, After, or Full), or some
            of their combinations. We then examine the importance of
            weight and directionality in Section IV-E, before investigating
            the potential for computational optimization through feature
            selection in Section IV-F. Finally, in Section IV-G, we compare
            the performance obtained using the best configuration of our
            framework with the selected baselines.
            A. Experimental Setup
            We have access to a database of 4 029 343 messages that
            were exchanged by the users of the browser-based multiplayer

            game SpaceOrigin, a French-language massively multiplayer
            online game. In this database, 779 messages have been flagged
            by one or more users as being abusive, and subsequently
            confirmed as abusive by the human game moderators: they
            constitute our abuse class. Each message belongs to a unique
            communication channel. A total of 226 distinct users have
            authored these abusive messages. We further extract 2000 messages at random from the messages not confirmed
            as abusive,
            to constitute the nonabuse class. Note that all the results we
            discuss in this paper are relative to the abuse class.
            We previously experimented with this dataset in
            [3] and [17]. However, since then we have detected
            certain inconsistencies in the database, preventing us from
            retrieving the context of certain messages. We cannot apply
            our classification method to them, so we discard them for the
            work presented here. Note that this concerns both classes.
            Moreover, our tests show that removing those samples does
            not significantly impact our previous performances. The
            resulting dataset is constituted of 1890 messages in the
            nonabuse class and 655 messages in the abuse class. Fig. 6a
            shows the distribution of abuse cases by user. It suggests that
            most abusive users need only a few warnings before mending
            their ways, but it also shows that some users are exceptional
            in the number of abuses they commit.
            Because of the relatively small dataset, we set up our
            experiment for a tenfold cross-validation. We split the dataset
            into 10 same-sized parts containing the same ratio of abusive
            to nonabusive messages. We use a 70%-train / 30%-test split,
            which means, for each run of the cross-validation, the train set
            is composed of 7 of those parts while the test set is composed
            of the remaining 3. We use Python-iGraph [54] to extract the
            conversational networks and process the graph-based features
            for each message. As a classifier, we use a support vector
            machine (SVM), implemented in the Sklearn toolkit [55] under
            the name C-support vector classification.
            We mainly experiment with 4 different sets of features: Full,
            Before, After, and All. For a given message, Before, After,
            and Full correspond to all the topological measures computed
            for the Before, After, and Full graphs, respectively. All is the
            union of all three sets, i.e., it includes all topological measures
            for all three graphs.
            In the remainder of this section, we occasionally provide
            computational time requirements. For context, the times
            that we provide correspond to single-threaded calculations
            performed on an Intel Xeon CPU E5-2620 v3s, clock speed
            2.5-GHz and 15-MB cache.
            B. Feature Dependence Study
            Each considered topological measure was originally defined
            to characterize a graph in a specific, distinct way. So, in theory,
            they could all be independent for a given graph, and thus all
            necessary to describe it completely. But in practice, according
            to the structure of the considered graph, some of them can
            be statistically dependent, and, therefore, redundant. In order
            to get a better understanding of the way these topological
            measures behave on our conversational graphs, we compare all
            the computed features using Pearson’s correlation coefficient.

            PAPEGNIES et al.: CONVERSATIONAL NETWORKS FOR AUTOMATIC ONLINE MODERATION

            49

            Fig. 6. a) Distribution of the number of abuse cases by user. b) Classification performance (F-measure) as a
            function of the context period size, for a sliding
            window of 10 messages and using the Recursive weight assignment strategy ( f R ). c) Classification
            performance (F-measure, in blue) and number of abuse
            occurrences in the context period (red), as functions of the context period size.

            In our context, where these features are later fetched to a
            classifier, only the strength of the association is relevant,
            i.e., the absolute value of the correlation (not its sign).
            We identify clusters of highly correlated features using the
            hclust function of the R language, which implements a
            standard hierarchical cluster analysis method, with average
            linkage. We use the Silhouette measure [56] as a criterion to
            select the best cut in the produced dendrograms. To keep the
            description short, we only focus on the most interesting results.
            A very small number of features are constant over all
            instances of the corpus, which means they have no discriminative power at all. For all three types of
            networks, the number
            of weak components is always 1, which means they are always
            (weakly) connected. This can be explained by our use of
            a sliding window: even if the context period contains two
            separate conversations, they will be connected, possibly by
            an edge of quasi-zero weight. In the After and Full graphs,
            the targeted user is never an articulation point. Moreover, in the
            Full graph, the number of articulation points is always zero.
            We already know that the graphs are connected, so this zero
            value means no single vertex removal can disconnect them.
            A few features are quasi-independent, in the sense they
            display almost no correlation with any other feature. This
            is the case of certain variants of the power, subgraph, and
            alpha centralities. From this point of view, they differ from the
            other spectral measures, which are overall strongly correlated.
            Certain variants of measures focusing on connectivity (strong
            component count, adhesion, cohesion, and radius) are also
            independent, and it is the case for a number of mesoscale
            features too, all of them based on the community structure. The
            fact that these features are only weakly (if at all) correlated
            to the others makes them singular, in the sense they are the
            only ones to capture certain structural changes in the conversational graphs. But it does not imply they
            have any particular
            discriminative power regarding the classification task at hand.
            However, they must be closely monitored in the rest of our
            experiments, because they constitute good candidates.
            The rest of the features forms highly correlated clusters,
            some of which are homogeneous in terms of measure variants,
            while some are heterogeneous. As explained in Section III-B,
            for each topological measure we consider several variants
            to define our features: directed vs. undirected, weighted vs.

            unweighted, averaged vs. individual. In our case, certain
            variants of the same measure are strongly correlated, which
            makes them redundant for our purpose. In particular, a very
            large number of measures, mainly distance- and communitybased, have strongly correlated direction-based
            variants. This
            indicates that most of the time, considering the direction
            of the interactions between users does not bring any additional information. This effect is clearly much
            less marked
            for average-based and weight-based variants. Thus, unlike
            direction, weight seems like an important aspect of our graphs,
            and averaging measures over all vertices also seems to bring
            some relevant information.
            Overall, we observe different behaviors, which cannot
            be explained only by the various characteristics of the
            features (micro/meso/macroscopic, un/directed, un/weighted,
            Before/After/Full graphs). This supports our decision to adopt
            an exploratory approach to identify the most appropriate
            features for our classification problem. The detected clusters of
            correlated features will be useful later to ease the interpretation
            of the classification results, as features belonging to the same
            cluster can be considered as interchangeable.
            C. Impact of Graph Extraction Parameters
            As explained in Section III-A, our graph extraction method
            has three important parameters: 1) size of the context period;
            2) size of the sliding window; and 3) weight assignment
            strategy. In this section, we explore how the classification
            performance varies depending on these parameters. Our goal
            here is both to get a better understanding of the parameters
            role, and to identify the most appropriate values without
            having to use brute force.
            As a reminder, the context period is the sequence of messages considered to classify the targeted message,
            and symmetrically built around this message. We expect it to have a strong
            effect on the classification performance, depending on its size.
            If it is too small, one can suppose it only includes a part of the
            conversation containing the targeted message, and, therefore,
            lacks some information necessary to make a proper decision
            regarding the abusive nature of this message. On the contrary,
            if it is too large, we assume it contains several conversations
            having nothing to do with the targeted message, which should
            also result in lower classification performance. In summary,

            50

            IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 6, NO. 1, FEBRUARY 2019

            Fig. 7. a) Classification performance (F-measure) for the 3 considered weight assignment strategies
            (Uniform, Linear and Recursive) and 2 context period
            sizes (200 and 1,350 messages), as a function of the sliding window size. b) Classification performance
            (F-measure) as a function of the context period size
            (in messages), for the 4 considered feature sets (Before, After, Full, All) as well as a combination of the
            first two of them (Before + After). c) Performance
            (F-measure) obtained for the (un)directed and (un)weighted feature sets, as a function of the context period
            size.

            when using a growing context period, we expect the classification performance to increase, then reach a
            plateau corresponding more or less to the typical duration of a conversation, and
            then decrease as the context period contains more and more
            noise (i.e., information not related to the targeted message).
            Fig. 6b shows the evolution of the classification performance, expressed in terms of F-measure for the abuse
            class,
            as a function of the context period size, as for it expressed
            in numbers of messages. We fix the sliding window to
            10 messages, and the recursive strategy f R to assign weights.
            All available features (All feature set) are used during the
            classification. We choose these extraction parameters because
            earlier testing showed that the recursive strategy yielded the
            best performance, and this sliding window size provides a
            good tradeoff between a graph that would be very sparse and,
            therefore, not informative enough, and one that would be very
            dense and thus too noisy.
            It appears that our assumption is only partially verified:
            the performance first increases with the context period size.
            However, it does not reach a plateau as expected, and, on the
            contrary, seems to go on increasing, albeit more and more
            slowly, as if it was logarithmically depending on the context
            period size. The maximal performance is obtained for a size
            of 1350 messages, but it is possible that even higher values
            can be obtained for larger context periods (which we did
            not check due to computational limitations). This means that
            our assumption regarding that large context periods would
            bring mainly additional noise is incorrect, because, on the
            contrary, they convey more relevant information concerning
            the classification task.
            We manually investigate a sample of our dataset to
            understand this trend. From reading a number of conversation
            logs, it first appears that conversation boundaries are not
            well defined, and so there is no typical duration for a
            conversation: this can explain the absence of a plateau in
            the plot. Furthermore, we based our assumptions with regard
            to performance on the idea that an abusive message has a
            specific impact on what happens after it is posted. Specifically,
            the conversation would show markers of normality before the
            message occurs and quickly devolves after that. As it turns
            out, in conversations where an abusive message is found,
            the author of the abusive message usually has been around

            for a while, and the message that is actually flagged and
            confirmed as abusive is not his first suspicious message.
            We assume that the classifier can take advantage of this type
            of situations, therefore, invalidating our previous assumption
            that a large context period would only bring noise.
            Note that more than one message of the abuse class can
            co-occur, i.e., can appear in the same context period, if it is
            large enough, which also supports our point. This is generally
            due to a single user sending multiple abusive messages in
            quick succession, or because the conversation devolves into
            name-calling following an initial abusive message. Fig. 6c
            shows the number of co-occurring abusive messages, as well
            as the F-measure performance, as functions of the size of the
            context period. There is a very strong match between both
            series, even if not perfect. This seems to back our assumption
            of the classifier taking advantage of the potential abuse cases
            happening around the targeted message. All these observations
            regarding the co-occurrence of abuse expose a couple of
            interesting perspectives: 1) user models can presumably yield
            features useful for classification and 2) a text-based model of
            the whole conversations would also likely be useful.
            We now explore the impact of the window size and weight
            assignment strategy on the overall performance. Fig. 7a shows
            the evolution of our performances for two fixed sizes of
            context periods (200 and our previously obtained optimum
            of 1350). The maximal window size considered is 19,
            which corresponds to almost twice the default GUI limitation
            (cf. Section III-A2).
            For our optimal context size, we obtain the best results for
            a window size of 9 and the linear assignment strategy, and for
            the window size 10 and the Recursive assignment strategy.
            Both of those strategies give greater importance to temporal
            proximity. Overall, there is no much difference between our
            weight assignment strategies. It seems that the specific values
            of the weights are not as important as their relative ordering.
            It is also worth noting that those window sizes are very close
            to the natural limitation of the GUI, which means they likely
            best capture the intended recipients of any given message.
            D. Temporal Aspects
            The results shown until now are all obtained using the All
            feature set, i.e., the features resulting from the calculation of

            PAPEGNIES et al.: CONVERSATIONAL NETWORKS FOR AUTOMATIC ONLINE MODERATION

            all topological measure variants for all three individual graphs
            (Full, Before, and After). However, using the Full or After
            graphs restricts the possible use cases for the system to tasks
            that do not require taking a decision as soon as the message
            to classify becomes available since the “future” context of the
            targeted message is taken into account. In order to have a
            system that is capable of doing so, we must investigate the
            impact of the Before features. Studying the features from
            the three individual graphs also allows us to get a better
            understanding of the system by providing a qualitative and
            accurate analysis of each part of the context.
            Fig. 7b shows the results obtained for classifiers built using
            combinations of the available feature sets: After, Before, and
            Full correspond to each graph considered separately, whereas
            Before + After denotes the union of the Before and After
            sets, and All represents the set of all computed features
            (Before, After, and Full). The conversational graphs used for
            these experiments are extracted using the recursive assignment
            strategy f R and a sliding window of 10 messages. Unless
            stated otherwise, we use these parameters in the rest of this
            paper as they match the best performance obtained during our
            greedy search of the parameter space (Section IV-C).
            Since the main idea behind our approach is to detect the
            nature of a message based on the reaction it triggers in the
            community, it is not surprising to see that the After feature
            set (in red curve) reaches an acceptable performance level on
            this task. However, what is surprising is that by using only
            the Before feature set (in blue curve), the system performs
            much better on the same task (at least for small context
            periods). This suggests that the interactions occurring before
            the targeted message reveal more about its abusive nature than
            those happening after.
            Nevertheless, when using large context periods, the performances obtained for Before and After get very
            similar. This
            indicates that what is important is not whether the messages
            used to extract the conversational graph precede or follow the
            targeted message, but rather how many of these messages are
            used. This supports our previous finding, regarding the fact
            that the context period size is the most important parameter of
            our graph extraction procedure. Moreover, based on the same
            observation, one could also think that the Before and After
            graphs convey approximately the same information, when
            considering large context periods, because the corresponding
            performances are roughly the same. However, this is disproved
            by the results obtained for the union of the Before and
            After feature sets (in yellow): the classification performance
            is noticeably higher, which means both types of graphs do not
            completely overlap, informationally speaking.
            It is worth noticing that we get almost the same performance
            with the Full feature set as with Before + After. One could
            assume that the use of two distinct graphs built on either
            sides of the targeted message would help better characterizing
            it, compared to the Full feature set, which covers the same
            time span based on a single graph. Indeed, when extracting
            the latter, the sliding window passes through the targeted
            message, and is likely to smooth the potentially relevant
            topological changes occurring right around it. However, even
            if the performance gap between Before and After seems to

            51

            widen when the context period gets larger, the difference is
            not clear, so this assumption is not verified. This means that
            the single Full graph is as approximately as informative as the
            joint use of both Before and After graphs. The latter option
            procures more flexibility in the possible application scenarios,
            but it contains twice as many features, and, therefore, requires
            roughly twice the computational time. This observation is
            confirmed when we consider the All feature set (in green),
            which contains all features for all three graphs. As expected,
            it performs best overall, since it is the union of all the
            other considered feature sets. However, the results are only
            marginally better than for Full and Before + After. This means
            that the information conveyed by the Full and Before + After
            feature sets essentially overlaps: using their union does not
            bring any noticeable performance increase.
            E. Impact of Weights and Directions
            We now investigate how considering the edge weights
            and directions in our features affects the classification
            performance. Based on the All feature set, we define four
            new feature sets, characterized by their focus on unweighted
            undirected (UU), UD, weighted undirected, and weighted
            directed measures, respectively. Concretely, each set includes
            the same group of core features, which are conceptually not
            concerned by the notion of weight or direction. This core is
            completed by features designed to consider or ignore weights
            or directions. For instance, the Clique Count is a core feature,
            whereas each one of the four variants of the diameter appears
            in a specific set.
            Fig. 7c displays how the corresponding classification performance (in terms of F-measure) evolves as a
            function of
            the context period size. It appears that both weighted feature
            sets (blue and yellow) dominate their unweighted counterparts
            (green and red curves) over the considered interval. This
            seems to confirm our assumption from Section III-A, regarding
            the fact that weights can help discriminate between certain
            structures of conversations, and/or distinguish consecutive
            conversations. There is a similar effect for directions, but
            it is much weaker, as each directed feature set (red and
            yellow) only partially dominates its undirected counterpart
            (green and blue). This is consistent with our observation from
            Section IV-B, regarding the high correlation noticed for certain
            measures, between their undirected and directed variants. This
            indicates that the direction of edges is not as relevant as
            their weight relatively to the classification task at hands. Yet,
            the best performance is reached when using both weights and
            directions. If the additional computational cost is not too high
            (and it is generally not the case), it is, therefore, worth using
            directed features.
            F. Feature Contributions
            In order to estimate the discriminative power of our features
            with regard to this classification task, we use a recursive
            feature elimination method. It takes a given feature set as
            input, and outputs its subset of so-called top features (TFs).
            These are the minimal subset of features allowing to reach
            97% of the performance obtained when considering the input

            52

            IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 6, NO. 1, FEBRUARY 2019

            TABLE I

            TABLE II

            C OMPARISON OF THE P ERFORMANCES O BTAINED W ITH THE F EATURE
            S ETS (A LL , B EFORE , F ULL , AND A FTER ) AND T HEIR S UBSETS
            OF T OP F EATURES (TF). T HE T OTAL R UNTIME I S
            E XPRESSED AS day:h:min:s

            C LUSTERS C ONTAINING THE TF S ( IN B OLD ) O BTAINED FOR THE A LL
            F EATURE S ET. T HE L ETTERS IN THE G RAPH C OLUMN S TAND
            FOR B EFORE (B), A FTER (A), AND F ULL (F). T HOSE IN THE
            S CALE C OLUMN M EAN G RAPH -S CALE (G) OR V ERTEX S CALE (N). T HOSE IN THE W GHT. (W EIGHTS ) AND D IR .
            (D IRECTIONS ) C OLUMNS S TAND F OR : U NWEIGHTED OR
            U NDIRECTED (U), W EIGHTED (W), D IRECTED (D),
            I NCOMING (I) AND O UTGOING (O).

            feature set. In order to identify these TFs, we apply an iterative
            method based on Sklearn. This toolkit allows us to fit a
            linear kernel SVM with the values of the input feature set,
            and provides a ranking of the individual features in that set,
            reflecting their relevance to the classification task. We then
            drop the least important feature, and train a new model
            using all the remaining features. We repeat this process until
            the classification performance reaches the targeted minimal
            threshold of 97% of the original F-measure score.
            We first apply this recursive feature elimination process
            to the All feature set in order to identify the overall best
            features, then do the same with the Before, After, and Full
            feature sets, for comparison purposes. Table I presents the
            performances and computational time costs measured for
            each of these complete feature sets, as well as for their
            respective TF subsets. It appears that, for all four feature sets,
            using the TFs during the classification allows reducing the
            runtime by up to 4 orders of magnitude, while retaining at
            least 97% of the F-measure value, which is very interesting
            from an application perspective. It means that the longer
            features to process do not bring more discriminative power
            than the shorter ones, regarding the classification task at hand
            (at least for our dataset). The fourth column describes the
            average runtime by message, and shows that the classifier
            could operate in real-time when limited to the TFs.
            It is important to notice that feature computation is by far
            the most computationally expensive step of our framework.
            By comparison, extracting the conversational graphs for the
            full corpus takes around 3 min, and performing the whole
            cross-validation (i.e., tenfold training and testing) only 1 min.
            The time required to compute our features depends on the size
            of the conversational graph, in terms of number of vertices
            and/or edges. The graph size is affected, in turn, by the
            number of users involved in the conversation (number of
            vertices) and the density of exchanged messages (number and
            weights of the edges). These characteristic are bounded by
            social and ergonomic (e.g., user interface) constraints, and can
            therefore be assumed as independent from the corpus size.
            The scalability of our method thus depends on that of the tool
            selected to perform the classification step: it is an SVM in
            this paper, but the end-user is free to use any other classifier
            instead.
            As explained in Section IV-B, certain of our features
            are strongly correlated, which led us to identify clusters of

            interchangeable features. Studying the TFs would result
            in missing this information: we must consider their clusters instead. Table II displays the nine clusters
            corresponding
            to the TFs obtained for the All feature set. For matters of
            space, we discuss in detail this sole feature set only. Note
            that these clusters generally contain several variants of one
            (or more) topological measure(s), as indicated by the four
            last columns. For instance, Cluster 10 contains all variants of
            average weighted and UU eigenvector centrality for all three
            types of graphs (Before, After, and Full). Also note that a
            letter G in the scale column can either refer to a naturally
            graph-scale feature, or to a vertex-scale feature averaged over
            V (cf. Section III-B). For completeness, the proper TFs are
            represented in bold.
            Cluster 9 contains only micro- (degree, strength, Burt’s
            constraint) and meso-scopic (Clique Count, Coreness) features
            describing the After and Full graphs. Moreover, all of them
            are graph-scale (as the vertex-scale measures are averaged over
            the graph). In contrast, Cluster 41 focuses on the same graphs
            and also contains the degree and strength, but as vertex-scale
            features this time. Put differently, Cluster 41 can be viewed
            as a vertex-scale counterpart of Cluster 9. This indicates that
            the microscopic characteristics of both the targeted vertex and
            the whole graph are relevant to our classification task.
            Cluster 10 is very large and contains almost only graphscale features. It focuses mainly on distance-based
            (diameter,

            PAPEGNIES et al.: CONVERSATIONAL NETWORKS FOR AUTOMATIC ONLINE MODERATION

            average distance, radius, closeness, eccentricity) and spectral
            (hub/authority, eigenvector, pagerank) macroscopic measures.
            Like the previous clusters, it essentially contains features
            computed on the After graph, but unlike them, it includes only
            a few features from the Full graph. Nevertheless, it appears
            as quite complementary of Cluster 9, in the sense it can be
            considered as its macroscopic counterpart.
            Cluster 49 suggests that the betweenness of the After
            and Full graphs mechanically increases with their number of
            vertices. But more importantly, it identifies the Vertex Count,
            i.e., the size of the conversation after the targeted message,
            as one of its most discriminative aspects, relatively to our classification task. The interpretation of
            Clusters 172 and 177 is
            even clearer, as each focus on a single measure (reciprocity and
            closeness), uniquely for the After graph. The bilateral nature
            of the exchanges after the targeted message, as well as how
            direct these are, can, therefore, also be considered as very
            important for the classification.
            Clusters 110 and 118 deal only with the Before graph.
            Cluster 110 includes variants of the weighted Closeness (both
            for the targeted vertex and in average) and Density. It can
            be considered as the Before counterpart of Cluster 177, which
            also focuses on the weighted Closeness but for the After graph.
            Cluster 118 contains distance-based and spectral macroscopic
            measures, mainly describing the whole graph. Thus, although
            much smaller, it can be seen as the Before counterpart of
            Cluster 10, semantically speaking.
            Finally, Cluster 119 contains the unweighted hub and
            authority scores of the targeted vertex, for both After and
            Before graphs. It can be opposed to both Clusters 10 and 118,
            which also contain hub and authority for the After and
            Before graphs, respectively, but in their weighted and averaged
            versions.
            Let us summarize our observations. A number of composite
            clusters describe the After/Full (Clusters 9, 10, and 41) and
            Before (Cluster 118) graphs at various scales and scopes. Two
            clusters focus more precisely on the Closeness, for the Before
            (Cluster 110) and After (Cluster 177) graphs. We assume that
            the classifier is able to take advantage of this to compare
            various aspects of the graphs, be it in terms of scale (Clusters 9
            versus 41), scope (Clusters 9 versus 10) or time (Cluster 10
            versus 118 and 177 versus 110). This means that 1) temporal
            aspects are useful for this classification task and 2) an abuse
            case is reflected by its impact on both the position of the
            abusive user in the graph and the overall aspect of the
            conversation.
            Each remaining cluster (49, 119, and 172) focuses on a measure of the After graph, highlighting their
            contribution to class
            discrimination. We examine more thoroughly these features,
            by considering separately their distributions in the abuse and
            nonabuse classes. For the number of users in the conversation,
            it turns out these distributions are quite different: the vertex
            count is relatively homogeneous and centered around 40 for
            the abuse class, whereas its distribution is heterogeneous
            (closer to a power law) when there is no abuse, with a very
            large number of very small networks (less than five users).
            Looking at the reciprocity, there is again a relatively homogeneous distribution for the abuse class,
            centered around 0.7.

            53

            For the nonabuse class, a part of the distribution is quite
            similarly homogeneous (albeit around 0.6), but the large
            majority of instances have either a 0 or 1 Reciprocity, i.e., only
            unilateral or bilateral edges, respectively. After verification,
            the former case corresponds to conversations that come to an
            abrupt end, whereas the latter is just a normally functioning
            conversation. Both cases are more likely to happen when few
            users are involved, which is consistent with our observations
            regarding the Vertex Count. However, both features are only
            partially correlated, which shows that the abuse class cannot
            be reduced only to a question of number of users involved in
            the conversation.
            The closeness seems to have a special role, since its
            weighted variants constitute their own clusters for the Before
            (Cluster 110) and After (Cluster 177) graphs. By comparison,
            the average unweighted Closeness is correlated with many
            other features as it belongs to the large Cluster 10: this is
            consistent with our previous observation that certain weighted
            variants appear to be more informative. Further examination
            shows that the closeness follows a power law-like distribution
            in both classes, covering 3 orders of magnitude. However,
            this heterogeneity is much more marked in the case of the
            nonabuse class. Concretely, the closeness is generally higher
            for the abuse class. This means that the average distance
            between the author of the targeted message and the rest of
            the graph decreases in case of abuse. This user becomes less
            peripheral (or more central), and the same goes for the other
            users of the graph (in average). This fits in quite well with
            assumptions about how abuse impacts a discussion: an abuser
            would tend not to be peripheral in a conversation, while we
            can reasonably assume that the other participants will be piling
            on and, therefore, be less peripheral themselves.
            Most mesoscopic measures are discarded during the feature
            elimination process. The only remaining ones are the clique
            count and the coreness, which are also the only ones not
            related to community structure. Yet, we had considered them
            as promising in Section IV-B, because they are uncorrelated
            with the others: it turns out the unique information they convey
            does not help solving this specific classification task. Inspection reveals that the modularity measure is
            overwhelmingly
            close to zero in both classes. This means that our networks
            generally do not have any community structure, which explains
            why the related features are not discriminative here.
            We also have identified and studied the clusters corresponding to the TFs of the Before, After, and Full
            feature sets. For
            matters of space, we do not present them in detail, though,
            and only discuss our most interesting observations. Certain
            clusters identified for the All feature set also appear for the
            other sets: those focusing on the considered type of graph,
            i.e., Clusters 110 and 118 for Before; 10, 41, 49, and 177 for
            After, and 9, 10, 41, and 49 for Full. Some of the missing clusters are replaced by semantically close and
            relatively correlated
            clusters. For instance, Before has a cluster containing exactly
            the same measure variants as Cluster 49 (vertex count and
            betweenness), but for the Before graph. Similarly, Full has a
            cluster focusing on the weighted closeness, as Cluster 177 does
            for After. We interpret Clusters 9 and 41 as describing the
            microscopic aspects of the After graph at the graph and vertex

            54

            IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 6, NO. 1, FEBRUARY 2019

            TABLE III
            B EST P ERFORMANCES FOR THE BASELINES AND C URRENT F RAMEWORK

            scale, respectively: Before has comparable clusters focusing
            similarly on the Before graph. Overall, we can say that when
            focusing on a specific type of graph (by opposition to All),
            the classifier takes advantage of informationally close clusters,
            albeit inferior in terms of discriminative power, as they lead
            to a lower performance.
            G. Baseline Comparison
            For matters of exhaustiveness, we assess the performance
            of our framework on a balanced version of our classes, instead
            of the unbalanced ones used throughout this section. In this
            setting, the abuse class stays the same, but the nonabuse class
            is reduced to the size of the abuse one, through sampling.
            When using these data, we observe a significant performance
            improvement for all feature sets. In particular, the F-measure
            values obtained for All and All-TF increase from 83.99 and
            82.65 to 88.87 and 87.10, respectively. Further investigations
            shows that this improvement is mainly due to a decrease in
            the number of false positives, itself caused by the smaller size
            of the nonabuse class. Such a balanced situation is unlikely in
            practice, though.
            Finally, we compare the results obtained using our framework with our two baselines (Table III): the
            content-based
            approach of [17] and the previous version of our graph-based
            method [3]. As a reminder, the main differences between the
            latter and our present framework are that we now extract a
            directed graph, and use a much larger number of topological measures as classification features. The
            combination of
            these two improvements leads to a significant performance
            increase over our previous effort. As described in Section IV-E,
            the contribution of edge directions to the overall performance
            is relatively minor. One could assume that the performance
            improvement is mainly caused by the major expansion of the
            feature set, however, this improvement is observed even when
            only using the TFs identified in Section IV-F. Yet, there are
            only 10 of them, by comparison to the 75 features used in [3].
            So the conclusion here is that both extracting a directed graph
            and selecting a more appropriate set of features (in particular,
            topological measures able to handle edge weights) helped
            improve the performance. More importantly, the performance
            is greatly improved compared to our content-based approach
            [17], which is quite representative of the preprocessing and
            features used in the literature when classifying such data. This
            is a major result, as it shows that the sole structure of the
            conversation is enough to efficiently detect abuses, without
            considering at all the content of the exchanged messages.
        </corps>
        <conclusion>
            V. C ONCLUSION
            In this paper, we tackle the problem of automatic abuse
            detection in online communities. We propose to model online

            conversations through graphs, and to perform the classification
            task using only graph-based features. The method, while
            simple, yields good results (up to a 83.89 F-measure), besting
            the score obtained with a content-based approach [17] and
            our previous graph-based effort [3]. It completely ignores the
            content of messages exchanged between users of an online
            community, which means it is robust to intentional obfuscation
            of messages by abusive users, as well as unintentional content
            noise. It is also inherently language independent. One important limitation of our method is the high
            computational time
            required to extract the features. However, we show that it can
            be very significantly reduced by working with a small subset
            of relevant features, resulting in more than 97% of the original
            performance for less than 0.01 of the processing time.
            We also show that while our method is originally not designed
            for real-time abuse detection, the information available at the
            time the message appears is discriminative enough to do so.
            A straightforward extension of our work is to take advantage of both content- and graph-based features, an
            approach
            previously applied in other contexts [57]. In our case, they are
            both based on completely different types of information, so we
            can assume they are complementary, which could improve
            the classification performance. At the very least, it will be
            interesting to combine the features of the Before graph with
            textual features since that can lead to a system useful for
            a prediction task. We also consider using a content-based
            classifier in a completely different ways, during the graph
            extraction process. Such a classifier could be trained to detect
            the nature of the interaction between two users, allowing to
            extract a signed network (negative edge for a hostile exchange,
            positive otherwise). This additional information is likely to
            improve the performance of our graph-based classifier.
            Finally, part of the future work will focus on applying
            our proposed approaches to other types of social network
            corpora. Indeed, chat logs are a special case of communications records that have a very specific structure
            (entanglement of discussions, near-synchronous communications,
            various topics in a single flow of discussion, uncertainty
            about who is the intended recipient of a message...) which
            do not necessarily appear in other forms of social networks,
            such as forums or microblogs. For example, since our results
            have shown that directionality is not the dominant graph
            construction parameter, we would be interested in evaluating
            its impact on a type of social media integrating a clear response
            structure (i.e., a clear identification of who answers whom,
            such as in a forum like Reddit or a tree-shaped comment
            section of a news website). In addition, another type of social
            network corpora might present a more distinct community
            structure and, therefore, render the meso-scale features we
            have presented more relevant.
        </conclusion>
        <discussion>
            Aucune discussion trouvée.
        </discussion>
        <biblio>
            R EFERENCES
            [1] French Republic. (2004). Loi n◦ 2004-575 du 21 Juin 2004 Pour la
            Confiance dans L’économie Numérique—Article 6. [Online]. Available:
            https://www.legifrance.gouv.fr/affichTexteArticle.do?idArticle=
            LEGIARTI000023711900&cidTexte=LEGITEXT000005789847
            [2] French Republic. (1982). Loi n◦ 82-652 du 29 Juillet 1982 sur la
            Communication Audiovisuelle—Article 93-3. [Online]. Available:
            https://www.legifrance.gouv.fr/affichTexteArticle.do?idArticle=
            LEGIARTI000020740559&cidTexte=LEGITEXT000006068759

            PAPEGNIES et al.: CONVERSATIONAL NETWORKS FOR AUTOMATIC ONLINE MODERATION

            [3] E. Papegnies, V. Labatut, R. Dufour, and G. Linarès, “Graph-based
            features for automatic online abuse detection,” in Proc. Int. Conf. Stat.
            Lang. Speech Process. Berlin, Germany: Springer, 2017, pp. 70–81.
            [4] E. Spertus, “Smokey: Automatic recognition of hostile messages,”
            in Proc. 14th Nat. Conf. Artif. Intell. 9th Conf. Innov. Appl. Artif.
            Intell. (AAAI), 1997, pp. 1058–1065.
            [5] Y. Chen, Y. Zhou, S. Zhu, and H. Xu, “Detecting offensive language
            in social media to protect adolescent online safety,” in Proc. IEEE Int.
            Conf. Privacy, Secur., Risk Trust Int. Conf. Social Comput., Sep. 2012,
            pp. 71–80.
            [6] K. Dinakar, R. Reichart, and H. Lieberman, “Modeling the detection
            of textual cyberbullying,” in Proc. 5th Int. AAAI Conf. Weblogs Social
            Media/Workshop Social Mobile Web, 2011, pp. 11–17.
            [7] V. S. Chavan and S. S. Shylaja, “Machine learning approach for detection
            of cyber-aggressive comments by peers on social media network,” in
            Proc. IEEE Int. Conf. Adv. Comput., Commun. Inform., Aug. 2015,
            pp. 2354–2358.
            [8] H. Mubarak, K. Darwish, and W. Magdy, “Abusive language detection
            on Arabic social media,” in Proc. 1st Workshop Abusive Lang. Online,
            2017, pp. 52–56.
            [9] A. H. Razavi, D. Inkpen, S. Uritsky, and S. Matwin, “Offensive language
            detection using multi-level classification,” in Proc. Can. Conf. Artif.
            Intell. Berlin, Germany: Springer, 2010, pp. 16–27.
            [10] N. Djuric, J. Zhou, R. Morris, M. Grbovic, V. Radosavljevic, and
            N. Bhamidipati, “Hate speech detection with comment embeddings,” in Proc. ACM 24th Int. Conf. World Wide
            Web, 2015,
            pp. 29–30.
            [11] J. H. Park and P. Fung, “One-step and two-step classification for abusive
            language detection on Twitter,” in Proc. 1st Workshop Abusive Lang.
            Online, 2017, pp. 41–45.
            [12] J. Pavlopoulos, P. Malakasiotis, and I. Androutsopoulos, “Deep learning
            for user comment moderation,” in Proc. 1st Workshop Abusive Lang.
            Online, 2017, pp. 25–35.
            [13] H. Hosseini, S. Kannan, B. Zhang, and R. Poovendran. (2017). “Deceiving Google’s perspective API built
            for detecting toxic comments.”
            [Online]. Available: https://arxiv.org/abs/1702.08138
            [14] H. Lee and A. Y. Ng, “Spam deobfuscation using a hidden Markov
            model,” in Proc. 2nd Conf. Email Anti-Spam, 2005, pp. 1–8.
            [15] S. Rojas-Galeano, “On obstructing obscenity obfuscation,” ACM Trans.
            Web, vol. 11, no. 2, p. 12, 2017.
            [16] D. Yin, Z. Xue, L. Hong, B. D. Davison, A. Kontostathis, and
            L. Edwards, “Detection of harassment on Web 2.0,” in Proc. Content
            Anal. WEB, 2009, pp. 1–7
            [17] E. Papegnies, V. Labatut, R. Dufour, and G. Linares, “Impact of content
            features for automatic online abuse detection,” in Proc. Int. Conf.
            Comput. Linguistics Intell. Text Process. Berlin, Germany: Springer,
            2017, pp. 404–419.
            [18] J. Cheng, C. Danescu-Niculescu-Mizil, and J. Leskovec, “Antisocial
            behavior in online discussion communities,” in Proc. Int. AAAI Conf.
            Web Social Media, 2015, pp. 61–70.
            [19] K. Balci and A. A. Salah, “Automatic analysis and identification of
            verbal aggression and abusive behaviors for online social games,”
            Comput. Hum. Behav., vol. 53, pp. 517–526, Dec. 2015.
            [20] P. Mutton, “Inferring and visualizing social networks on Internet
            relay chat,” in Proc. IEEE 8th Int. Conf. Inf. Vis., Jul. 2004,
            pp. 35–43.
            [21] O. I. Osesina, J. P. McIntire, P. R. Havig, E. E. Geiselman, C. Bartley,
            and M. E. Tudoreanu, “Methods for extracting social network data from
            chatroom logs,” Proc. SPIE, vol. 8389, p. 83891H, Jun. 2012. [Online].
            Available:
            https://www.spiedigitallibrary.org/conferenceproceedings-ofspie/8389/83891H/Methods-for-extracting-social-network-data-fromchatroom-logs/10.1117/12.920019.short?SSO=1
            [22] A. Gruzd and C. Haythornthwaite, “Automated discovery and
            analysis of social networks from threaded discussions,” in Proc.
            Int. Netw. Social Netw. Anal. Conf., 2008. [Online]. Available:
            https://repository.arizona.edu/handle/10150/105081
            [23] A. Çamtepe, M. S. Krishnamoorthy, and B. Yener, “A tool for Internet
            chatroom surveillance,” in Proc. Int. Conf. Intell. Secur. Inform. Berlin,
            Germany: Springer, 2004, pp. 252–265.
            [24] M. Forestier, J. Velcin, and D. Zighed, “Extracting social networks
            to understand interaction,” in Proc. Int. Conf. Adv. Social Netw. Anal.
            Mining, Jul. 2011, pp. 213–219.
            [25] S. Tavassoli, M. Moessner, and K. A. Zweig, “Constructing social
            networks from semi-structured chat-log data,” in Proc. IEEE/ACM Int.
            Conf. Adv. Social Netw. Anal. Mining, Aug. 2014, pp. 146–149.
            [26] T. Sinha and I. Rajasingh, “Investigating substructures in goal oriented
            online communities: Case study of Ubuntu IRC,” in Proc. IEEE Int.
            Adv. Comput. Conf., Feb. 2014, pp. 916–922.

            55

            [27] T. Anwar and M. Abulaish, “A social graph based text mining framework
            for chat log investigation,” Digit. Invest., vol. 11, no. 4, pp. 349–362,
            2014.
            [28] K. Garimella, G. De Francisci Morales, A. Gionis, and M. Mathioudakis,
            “Quantifying controversy on social media,” in Proc. 9th ACM Int. Conf.
            Web Search Data Mining, 2015, pp. 33–42.
            [29] D. R. White and F. Harary, “The cohesiveness of blocks in social
            networks: Node connectivity and conditional density,” Sociol. Methodol.
            Banner, vol. 31, no. 1, pp. 305–359, 2001.
            [30] M. E. J. Newman and M. Girvan, “Finding and evaluating community
            structure in networks,” Phys. Rev. E, Stat. Phys. Plasmas Fluids Relat.
            Interdiscip. Top., vol. 69, p. 026113, Feb. 2004.
            [31] R. D. Luce and A. D. Perry, “A method of matrix analysis of group
            structure,” Psychometrika, vol. 14, no. 2, pp. 95–116, 1949.
            [32] S. Wasserman and K. Faust, Social Network Analysis: Methods
            and Applications, vol. 8. Cambridge, U.K.: Cambridge Univ. Press,
            1994,
            [33] M. E. J. Newman, “Assortative mixing in networks,” Phys. Rev. Lett.,
            vol. 89, no. 20, p. 208701, Oct. 2002.
            [34] P. Bonacich, “Factoring and weighting approaches to status scores and
            clique identification,” J. Math. Sociol., vol. 2, no. 1, pp. 113–120, 1972.
            [35] J. M. Kleinberg, “Authoritative sources in a hyperlinked environment,”
            J. ACM , vol. 46, no. 5, pp. 604–632, 1999.
            [36] L. Katz, “A new status index derived from sociometric analysis,”
            Psychometrika, vol. 18, no. 1, pp. 39–43, 1953.
            [37] P. Bonacich, “Power and centrality: A family of measures,” Amer.
            J. Sociol., vol. 92, no. 5, pp. 1170–1182, 1987.
            [38] S. Brin and L. Page, “The anatomy of a large-scale hypertextual
            Web search engine,” Comput. Netw. ISDN Syst., vol. 30, nos. 1–7,
            pp. 107–117, Apr. 1998.
            [39] E. Estrada and J. A. Rodríguez-Velázquez, “Subgraph centrality in
            complex networks,” Phys. Rev. E, Stat. Phys. Plasmas Fluids Relat.
            Interdiscip. Top., vol. 71, no. 5, p. 056103, 2005.
            [40] L. C. Freeman, “A set of measures of centrality based on betweenness,”
            Sociometry, vol. 40, no. 1, pp. 35–41, Mar. 1977.
            [41] A. Bavelas, “Communication patterns in task-oriented groups,”
            J. Acoust. Soc. Amer., vol. 22, no. 6, pp. 725–730, 1950.
            [42] F. Harary, Graph Theory. Reading, MA, USA: Addison-Wesley, 1969.
            [43] S. B. Seidman, “Network structure and minimum degree,” Social Netw.,
            vol. 5, no. 3, pp. 269–287, 1983.
            [44] R. Guimerà and L. A. N. Amaral, “Functional cartography of
            complex metabolic networks,” Nature, vol. 433, pp. 895–900,
            Feb. 2005.
            [45] V. Labatut, N. Dugué, and A. Perez, “Identifying the community roles
            of social capitalists in the Twitter network,” in Proc. IEEE/ACM Int.
            Conf. Adv. Social Netw. Anal. Mining, Aug. 2014, pp. 371–374.
            [46] M. E. Shaw, “Group structure and the behavior of individuals in small
            groups,” J. Psychol., vol. 38, no. 1, pp. 139–149, 1954.
            [47] A. Barrat, M. Barthélemy, R. Pastor-Satorras, and A. Vespignani,
            “The architecture of complex weighted networks,” Proc. Nat. Acad. Sci.
            USA, vol. 101, no. 11, pp. 3747–3752, Mar. 2004.
            [48] D. J. Watts and S. H. Strogatz, “Collective dynamics of ‘small-world’
            networks,” Nature, vol. 393, no. 6684, pp. 440–442, 1998.
            [49] R. S. Burt, “Structural holes and good ideas1,” Amer. J. Sociol., vol. 110,
            no. 2, pp. 349–399, 2004.
            [50] L. C. Freeman, “Centrality in social networks conceptual clarification,”
            Social Netw., vol. 1, no. 3, pp. 215–239, 1979.
            [51] P. Bonacich and P. Lloyd, “Eigenvector-like measures of centrality
            for asymmetric relations,” Social Netw., vol. 23, no. 3, pp. 191–201,
            2001.
            [52] M. Rosvall and C. T. Bergstrom, “Maps of random walks on complex
            networks reveal community structure,” Proc. Nat. Acad. Sci. USA,
            vol. 105, no. 4, pp. 1118–1123, 2008.
            [53] N. Dugué, V. Labatut, and A. Perez, “A community role approach to
            assess social capitalists visibility in the Twitter network,” Social Netw.
            Anal. Mining, vol. 5, p. 26, Dec. 2015.
            [54] G. Csardi and T. Nepusz, “The igraph software package for
            complex network research,” Inter J. Complex Syst., vol. 1695,
            no. 5, pp. 1–9, 2006. [Online]. Available: http://www.interjournal.
            org/manuscript_abstract.php?361100992
            [55] F. Pedregosa et al., “Scikit-learn: Machine learning in Python,” J. Mach.
            Learn. Res., vol. 12, pp. 2825–2830, Oct. 2011.
            [56] P. J. Rousseeuw, “Silhouettes: A graphical aid to the interpretation and
            validation of cluster analysis,” J. Comput. Appl. Math., vol. 20, no. 1,
            pp. 53–65, 1987.
            [57] A. Rumshisky et al., “Combining network and language indicators for
            tracking conflict intensity,” in Proc. Int. Conf. Social Inform. Berlin,
            Germany: Springer, 2017, pp. 391–404.
        </biblio>
    </article>
    <article>
        <preamble>Exact_Clustering_via_Integer_Programming_and_Maximum_Satisfiability.pdf</preamble>
        <titre>Exact Clustering via Integer Programming and Maximum Satisfiability</titre>
        <auteur>
            Atsushi Miyauchi (RIKEN AIP, Tokyo, Japan),
            Tomohiro Sonobe (National Institute of Informatics & JST ERATO, Tokyo, Japan),
            Noriyoshi Sukegawa (Chuo University, Tokyo, Japan)
        </auteur>
        <abstract>
            We consider the following general graph clustering problem:
            given a complete undirected graph G = (V, E, c) with an
            edge weight function c : E → Q, we are asked to ﬁnd a
            partition C of V that maximizes the sum of edge weights
            within the clusters in C. Owing to its high generality, this
            problem has a wide variety of real-world applications, including correlation clustering, group technology,
            and community
            detection. In this study, we investigate the design of mathematical programming formulations and constraint
            satisfaction
            formulations for the problem. First, we present a novel integer linear programming (ILP) formulation that
            has far fewer
            constraints than the standard ILP formulation by Grötschel
            and Wakabayashi (1989). Second, we propose an ILP-based
            exact algorithm that solves an ILP problem obtained by modifying our above ILP formulation and then performs
            simple
            post-processing to produce an optimal solution to the original
            problem. Third, we present maximum satisﬁability (MaxSAT)
            counterparts of both our ILP formulation and ILP-based exact
            algorithm. Computational experiments using well-known realworld datasets demonstrate that our ILP-based
            approaches and
            their MaxSAT counterparts are highly effective in terms of
            both memory efﬁciency and computation time.

        </abstract>
        <introduction>
            Clustering is a fundamental tool in data analysis. Roughly
            speaking, the task of clustering is to divide a given set of
            objects into subsets of homogeneous objects. To date, various problem settings and optimization algorithms
            have been
            extensively studied (Aggarwal and Reddy 2013; Jain, Murty,
            and Flynn 1999; Xu and Wunsch 2005).
            Let us consider the following general graph clustering
            problem. An instance is a complete undirected graph G =
            (V, E, c) with an edge weight function c : E → Q, where
            Q is the set of rational numbers. For simplicity, we denote
            cij = c({i, j}) for each {i, j} ∈ E. The edge weight
            cij expresses the degree of preference that i, j ∈ V are
            assigned to the same cluster; if cij is positive, we wish
            to assign i, j ∈ V to the same cluster, whereas if cij is
            negative, we wish to assign i, j ∈ V to different clusters.
            The goal is to ﬁnd a partition C = {V1 , V2 , . . . , Vk } (i.e.,

            Mathematical programming formulations. Grötschel
            and Wakabayashi (1989) introduced a 0-1 integer linear
            programming (ILP) formulation for CPP, which has been
            employed by many algorithms for CPP and its variants
            (e.g., (Agarwal and Kempe 2008; Bruckner et al. 2013;
            Jaehn and Pesch 2013; Miyauchi and Miyamoto 2013;
            Nowozin and Jegelka 2009; Oosten, Rutten, and Spieksma
            2001; Van Gael and Zhu 2007)). The ILP formulation is
            simple and intuitive, but not sufﬁciently scalable for realworld applications in terms of both memory
            requirements
            and computation time. In particular, the issue of memory
            requirements is quite
            serious. Letting n = |V |, the ILP for
            mulation has 3 n3 = Θ(n3 ) constrains, which grows rapidly
            as n increases. For example, if n = 1,000, the number of con-

            c 2018, Association for the Advancement of Artiﬁcial
            Copyright 
            Intelligence (www.aaai.org). All rights reserved.

            1387

            straints reaches the order of half a billion; it is quite difﬁcult
            to store such an ILP formulation on a standard computer.
            To overcome this issue, much effort has been dedicated
            to constructing ILP formulations for CPP with fewer constraints. Dinh and Thai (2015) addressed a special
            case of
            CPP, which is called the modularity maximization problem (Fortunato 2010; Newman and Girvan 2004), and
            derived a set of redundant constraints in the ILP formulation
            by Grötschel and Wakabayashi (1989). By removing the constraints in advance, they introduced an ILP
            formulation with
            fewer constraints for the special case. Recently, Miyauchi
            and Sukegawa (2015b) generalized Dinh and Thai’s result to
            CPP. If m≥0 denotes the number of nonnegative-weighted
            edges in G, i.e., m≥0 = |{{i, j} ∈ E : cij ≥ 0}|, the
            ILP formulation by Miyauchi and Sukegawa (2015b) has
            O(nm≥0 ) constraints, which improves on the ILP formulation by Grötschel and Wakabayashi (1989) for the
            case in
            which m≥0 is not large (e.g., m≥0 = O(n)).
            However, for most real-world instances of CPP, the parameter m≥0 is large owing to the large number of edges
            with weight zero; thus, the beneﬁt of the above reformulation
            is quite limited for real-world applications. In fact, computational experiments in Miyauchi and Sukegawa
            (2015b)
            demonstrated that the decrease in the number of constraints—
            which is about 20% at most—is not signiﬁcant. Moreover,
            their reformulation does not reduce computation time signiﬁcantly; in fact, in some cases computation time
            increases.

            formulations for CPP. Our contribution can be summarized
            as follows:
            1. We present a novel ILP formulation for CPP in which
            the number of constraints is O(nm>0 ), where m>0 is
            the number of positive-weighted edges in G, i.e., m>0 =
            |{{i, j} ∈ E : cij > 0}|.
            2. We also propose an ILP-based exact algorithm for CPP.
            The algorithm ﬁrst solves an ILP problem obtained by
            modifying our above ILP formulation and then performs
            simple post-processing to obtain an optimal solution to
            CPP.
            3. We present MaxSAT counterparts of both our ILP formulation and ILP-based exact algorithm.
            4. We conduct thorough experiments to evaluate the effectiveness of our ILP-based approaches and their
            MaxSAT
            counterparts in terms of both memory efﬁciency and computation time.
            We ﬁrst describe our ﬁrst result above in detail. To design
            our ILP formulation, we effectively use the above result by
            Miyauchi and Sukegawa (2015b). Recall that they proposed
            an ILP formulation for CPP that has O(nm≥0 ) constraints.
            The serious problem with their formulation is that for most
            real-world instances of CPP, the parameter m≥0 is large
            owing to the large number of edges with weight zero. The
            critical idea behind the design of our ILP formulation is to
            perturb the edge weight function of a given instance so that
            all edges with weight zero have some negative weight. By
            doing this, the resulting instance has small m≥0 ; thus, the
            ILP formulation by Miyauchi and Sukegawa (2015b) for the
            resulting instance, which is our proposed ILP formulation for
            the original instance, has far fewer constraints. Our theoretical analysis demonstrates that if the negative
            values used for
            the perturbation are close to zero, the proposed formulation
            obtains an optimal solution to the original instance.
            We describe our second result in detail. In our ILP formulation above, it is necessary to deal with some
            negative
            perturbation values very close to zero. Unfortunately, such
            values may cause numerical instability and therefore increase
            computation time in practice; hence, such perturbation should
            be avoided if possible. To this end, we introduce an ILP problem that is identical to our above ILP
            formulation for CPP
            except that it uses an unperturbed objective function. This
            modiﬁed ILP problem also has O(nm>0 ) constraints but
            does not depend on the perturbation. However, the ILP problem itself is incomplete as an ILP formulation for
            CPP; in
            fact, an optimal solution to the ILP problem may be infeasible for CPP. Thus, to obtain an optimal solution
            to CPP, the
            algorithm also performs simple post-processing that runs in
            linear time.
            We next describe our third result in detail. As mentioned
            above, Berg and Järvisalo (2017) introduced a MaxSAT formulation called MaxSAT-Transitive, which is the
            MaxSAT
            counterpart of the ILP formulation by Grötschel and Wakabayashi (1989). That is, MaxSAT-Transitive uses
            hard clauses
            to represent the constraints in the ILP formulation and soft
            clauses associated with appropriate weights to represent its
            objective function. Beginning with MaxSAT-Transitive, we

            Constraint satisfaction formulations. Very recently,
            Berg and Järvisalo (2017) developed MaxSAT formulations
            for an optimization problem called the weighted correlation
            clustering problem (WCC). It should be noted that WCC is
            equivalent to CPP from an exact optimization perspective
            (see Example 1 in Section 2). Thus, the MaxSAT formulations by Berg and Järvisalo (2017) for WCC can be
            seen as
            exact formulations for CPP. WCC and its variants have been
            actively studied in the ﬁeld of artiﬁcial intelligence (Ahn
            et al. 2015; Awasthi, Balcan, and Voevodski 2014; Bansal,
            Blum, and Chawla 2004; Bonchi, Gionis, and Ukkonen 2013;
            Chierichetti, Dalvi, and Kumar 2014; Kim et al. 2014;
            Puleo and Milenkovic 2016; Van Gael and Zhu 2007).
            Speciﬁcally, Berg and Järvisalo (2017) developed three
            MaxSAT formulations: MaxSAT-Transitive, MaxSAT-Unary,
            and MaxSAT-Binary. MaxSAT-Transitive is the MaxSAT counterpart of the ILP formulation by Grötschel and
            Wakabayashi (1989). MaxSAT-Unary and MaxSAT-Binary are
            MaxSAT formulations that employ the unary encoding and binary encoding techniques, respectively. In their
            experiments,
            the MaxSAT formulations were compared with the ILP formulation by Grötschel and Wakabayashi (1989). The
            results
            of their experiments showed that MaxSAT-Binary outperforms
            both the other MaxSAT formulations and the ILP formulation.
            Thus, MaxSAT-Binary is known to be state-of-the-art in terms
            of exact formulation for WCC and thus for CPP.
        </introduction>
        <corps>
            Our Contribution

            In this study, we further investigate the design of mathematical programming formulations and constraint
            satisfaction

            1388

            can reproduce our results for ILP in the context of MaxSAT;
            speciﬁcally, we can obtain MaxSAT counterparts of both our
            ILP formulation and ILP-based exact algorithm.
            Finally, we describe our fourth result in detail. In a series
            of experimental assessments, we compare our ILP-based approaches and their MaxSAT counterparts with the
            previous
            formulations using well-known real-world datasets arising
            in the context of correlation clustering, group technology,
            and community detection. The results demonstrate that our
            approaches signiﬁcantly outperform the state-of-the-art formulations in terms of both memory efﬁciency and
            computation time. In particular, our ILP-based approaches can solve
            a real-world instance with a few thousand vertices for which
            the ILP formulation by Grötschel and Wakabayashi (1989)
            has more than eight billion constraints.

            2

            tems (Groover 2007). Suppose that the goal is to develop
            a manufacturing system for some new product, comprising p
            parts that are processed by q machines. In such a situation,
            the goal of group technology is to ﬁnd a suitable partition of
            the set of parts and machines needed to deﬁne an efﬁcient
            cellular manufacturing system.
            As mentioned in Oosten, Rutten, and Spieksma (2001),
            group technology can be modeled as CPP. An instance G =
            (V, E, c) of CPP is constructed as follows: Let V be a union
            of the set of p parts and the set of q machines. An edge
            {i, j} ∈ E between a part i and a machine j has weight 1 if i
            is processed by j and −1 otherwise. Each edge between two
            parts or two machines has weight zero.
            Example 3 (Community detection). Community detection
            is a fundamental task in network analysis that aims to ﬁnd
            a partition of the set of vertices into communities (Fortunato 2010). Intuitively speaking, a community is a
            subset
            of vertices densely connected internally but sparsely connected with the rest of the network. Recently, the
            issue of
            community detection in bipartite networks has garnered a
            signiﬁcant amount interest (Fortunato 2010). Barber (2007)
            introduced a quality function called the bipartite modularity, which is appropriate for community detection
            in bipartite networks. Let G = (V  , E  ) be an undirected bipartite graph for which it is known that V 
            is divided into
            VL and VR so that each edge has one endpoint in VL and
            the other in VR . The bipartite modularity, a quality function for a partition C of V  , can
             be written as Qb (C) =
            
            
            di dj
            1
            i∈V 
            j∈V  Aij − |E  | δ(C(i), C(j)), where Aij
            |E  |

            Application Examples

            CPP is a general clustering problem and therefore has a wide
            variety of applications. Here we provide some important
            application examples.
            Example 1 (Correlation clustering). Correlation clustering
            was introduced by Bansal, Blum, and Chawla (2004) for
            clustering with qualitative information. As an example, we
            consider document clustering, in which a set of documents is
            to be clustered into topics, with the hindering constraint that
            there is no exact prior deﬁnition of what a “topic” constitutes.
            Alternatively, it can be assumed that there exists qualitative
            similarity information indicating that a number of pairs of
            documents are similar or dissimilar. In such a situation, the
            goal of correlation clustering is to ﬁnd a partition of the set of
            documents that agrees as much as possible with the similarity
            information.
            A mathematical formulation of correlation clustering is as
            follows: Let G = (V  , E+ , E− ) be an edge-labeled undirected graph in which each edge e ∈ E+ is labeled
            as “+”
            (similar) and each edge e ∈ E− is labeled as “−” (dissimilar). Note that E+ ∩ E− = ∅ holds. The maximization
            version, M AX AGREE, asks for a partition C of V  that maximizes agreements (the number of + edges within
            clusters
            plus the number of − edges across clusters). The minimization version, M IN D ISAGREE, asks for a partition
            C of V 
            that minimizes disagreements (the number of − edges within
            clusters plus the number of + edges across clusters). These
            problems are equivalent in terms of optimality and are both
            NP-hard (Bansal, Blum, and Chawla 2004). WCC, which was
            mentioned above, deals with edge-weighted generalizations
            of both M AX AGREE and M IN D ISAGREE.
            M AX AGREE and M IN D ISAGREE can be reduced to CPP.
            An instance G = (V, E, c) of CPP is constructed as follows: Let V = V  . For each {i, j} ∈ E, we set cij =
            1 if
            {i, j} ∈ E+ , cij = −1 if {i, j} ∈ E− , and cij = 0 otherwise. Clearly, an optimal solution to CPP
            corresponds to an
            optimal solution to both M AX AGREE and M IN D ISAGREE. It
            should be noted that WCC can also be reduced to CPP and
            CPP can be reduced to WCC.
            Example 2 (Group technology). Group technology plays
            a key role in the design of efﬁcient manufacturing sys-

            L

            R

            is the (i, j) component of the adjacency matrix of G and di is
            the degree of i ∈ V  . The bipartite modularity maximization
            problem is NP-hard (Miyauchi and Sukegawa 2015a).
            The problem can be reduced to CPP. An instance G =
            (V, E, c) of CPP is constructed as follows: Let V = V  . An
            edge {i, j} ∈ E between i ∈ VL and j ∈ VR has weight
            Aij
            di dj
            
            |E  | − |E  |2 . Each edge between two vertices in VL or two
            
            vertices in VR has weight zero.

            3

            ILP Formulation

            We ﬁrst revisit the standard formulation by Grötschel and
            Wakabayashi (1989) and the reformulation by Miyauchi and
            Sukegawa (2015b). Let V = {1, 2, . . . , n} and P = {(i, j) :
            1 ≤ i
            < j
            ≤ n}. For each (i, j) ∈ P , we introduce a decision
            variable xij equal to 1 if i, j ∈ V are in the same cluster and
            0 otherwise. Then the ILP formulation by Grötschel and
            Wakabayashi (1989) can be represented as follows:
            
            P(G) : max.
            cij xij
            (i,j)∈P

            xij + xjk − xik ≤ 1 ∀(i, j, k) ∈ T,
            xij − xjk + xik ≤ 1 ∀(i, j, k) ∈ T,
            −xij + xjk + xik ≤ 1 ∀(i, j, k) ∈ T,
            ∀(i, j) ∈ P,
            xij ∈ {0, 1}
            where T = {(i, j, k) : 1 ≤ i
            < j
            < k
            ≤ n}. The triangle
            inequality constraints stipulate that for any i, j, k ∈ V , if
            s. t.

            1389

            i, j ∈ V are in the same cluster and j, k ∈ V are also in
            the same cluster, then i, k ∈
             
             V must be in the same cluster.
            The ILP formulation has n2 = Θ(n2 ) variables and 3 n3 =
            Θ(n3 ) triangle inequality constraints.
            Miyauchi and Sukegawa (2015b) derived a set of redundant triangle inequality constraints in P(G). By
            removing
            the constraints in advance, they introduced the following ILP
            formulation:
            
            RP(G) : max.
            cij xij

            We now show the optimality of x to P(G). Let x =
            (xij )(i,j)∈P be an optimal solution to P(G) and a its objective
            value in P(G). Since all the constraints in RP(G) are also contained in P(G), the solution x is feasible for
            RP(G). The ob
            jective value of x in RP(G), i.e., (i,j)∈P cij xij , is strictly
            greater than a − 1 because the decrement, due to the change
            from P(G)
             G),
            n of the objective value of x is at most
             toRP(
            n
            ·
            ·|E0 |
            <
            1/
            2 = 1. As for the objective
             2
             value of x in
            P(G), i.e., (i,j)∈P cij xij , we have a ≥ (i,j)∈P cij xij ≥
            
            
            (i,j)∈P cij xij ≥
            (i,j)∈P cij xij > a − 1, where the ﬁrst
            inequality follows from the feasibility of x in P(G), and the
            third inequality follows from the optimality and feasibility of
            x and x, respectively, in RP(G). Since
            the objective value
            of x in P(G) is an integer, we have (i,j)∈P cij xij = a.
            Therefore, x is optimal to P(G).

            (i,j)∈P

            s. t.

            1
            xij + xjk − xik ≤ 1 ∀(i, j, k) ∈ T≥0
            ,
            2
            xij − xjk + xik ≤ 1 ∀(i, j, k) ∈ T≥0
            ,
            3
            −xij + xjk + xik ≤ 1 ∀(i, j, k) ∈ T≥0
            ,

            xij ∈ {0, 1}

            ∀(i, j) ∈ P,

            1
            2
            = {(i, j, k) ∈ T : cij ≥ 0 or cjk ≥ 0}, T≥0
            =
            where T≥0
            3
            {(i, j, k) ∈ T : cij ≥ 0 or cik ≥ 0}, and T≥0 = {(i, j, k) ∈
            T : cjk ≥ 0 or cik ≥ 0}. They proved the following theorem:
            Theorem 1 (Theorem 1 in Miyauchi and Sukegawa (2015b)).
            Let G = (V, E, c) be an arbitrary instance of CPP. P(G) and
            RP(G) share the same set of optimal solutions.
            Therefore, we can solve RP(G) instead of P(G) to obtain an optimal solution to CPP. Note that the number of
            triangle inequality constraints in RP(G) can be evaluated
            as O(nm≥0 ), where m≥0 is the number of nonnegativeweighted edges in G, i.e., m≥0 = |{(i, j) ∈ P : cij ≥
            0}|.

            3.1

            4

            We introduce the following ILP problem:
            
            RP∗ (G) : max.
            cij xij
            (i,j)∈P

            s. t.

            3
            ,
            −xij + xjk + xik ≤ 1 ∀(i, j, k) ∈ T>0
            xij ∈ {0, 1}
            ∀(i, j) ∈ P,
            1
            2
            where T>0
            = {(i, j, k) ∈ T : cij > 0 or cjk > 0}, T>0
            =
            3
            {(i, j, k) ∈ T : cij > 0 or cik > 0}, and T>0
            = {(i, j, k) ∈
            T : cjk > 0 or cik > 0}. Note here that the set of constraints
            is the same as in RP(G), whereas the objective function is
            the same as in P(G) and RP(G), i.e., the unperturbed one.
            Unfortunately, RP∗ (G) may fail to obtain an optimal
            solution to P(G). In fact, there exist instances such that
            an optimal solution to RP∗ (G) is infeasible for P(G). For
            example, consider G = (V, E, c) with V = {1, 2, 3, 4},
            c12 = 1, c13 = c23 = −1, and c14 = c24 = c34 = 0.
            A 0-1 vector x∗ = (x∗ij ) such that x∗12 = x∗14 = x∗24 =
            x∗34 = 1 and x∗13 = x∗23 = 0 is one of the optimal solutions to RP∗ (G); however, the triangle inequality
            constraint
            −x13 + x34 + x14 ≤ 1 in P(G) is violated.
            To obtain an optimal solution to P(G), we perform the
            following simple post-processing, which we refer to as pp,
            ∗
            = {(i, j) ∈
            for an optimal solution x∗ to RP∗ (G): Let P>0
            ∗
            P : xij = 1, cij > 0}. Obtain a set of weakly connected
            ∗
            components {V1 , V2 , . . . , Vk } of (V, P>0
            ) by the depth-ﬁrst
            ∗
            search. Output a 0-1 vector x that corresponds to the partition {V1 , V2 , . . . , Vk }, i.e., x∗ such that
            x∗ij = 1 if and only
            if i, j ∈ Vq for some q ∈ {1, 2, . . . , k}. Note that pp runs in
            time linear in the size of G.

            Here we present our ILP formulation. Without loss of generality, we assume that the edge weight function c
            is integervalued. When c is rational-valued, we can immediately obtain
            an equivalent instance in which c is integer-valued by multiplying an appropriate value for each cij .
            Let E0 = {{i, j} ∈ E : cij = 0}. We deﬁne an edge
            weight function c so that for each {i, j} ∈ E,
            −
            cij

            1
            xij + xjk − xik ≤ 1 ∀(i, j, k) ∈ T>0
            ,
            2
            xij − xjk + xik ≤ 1 ∀(i, j, k) ∈ T>0
            ,

            Our Formulation

            cij =

            ILP-Based Exact Algorithm

            if {i, j} ∈ E0 ,
            otherwise,

             
            where  ∈ (0, 1/ n2 ). Let us introduce a new instance G =
            (V, E, c). Then the number of triangle inequality constraints
            in RP(G) is O(nm>0 ), where m>0 is the number of positiveweighted edges in G, i.e., m>0 = |{(i, j) ∈ P : cij
            > 0}|.
            It is expected that as  > 0 is sufﬁciently small, an optimal
            solution to RP(G) is also optimal to RP(G) and thus to P(G).
            In fact, we have the following theorem:
            Theorem 2. Let G = (V, E, c) be an arbitrary instance of
            CPP such that c is integer-valued. Any optimal solution to
            RP(G) is also optimal to P(G).
            Proof. Let x = (xij )(i,j)∈P be an arbitrary optimal solution
            to RP(G). From Theorem 1, x is an optimal solution to
            P(G), which implies that x satisﬁes all the triangle inequality
            constraints for T . Thus, x is also feasible for P(G).

            4.1

            Correctness

            Here we demonstrate that our algorithm (i.e., RP∗ (G)+pp)
            returns an optimal solution to P(G). To this end, it sufﬁces

            1390

            to show that the objective value of x∗ remains the same as
            that of x∗ (in P(G) and RP∗ (G)) because x∗ is feasible for
            P(G) and x∗ is optimal to a relaxation RP∗ (G) of P(G). For
            convenience, we deﬁne Pin∗ = {(i, j) ∈ P : x∗ij = 1} and
            ∗
            Pout
            = P \ Pin∗ . We have the following lemmas:
            
            
            Lemma 1. It holds that
            cij x∗ij =
            cij x∗ij .
            (i,j)∈Pin∗

            otherwise. Then MaxSAT-Transitive, which we refer to as
            S-P(G) in the present study, can be represented as follows:
            Hard clauses:
            (¬xij ∨ ¬xjk ∨ xik )
            ∀(i, j, k) ∈ T,
            (¬xij ∨ xjk ∨ ¬xik )
            ∀(i, j, k) ∈ T,
            (xij ∨ ¬xjk ∨ ¬xik )
            ∀(i, j, k) ∈ T,
            Soft clauses:
            (xij ) with weight cij ∀(i, j) ∈ P with {i, j} ∈ E+ ,
            (¬xij ) with weight cij ∀(i, j) ∈ P with {i, j} ∈ E− ,

            (i,j)∈Pin∗

            Proof. It sufﬁces to show that for any q ∈ {1, 2, . . . , k},
            it holds that x∗ij = 1 for each i, j ∈ Vq with i
            < j. Fix
              q
            ∈ {1, 2, . . . , k}. Let i, j be a pair of distinct vertices of Vq .
            ∗
            Since Vq is weakly connected by P>0
            , there exists a path on
            ∗
            P>0
            that connects i and j if we ignore the direction of edges.
            Denote this (undirected) path by i = v0 , v1 , . . . , vt = j.
            Since cv0 v1 > 0 (and cv1 v2 > 0), RP∗ (G) has the constraint
            xv0 v1 + xv1 v2 − xv0 v2 ≤ 1. Note here that in this notation,
            it is necessary that v0
            < v1
            < v2 holds. If it is not the
              case, we should swap the order of the indices appropriately.
            Substituting x∗v0 v1 = x∗v1 v2 = 1 to this constraint, we have
            x∗v0 v2 = 1. Moreover, since cv2 v3 > 0, RP∗ (G) also has the
            constraint xv0 v2 + xv2 v3 − xv0 v3 ≤ 1. Substituting x∗v0 v2 =
            x∗v2 v3 = 1 to this constraint, we have x∗v0 v3 = 1. Repeating
            this operation, we ﬁnally have x∗v0 vt = x∗ij = 1.
            
            Lemma 2. It holds that
            cij x∗ij ≤ 0.

            where T = {(i, j, k) : 1 ≤ i
            < j
            < k
            ≤ n}. The set of
            hard clauses is a clausal representation of the set of triangle
            inequality constraints in P(G), and the set of soft clauses is a
            clausal representation of the objective function in P(G).
            By beginning with S-P(G), we can reproduce our results
            for ILP in the context of MaxSAT; speciﬁcally, we can obtain S-RP(G), S-RP(G), and S-RP∗ (G)+S-pp, which are
            the
            MaxSAT counterparts of RP(G), RP(G), and RP∗ (G)+pp,
            respectively. The detailed description of them is omitted owing to space limitations. We have the following
            theorems:
            Theorem 4 (MaxSAT counterpart of Theorem 2). Let G =
            (V, E+ , E− , c) be an arbitrary instance of WCC such that c
            is integer-valued. Any optimal solution to S-RP(G) is also
            optimal to S-P(G).

            ∗
            (i,j)∈Pout

            ∗
            ∗
            Proof. For each (i, j) ∈ Pout
            , we have (i, j) ∈
            / P>0
            . If other∗
            ∗
            wise, then xij = 1 and thus (i, j) ∈ Pin . Therefore, for each
            ∗
            (i, j) ∈ Pout
            , we have x∗ij = 0 or cij ≤ 0, which proves the
            lemma.
            
            ∗
            By Lemmas 1 and 2, we have
            (i,j)∈P cij xij =
            
            
            
            ∗
            ∗
            ∗
            ≥ (i,j)∈P ∗ cij xij + (i,j)∈Pout
            ∗ c x
            ∗ cij xij =
            in
            (i,j)∈Pin ij ∗ij
            (i,j)∈P cij xij . Therefore, we have the following theorem:

            Theorem 5 (MaxSAT counterpart of Theorem 3). Let G =
            (V, E+ , E− , c) be an arbitrary instance of WCC such that c
            is integer-valued. Any True-False assignment returned by
            our algorithm (i.e., S-RP∗ (G)+S-pp) is optimal to S-P(G).

            6

            The purpose of our experiments is to evaluate the effectiveness of our ILP-based approaches and their MaxSAT
            counterparts in terms of both memory efﬁciency and computation
            time. To this end, we use well-known real-world datasets arising in the context of correlation clustering,
            group technology,
            and community detection.

            Theorem 3. Let G = (V, E, c) be an arbitrary instance of
            CPP such that c is integer-valued. Any 0-1 vector returned
            by our algorithm (i.e., RP∗ (G)+pp) is optimal to P(G).

            5

            Experimental Evaluation

            MaxSAT Counterparts

            6.1

            Here we present MaxSAT counterparts of both our ILP
            formulation (i.e., RP(G)) and ILP-based exact algorithm
            (i.e., RP∗ (G)+pp). It should be noted that we here consider WCC rather than CPP. As described above, WCC is
            equivalent to CPP from an exact optimization perspective.
            Let G = (V, E+ , E− , c) be an instance of WCC. Note that
            c : E+ ∪ E− → Q>0 is an edge weight function, where Q>0
            is the set of positive rational numbers. The (positive) edge
            weights represent the strength of similarity and dissimilarity
            for {i, j} ∈ E+ and {i, j} ∈ E− , respectively. For simplicity, we denote cij = c({i, j}) for each {i, j} ∈
            E+ ∪ E− .
            We revisit MaxSAT-Transitive introduced by Berg and
            Järvisalo (2017), which is the MaxSAT counterpart of P(G).
            Let V = {1, 2, . . . , n} and P = {(i, j) : 1 ≤ i
            < j
            ≤ n}.
            For each (i, j) ∈ P , we introduce a Boolean variable xij
            equal to True if i, j ∈ V are in the same cluster and False

            ILP-Based Approaches

            We ﬁrst compare our ILP formulation (i.e., RP(G)) and ILPbased exact algorithm (i.e., RP∗ (G)+pp) with the
            previous
            ILP formulations (i.e., P(G) and RP(G)) and the state-ofthe-art MaxSAT formulation (i.e., MaxSAT-Binary). As
            for
            RP(G), we set the parameter  as follows:
             = n12 and
             n
            2
            n2 , both of which are less than 1/ 2 . All ILP formulations were solved using Gurobi Optimizer 7.0.2 with
            default
            parameters. As for MaxSAT-Binary, we employed the preprocessing and symmetry-breaking operations mentioned
            in
            Berg and Järvisalo (2017). The preprocessing was simulated
            using Coprocessor 3.0 and the symmetry-breaking procedure called R EMOVE S LACK was applied. MaxSAT-Binary
            was solved using MaxHS 2.9, as recommended in Berg and
            Järvisalo (2017).

            1391

            Table 1: Instances used in our experiments.
            ID

            Name

            n

             
            m>0 / n2

             
            m≥0 / n2

            P1
            P2
            P3
            P4

            Protein1
            Protein2
            Protein3
            Protein4

            669
            587
            567
            654

            4.22%
            6.36%
            6.39%
            2.64%

            98.49%
            98.21%
            97.95%
            98.87%

            G9
            G14
            G16
            G17
            G18
            G19
            G21
            G25
            G26
            G27
            G28
            G29
            G30
            G31
            G32
            G33
            G34
            G35

            Ch-8x20b
            Mc-16x24
            KI-16x43
            Ca-18x24
            MT-20x20
            Ku-20x23
            Bo-20x35
            CH5-24x40
            CH6-24x40
            CH7-24x40
            Mc-27x27
            Ca-28x46
            Ku-30x41
            St-30x50-1
            St-30x50-2
            Ki-36x90
            MC-37x53
            Ch-40x100

            28
            40
            59
            42
            40
            43
            55
            64
            64
            64
            54
            74
            71
            80
            80
            126
            90
            140

            24.07%
            11.47%
            7.36%
            10.22%
            14.23%
            12.51%
            10.03%
            6.50%
            6.50%
            6.50%
            15.30%
            7.81%
            5.15%
            4.87%
            5.28%
            4.23%
            24.39%
            4.32%

            81.75%
            61.81%
            67.15%
            60.05%
            62.95%
            61.57%
            62.90%
            58.88%
            58.88%
            58.88%
            64.36%
            60.13%
            55.65%
            57.41%
            57.82%
            66.41%
            75.43%
            63.21%

            C1
            C2
            C3
            C4
            C5
            C6

            Wafa-CEO
            Divorces
            Hollywood movies
            Scotland interlocks
            Graph product
            Network science

            41
            59
            102
            244
            674
            2,549

            11.10%
            11.63%
            3.73%
            1.21%
            0.27%
            0.08%

            63.54%
            85.33%
            56.79%
            59.00%
            50.43%
            53.11%

            of memory capacity. Although RP(G) had fewer constraints
            than P(G), the decrement was quite small, with at most 0.1%
            of the constraints removed. By contrast, our formulations,
            RP(G) and RP∗ (G), had far fewer constraints, with about
            90% of the constraints removed. Correspondingly, the memory limit was not exceeded, and optimal solutions
            were obtained for instances P1 and P2. The results also show that
            our formulations outperformed MaxSAT-Binary. In fact, only
            RP(G) and RP∗ (G)+pp could obtain an optimal solution to
            instance P2 and nearly-optimal solutions for instances P3 and
            P4, although MaxSAT-Binary solved instance P1 faster.
            Group technology. An assessment of group technology
            instances was then conducted; some of these are listed
            in the middle section of Table 1. The instances were
            generated from manufacturing cell formation datasets on
            http://mauricio.resende.info/data in the manner
            described in Example 2; a detailed description of the datasets
            can be found in Gonçalves and Resende (2004). Although
            there were originally 35 instances, which were indexed from
            G1 to G35, some instances are omitted owing to space limitations. Our preliminary experiments showed that
            all formulations could solve the omitted instances within 10.0 s.
            The results are summarized in Table 2. Because the instances are smaller than those used in correlation
            clustering,
            P(G) and RP(G) could always be stored on the machine.
            Although RP(G) had fewer constraints than P(G), the computation time (or the relative gap) increased in 7
            out of 17
            instances. Again, our formulations, RP(G) and RP∗ (G), produced far fewer constraints. In fact, even for
            relatively small
            instances, at least 50% of the constraints were removed, with
            the ﬁgure increasing to above 90% for some large instances.
            Furthermore, the computation time was reduced signiﬁcantly.
            In particular, RP∗ (G)+pp could obtain optimal solutions for
            all instances within the time limit. The results also show that
            MaxSAT-Binary performed no better than P(G) and RP(G).
            In fact, for any instance that could not be solved by P(G)
            and RP(G), MaxSAT-Binary also failed to obtain an optimal
            solution, and moreover, only MaxSAT-Binary exceeded the
            memory limit for some instances owing to its signiﬁcant
            memory requirements in the search phase.

            The time limit was set to 4 hours and the memory limit was
            set to 96 GB. The experiments were conducted on a Linux
            machine with Intel Xeon Processor E5-2650 v2 2.60 GHz
            CPU and 96 GB RAM.
            Correlation clustering. Correlation clustering instances
            were ﬁrst tested. Speciﬁcally, we considered M AX AGREE in
            the edge-weighted setting (i.e., WCC). The upper section of
            Table 1 lists the instances on which experiments were conducted, which were generated from protein sequence
            datasets
            on http://www.paccanarolab.org/scps in the same
            manner as in Berg and Järvisalo (2017). The data consist of
            similarity values between amino-acid sequences that were
            computed using BLAST (Altschul et al. 1990).
            The results are detailed
             in Table 2. The number of variables is always equal to n2 = n(n − 1)/2. The number of
            constraints in RP∗ (G) is omitted because it is exactly the
            same as in RP(G). OM in some columns indicates that the
            memory requirements of the formulation (and the solving
            phase) exceed the limit (i.e., 96 GB). For the formulations
            that could be stored on the machine but could not be solved
            within the time limit, the relative gaps (i.e., (UB − LB)/LB,
            where UB and LB, respectively, are the upper and lower
            bounds on the optimal value) obtained by the time limit are
            presented within parentheses if they are ﬁnite; otherwise OT
            is given. For each instance, the best computation time (or the
            relative gap) among the formulations is written in bold.
            It is seen that neither P(G) nor RP(G) could be stored on
            the machine for instances P1 and P4 owing to a shortage

            Community detection. Community detection instances
            were then tested, with particular consideration given to the
            bipartite modularity maximization problem. The last section
            of Table 1 lists the instances on which experiments were
            conducted, which were generated from network datasets on
            http://vlado.fmf.uni-lj.si/pub/networks/data

            in the manner described in Example 3.
            The results are detailed in Table 2. It is seen that the trend
            of the results is similar to that produced in the correlation
            clustering and group technology assessments, i.e., RP(G)
            and RP∗ (G)+pp outperformed P(G), RP(G), and MaxSATBinary. Most strikingly, our approaches could obtain an
            optimal solution to instance C6 with 2,549 vertices, while P(G)
            and RP(G) required more than 8 billion and 6 billion constraints, respectively. Furthermore, MaxSAT-Binary
            left a very
            large relative gap (i.e., 6,350.3%).

            1392

            Table 2: Results for the previous ILP formulations, our ILP-based approaches, and the state-of-the-art
            MaxSAT formulation.
            ID
            P1
            P2
            P3
            P4

            149,038,482
            100,614,735
            90,660,465
            139,222,212

            G14
            G16
            G17
            G18
            G19
            G21
            G25
            G26
            G27
            G28
            G29
            G30
            G31
            G32
            G33
            G34
            G35
            C1
            C2
            C3
            C4
            C5
            C6

            6.2

            RP(G)
            #constr.

             = n12
            time(s)

             = n22
            time(s)

            RP∗ (G)+pp
            time(s)

            #var.

            #clauses

            time(s)

            OM
            OT
            OT
            OM

            12,106,852
            12,191,742
            10,995,760
            7,161,805

            4,010.3
            2,774.3
            (0.3%)
            (0.2%)

            3,505.6
            3,430.9
            (0.3%)
            (0.2%)

            3,796.2
            3,039.9
            (0.3%)
            (0.2%)

            167,184
            179,968
            173,928
            109,692

            684,568
            744,956
            719,876
            434,570

            2,304.8
            (48.1%)
            (18.1%)
            (119.2%)

            23,366
            17.0
            84,297
            551.1
            28,920
            14.0
            25,661
            803.9
            31,606
            52.4
            67,196
            149.2
            102,860
            245.2
            102,875
            418.3
            102,887
            1,145.3
            65,114
            1,419.3
            161,752
            (8.5%)
            137,420
            30.7
            199,476
            85.0
            200,359
            4,556.4
            716,019 (132.1%)
            326,247 (82.9%)
            1,121,684
            13.4

            5,902
            13,560
            6,640
            7,839
            8,646
            14,938
            15,750
            15,765
            15,777
            21,044
            28,840
            17,222
            23,508
            25,405
            68,435
            146,067
            112,904

            4.2
            14.2
            5.2
            433.6
            18.9
            21.0
            16.6
            62.2
            227.2
            1,322.7
            1,596.3
            4.7
            10.7
            127.0
            (6.0%)
            5,968.0
            1.3

            4.6
            13.9
            4.8
            343.6
            18.6
            21.2
            12.9
            61.1
            258.1
            1,193.3
            1,715.2
            6.2
            12.0
            107.8
            (6.7%)
            7344.6
            1.3

            4.2
            12.4
            3.2
            273.9
            14.4
            19.1
            14.7
            25.5
            168.9
            570.1
            1,296.5
            4.3
            10.6
            47.0
            11,963.4
            4,131.6
            1.3

            3,412
            6,212
            3,960
            3,680
            4,196
            6,260
            9,536
            9,536
            9,536
            6,480
            12,628
            12,064
            14,620
            14,620
            25,980
            18,909
            42,240

            12,120
            22,440
            14,122
            12,962
            15,128
            22,856
            31,718
            31,718
            31,718
            23,758
            48,267
            46,280
            55,683
            55,683
            100,201
            73,342
            168,918

            12.3
            64.3
            12.5
            OM
            219.5
            182.2
            110.4
            (1.4%)
            (160.6%)
            OM
            OM
            12.3
            14.9
            (137.2%)
            OM
            (149.5%)
            17.8

            9.4
            10.4
            75.6
            74.5
            8,711.4 10,821.9
            (1.3%)
            (1.9%)
            15.9
            16.2
            397.2
            407.3

            7.5
            64.8
            (0.4%)
            (1.5%)
            18.2
            402.4

            P(G)
            #constr.
            time(s)
            OM
            OM
            OT
            OM

            27,417
            20.8
            97,527
            233.3
            34,440
            18.4
            29,640
            804.4
            37,023
            66.6
            78,705
            126.8
            124,992
            474.5
            124,992
            426.8
            124,992
            1,338.4
            74,412
            2,426.5
            194,472
            (6.4%)
            171,465
            29.8
            246,480
            78.4
            246,480
            2,634.1
            842,520 (151.3%)
            352,440 (13.2%)
            1,342,740
            16.3
            31,980
            97,527
            515,100
            7,174,332
            152,410,272
            8.3G

            54.8
            4,927.8
            (11.6%)
            (8.4%)
            OM
            OM

            RP(G)
            #constr.
            time(s)
            148,995,305
            100,571,045
            90,611,160
            139,199,019

            27,466
            93,475
            413,487
            5,840,182
            114,839,168
            6.3G

            41.4
            4,749.9
            (11.8%)
            (8.4%)
            OM
            OM

            6,640
            20,116
            37,470
            172,070
            822,272
            13,130,379

            3,612
            12,952
            14.1
            4,308
            15,232
            138.9
            23,190
            90,651
            OM
            129,024
            527,718
            (141.9%)
            1,369,960 5,888,398
            4,994.9
            21,417,336 94,669,067 (6,350.3%)

            MaxSAT Counterparts
            Table 3: Results for the MaxSAT counterparts.

            Next we compare our MaxSAT formulation (i.e., S-RP(G))
            and MaxSAT-based exact algorithm (i.e., S-RP∗ (G)+S-pp)
            with the MaxSAT counterparts of P(G) and RP(G) (i.e.,
            S-P(G) and S-RP(G)). As for S-RP(G), we set the parameter
             as in its ILP counterpart, i.e.,  = n12 and n22 . All MaxSAT
            formulations were solved using MaxHS 2.9.
            The results are detailed in Table 3 with the same notations
            as in Table 2. As for S-RP(G), the left and right columns
            correspond to the results of  = n12 and  = n22 , respectively. Note that the number of hard clauses in
            each MaxSAT
            formulation is equal to the number of constraints in its ILP
            counterpart (see Table 2). It is seen that our results are still
            effective for MaxSAT. In fact, S-RP(G) could solve 6 out of
            11 instances that could be solved neither by S-P(G) nor by
            S-RP(G), and moreover, S-RP∗ (G) could solve instances P1,
            P2, and P4 much faster than S-P(G) and S-RP(G).

            7

            MaxSAT-Binary
        </corps>
        <conclusion>
            In this study, we have investigated the design of mathematical
            programming formulations and constraint satisfaction formulations for CPP. More speciﬁcally, we have
            presented a novel
            ILP formulation, an ILP-based exact algorithm, and their
            MaxSAT counterparts. The experimental results demonstrate
            that our approaches signiﬁcantly outperform the state-of-theart formulations in terms of both memory
            efﬁciency and
            computation time.

            1393

            S-RP(G)
            S-RP∗ (G)+S-pp
            time(s) time(s)
            time(s)

            ID

            S-P(G)
            time(s)

            S-RP(G)
            time(s)

            P1
            P2
            P3
            P4

            4,362.6
            2,690.6
            (11.0%)
            6,998.2

            5,284.4
            (0.5%) (0.7%)
            2,779.7 2,049.8 2,366.6
            (3.1%) (253.1%) (8.2%)
            7,773.0 (31.8%) (31.8%)

            1,058.4
            486.0
            (3.9%)
            3,200.5

            G14
            G16
            G17
            G18
            G19
            G21
            G25
            G26
            G27
            G28
            G29
            G30
            G31
            G32
            G33
            G34
            G35

            14.9
            19.6
            6.1
            (140.0%)
            195.7
            124.3
            31.9
            2,017.7
            (121.1%)
            OM
            OM
            7.1
            8.7
            (118.0%)
            OM
            (164.4%)
            14.4

            14.5
            20.1
            6.2
            (140.0%)
            198.7
            126.6
            43.4
            1,986.3
            (121.1%)
            OM
            OM
            6.1
            9.3
            (118.0%)
            OM
            (164.4%)
            11.4

            16.6
            18.0
            41.4
            49.0
            11.4
            13.1
            3,782.8 3,868.5
            45.4
            50.8
            49.9
            60.2
            46.2
            44.4
            348.5
            484.6
            1,615.8 2,338.5
            3,995.1 6,949.2
            8,658.7 11085.3
            15.8
            16.8
            26.9
            28.2
            772.0
            841.4
            OT
            OT
            OT 13,421.1
            2.8
            2.4

            21.8
            39.2
            10.5
            (140.0%)
            637.7
            132.4
            32.3
            4197.1
            (123.7%)
            OM
            OM
            14.7
            8.9
            11,172.4
            OM
            (154.9%)
            3.1

            C1
            13.8
            13.4
            C2
            83.5
            87.6
            C3 (1,340.7%) (1,340.7%)
            C4
            (65.8%)
            (65.8%)
            C5
            4,162.6
            2,794.7
            C6
            OM
            OM

            14.5
            18.0
            119.3
            82.9
            OT
            OT
            OT
            OT
            525.7
            555.7
            (18.3%) (25.7%)

            9.7
            106.2
            (802.1%)
            (75.3%)
            268.0
            (8.5%)

        </conclusion>
        <discussion>
            Aucune discussion trouvée.
        </discussion>
        <biblio>
            Grötschel, M., and Wakabayashi, Y. 1989. A cutting plane
            algorithm for a clustering problem. Mathematical Programming 45(1–3):59–96.
            Jaehn, F., and Pesch, E. 2013. New bounds and constraint
            propagation techniques for the clique partitioning problem.
            Discrete Applied Mathematics 161(13–14):2025–2037.
            Jain, A. K.; Murty, M. N.; and Flynn, P. J. 1999. Data
            clustering: A review. ACM Computing Surveys 31(3):264–
            323.
            Kim, S.; Yoo, C. D.; Nowozin, S.; and Kohli, P. 2014. Image
            segmentation using higher-order correlation clustering. IEEE
            Transactions on Pattern Analysis and Machine Intelligence
            36(9):1761–1774.
            Miyauchi, A., and Miyamoto, Y. 2013. Computing an upper bound of modularity. European Physical Journal B
            86(7):302.
            Miyauchi, A., and Sukegawa, N. 2015a. Maximizing Barber’s bipartite modularity is also hard. Optimization
            Letters
            9(5):897–913.
            Miyauchi, A., and Sukegawa, N. 2015b. Redundant constraints in the standard formulation for the clique
            partitioning
            problem. Optimization Letters 9(1):199–207.
            Newman, M. E. J., and Girvan, M. 2004. Finding and
            evaluating community structure in networks. Physical Review
            E 69:026113.
            Nowozin, S., and Jegelka, S. 2009. Solution stability in
            linear programming relaxations: Graph partitioning and unsupervised learning. In ICML ’09: Proceedings of
            the 26th
            International Conference on Machine Learning, 769–776.
            Oosten, M.; Rutten, J. H. G. C.; and Spieksma, F. C. R. 2001.
            The clique partitioning problem: Facets and patching facets.
            Networks 38(4):209–226.
            Puleo, G. J., and Milenkovic, O. 2016. Correlation clustering
            and biclustering with locally bounded errors. In ICML ’16:
            Proceedings of the 33rd International Conference on Machine Learning.
            Van Gael, J., and Zhu, X. 2007. Correlation clustering for
            crosslingual link detection. In IJCAI ’07: Proceedings of the
            20th International Joint Conference on Artiﬁcial Intelligence,
            1744–1749.
            Wakabayashi, Y. 1986. Aggregation of Binary Relations: Algorithmic and Polyhedral Investigations. Ph.D.
            Dissertation,
            Universität Augsburg.
            Xu, R., and Wunsch, D. 2005. Survey of clustering algorithms. IEEE Transactions on Neural Networks
            16(3):645–
            678.

            The authors would like to thank the anonymous reviewers for
            their valuable suggestions and helpful comments. The ﬁrst
            author is supported by a Grant-in-Aid for Research Activity
            Start-up (No. 17H07357). This work was supported by JST
            ERATO Grant Number JPMJER1201, Japan.

            References
            Agarwal, G., and Kempe, D. 2008. Modularity-maximizing
            graph communities via mathematical programming. European Physical Journal B 66(3):409–418.
            Aggarwal, C. C., and Reddy, C. K. 2013. Data Clustering:
            Algorithms and Applications. CRC Press.
            Ahn, K. J.; Cormode, G.; Guha, S.; McGregor, A.; and Wirth,
            A. 2015. Correlation clustering in data streams. In ICML ’15:
            Proceedings of the 32nd International Conference on Machine Learning, 2237–2246.
            Altschul, S. F.; Gish, W.; Miller, W.; Myers, E. W.; and
            Lipman, D. J. 1990. Basic local alignment search tool.
            Journal of Molecular Biology 215(3):403–410.
            Awasthi, P.; Balcan, M.-F.; and Voevodski, K. 2014. Local algorithms for interactive clustering. In ICML
            ’14: Proceedings
            of the 31st International Conference on Machine Learning,
            550–558.
            Bansal, N.; Blum, A.; and Chawla, S. 2004. Correlation
            clustering. Machine Learning 56(1–3):89–113.
            Barber, M. J. 2007. Modularity and community detection in
            bipartite networks. Physical Review E 76:066102.
            Berg, J., and Järvisalo, M. 2017. Cost-optimal constrained
            correlation clustering via weighted partial maximum satisﬁability. Artiﬁcial Intelligence 244:110–142.
            Bonchi, F.; Gionis, A.; and Ukkonen, A. 2013. Overlapping
            correlation clustering. Knowledge and Information Systems
            35(1):1–32.
            Bruckner, S.; Hüffner, F.; Komusiewicz, C.; and Niedermeier,
            R. 2013. Evaluation of ILP-based approaches for partitioning
            into colorful components. In SEA ’13: Proceedings of the
            12th International Symposium on Experimental Algorithms,
            176–187.
            Chierichetti, F.; Dalvi, N.; and Kumar, R. 2014. Correlation
            clustering in MapReduce. In KDD ’14: Proceedings of the
            20th ACM SIGKDD International Conference on Knowledge
            Discovery and Data Mining, 641–650.
            Dinh, T. N., and Thai, M. T. 2015. Toward optimal community detection: From trees to general weighted
            networks.
            Internet Mathematics 11(3):181–200.
            Fortunato, S. 2010. Community detection in graphs. Physics
            Reports 486(3):75–174.
            Gonçalves, J. F., and Resende, M. G. C. 2004. An evolutionary algorithm for manufacturing cell formation.
            Computers
            & Industrial Engineering 47(2–3):247–273.
            Groover, M. P. 2007. Automation, Production Systems, and
            Computer-Integrated Manufacturing. Prentice Hall Press.
            1394
        </biblio>
    </article>
    <article>
        <preamble>LDA_resume.pdf</preamble>
        <titre>Automatic Summarization Approaches to Speed up Topic Model Learning Process</titre>
        <auteur>
            Mohamed Morchid, Juan-Manuel Torres-Moreno, Richard Dufour,
            Javier Ramirez-Rodriguez, Georges Linarès
            Affiliations :
            1. LIA - Université d’Avignon, France
            2. SFR Agorantic, Université d’Avignon, France
            3. École Polytechnique de Montréal, Canada
            4. Universidad Autónoma Metropolitana–Azcapotzalco, Mexico
        </auteur>
        <abstract>
            The number of documents available into Internet moves each
            day up. For this reason, processing this amount of information effectively
            and expressibly becomes a major concern for companies and scientists.
            Methods that represent a textual document by a topic representation
            are widely used in Information Retrieval (IR) to process big data such
            as Wikipedia articles. One of the main difficulty in using topic model on
            huge data collection is related to the material resources (CPU time and
            memory) required for model estimate. To deal with this issue, we propose to build topic spaces from
            summarized documents. In this paper, we
            present a study of topic space representation in the context of big data.
            The topic space representation behavior is analyzed on different languages. Experiments show that topic
            spaces estimated from summaries
            are as relevant as those estimated from the complete documents. The real
            advantage of such an approach is the processing time gain: we showed
            that the processing time can be drastically reduced using summarized
            documents (more than 60% in general). This study finally points out the
            differences between thematic representations of documents depending on
            the targeted languages such as English or latin languages.
        </abstract>
        <introduction>
            he number of documents available into Internet moves each day up in an exponential way. For this reason,
            processing this amount of information effectively
            and expressibly becomes a major concern for companies and scientists. An important part of the information
            is conveyed through textual documents such as
            blogs or micro-blogs, general or advertise websites, and encyclopedic documents.
            This last type of textual data increases each day with new articles, which convey large and heterogenous
            information. The most famous and used collaborative
            Internet encyclopedia is Wikipedia, enriched by worldwide volunteers. It is the
            12th most visited website in the USA, with around 10.65 million users visiting

            the site daily, and a total reaching 39 millions of the estimated 173.3 million
            Internet users in the USA1 2 .
            The massive number of documents provided by Wikipedia is mainly exploited
            by Natural Language Processing (NLP) scientists in various tasks such as keyword extraction, document
            clustering, automatic text summarization, etc. Different classical representations of a document, such as
            term-frequency based
            representation [1], have been proposed to extract word-level information from
            this large amount of data in a limited time. Nonetheless, these straightforward
            representations obtain poor results in many NLP tasks with respect to more
            abstract and complex representations. Indeed, the classical term-frequency representation reveals little in
            way of intra- or inter-document statistical structure,
            and does not allow us to capture possible and unpredictable context dependencies. For these reasons, more
            abstract representations based on latent topics have
            been proposed. The most known and used one is the latent Dirichlet allocation
            (LDA) [2] approach which outperforms classical methods in many NLP tasks.
            The main drawback of this topic-based representation is the time needed to learn
            LDA latent variables. This massive waste of time that occurs during the LDA
            learning process, is mainly due to the documents size along with the number of
            documents, which is highly visible in the context of big data such as Wikipedia.
            The solution proposed in this article is to summarize text documents contained into a big data corpus (here
            Wikipedia) and then, learn a LDA topic
            space. This should answer the these three raised difficulties:
            • reducing the processing time during the LDA learning process,
            • retaining the intelligibility of documents,
            • maintaining the quality of LDA models.
            With this text summarization approach, the size of documents will be drastically reduced, the
            intelligibility of documents will be preserved, and we make
            the assumption that the LDA model quality will be conserved. Moreover, for
            all these reasons, the classical term-frequency document reduction is not considered in this paper. Indeed,
            this extraction of a subset of words to represent the
            document content allows us to reduce the document size, but does not keep the
            document structure and then, the intelligibility of each document.
            The main objective of the paper is to compare topic space representations
            using complete documents and summarized ones. The idea behind is to show the
            effectiveness of this document representation, in terms of performance and timeprocessing reduction, when
            summarized documents are used. The topic space
            representation behavior is analyzed on different languages (English, French and
            Spanish). In the series of proposed experiments, the topic models built from
            complete and summarized documents are evaluated using the Jensen-Shannon
            (J S) divergence measure as well as the perplexity measure. To the best of our
            knowledge, this is the most extensive set of experiments interpreting the evaluation of topic spaces built
            from complete and summarized documents without
            human models.
            1
            2

            http://www.alexa.com
            http://www.metrics2.com

            The rest of the paper is organized in the following way: first, Section 2 introduces related work in the
            areas of topic modeling and automatic text summarization evaluations. Then, Section 3 describes the proposed
            approach, including
            the topic representation adopted in our work and the different summarization
            systems employed. Section 4 presents the topic space quality measures used for
            the evaluation. Experiments carried out along with with the results presented
            in Section 5. A discussion is finally proposed in Section 6 before concluding
            in Section 7.
        </introduction>
        <corps>
            Several methods were proposed by Information Retrieval (IR) researchers to
            process large corpus of documents such as Wikipedia encyclopedia. All these
            methods consider documents as a bag-of-words [1] where the word order is not
            taken into account.
            Among the first methods proposed in IR, [3] propose to reduce each document from a discrete space (words and
            documents) to a vector of numeral values
            represented by the word counts (number of occurrences) in the document named
            TF-IDF [4]. This approach showed its effectiveness in different tasks, and more
            precisely in the basic identification of discriminative words for a document [5].
            However, this method has many weaknesses such as the small amount of reduction in description length, or the
            weak of inter- or intra-statistical structure of
            documents in the text corpus.
            To substantiate the claims regarding TF-IDF method, IR researchers have
            proposed several other dimensionality reductions such as Latent Semantic Analysis (LSA) [6, 7] which uses a
            singular value decomposition (SVD) to reduce the
            space dimension.
            This method was improved by [8] which proposed a Probabilistic LSA (PLSA).
            PLSA models each word in a document as a sample from a mixture model, where
            the mixture components are multinomial random variables that can be viewed as
            representations of topics. This method demonstrated its performance on various
            tasks, such as sentence [9] or keyword [10] extraction. In spite of the effectiveness
            of the PLSA approach, this method has two main drawbacks. The distribution
            of topics in PLSA is indexed by training documents. Thus, the number of its
            parameters grows with the training document set size and then, the model is
            prone to overfitting which is a main issue in an IR task such as documents clustering. However, to address
            this shortcoming, a tempering heuristic is used to
            smooth the parameter of PLSA models for acceptable predictive performance:
            the authors in [11] showed that overfitting can occur even if tempering process
            is used.
            To overcome these two issues, the latent Dirichlet allocation (LDA) [2] method
            was proposed. Thus, the number of LDA parameters does not grow with the size
            of the training corpus and LDA is not candidate for overfitting. Next section describes more precisely the
            LDA approach that will be used in our experimental
            study.

            [12] evaluated the effectiveness of the Jensen-Shannon (J S) theoretic measure [13] in predicting systems
            ranks in two text summarization tasks: queryfocused and update summarization. They have shown that ranks
            produced by
            Pyramids and those produced by J S measure correlate. However, they did not
            investigate the effect of the measure in other complex summarization tasks. [14,
            15] using the FRESA system, reported evaluation results without human references on several tasks: generic
            and multi-document summarization (DUC 2004
            Task 2), biographical summarization (DUC 2004 Task 5), opinion summarization
            (TAC 2008 OS) and summarization in languages other than English.
            Next section describes the proposed approach followed in this article, including the topic space
            representation with the LDA approach and its evaluation
            with the perplexity and the FRESA divergence metrics.

            3

            Overview of the proposed approach

            Figure 1 describes the approach proposed in this paper to evaluate the quality of a topic model
            representation with and without automatic summarization
            systems. The latent Dirichlet allocation (LDA) approach, described in details in
            the next section, is used for topic representation, in conjunction with different
            state-of-the-art summarization systems presented in Section 3.2.
            3.1

            Topic representation: latent Dirichlet allocation

            LDA is a generative model which considers a document, seen as a bag-of-words [1],
            as a mixture of latent topics. In opposition to a multinomial mixture model, LDA
            considers that a theme is associated to each occurrence of a word composing the
            document, rather than associate a topic with the complete document. Thereby,
            a document can change of topics from a word to another. However, the word
            occurrences are connected by a latent variable which controls the global respect
            of the distribution of the topics in the document. These latent topics are characterized by a distribution
            of word probabilities associated with them. PLSA and
            LDA models have been shown to generally outperform LSA on IR tasks [16].
            Moreover, LDA provides a direct estimate of the relevance of a topic knowing a
            word set.
            Figure 2 shows the LDA formalism. For every document d of a corpus D, a
            first parameter θ is drawn according to a Dirichlet law of parameter α. A second
            parameter φ is drawn according to the same Dirichlet law of parameter β. Then,
            to generate every word w of the document c, a latent topic z is drawn from
            a multinomial distribution on θ. Knowing this topic z, the distribution of the
            words is a multinomial of parameters φ. The parameter θ is drawn for all the
            documents from the same prior parameter α. This allows to obtain a parameter
            binding all the documents together [2].
            Several techniques have been proposed to estimate LDA parameters, such as
            Variational Methods [2], Expectation-propagation [17] or Gibbs Sampling [18].
            Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) [19] and

            Wikipedia
            English, Spanish or French

            TEST

            TRAIN

            Summary
            System

            Full text

            Artex,
            Baseline First,
            Baseline Random

            Latent Dirichlet Allocation

            Topic spaces
            from documents
            not summarized

            Topic spaces
            from documents
            summarized

            Perplexity

            KL

            Fig. 1. Overview of the proposed approach.

            gives a simple algorithm to approximate inference in high-dimensional models
            such as LDA [20]. This overcomes the difficulty to directly and exactly estimate
            parameters that maximize the likelihood of the whole data collection defined as:
            QM
            →
            −
            →
            −
            −
            −
            −
            −
            p(W |→
            α , β ) = m=1 p(→
            w m |→
            α , β ) for the whole data collection W = {→
            w m }M
            m=1
            →
            −
            −
            knowing the Dirichlet parameters →
            α and β .
            The first use of Gibbs Sampling for estimating LDA is reported in [18] and
            a more comprehensive description of this method can be found in [20].
            The next section describes the income of the LDA technique. The input of
            the LDA method is an automatic summary of each document of the train corpus.
            These summaries are built with different systems.
            3.2

            Automatic text summarization systems

            Various text summarization systems have been proposed over the years [21].
            Two baseline systems as well as the ARTEX summarization system, that reaches
            state-of-the-art performance [22], are presented in this section.
            Baseline first (BF) The Baseline first (or leadbase) selects the n first sentences
            of the documents, where n is determined by a compression rate. Although very

            β

            φ
            word
            distribution

            α

            θ

            z

            w

            topic
            distribution

            topic

            word

            N

            D

            Fig. 2. LDA Formalism.

            simple, this method is a strong baseline for the performance of any automatic
            summarization system [23, 24]. This very old and very simple sentence weighting
            heuristic does not involve any terms at all: it assigns highest weight to the first
            sentences of the text. Texts of some genres, such as news reports or scientific
            papers, are specifically designed for this heuristic: e.g., any scientific paper contains a ready summary at
            the beginning. This gives a baseline [25] that proves to
            be very hard to beat on such texts. It is worth noting that in Document Understanding Conference (DUC)
            competitions [25] only five systems performed above
            this baseline, which does not demerit the other systems because this baseline is
            genre-specific.
            Baseline random (BR) The Baseline random [23] randomly selects n sentences of the documents, where n is also
            determined by a compression rate.
            This method is the classic baseline for measuring the performance of automatic
            summarization systems.
            ARTEX AnotheR TEXt (ARTEX) algorithm [22] is another simple extractive
            algorithm. The main idea is to represent the text in a suitable space model
            (VSM). Then, an average document vector that represents the average (the
            “global topic”) of all sentence vectors is constructed. At the same time, the
            “lexical weight” for each sentence, i.e. the number of words in the sentence, is
            obtained. After that, the angle between the average document and each sentence
            is calculated. Narrow angles α indicate that the sentences near the “global topic”
            should be important and are therefore extracted. See Figure 3 for the VSM of
            words: p vector sentences and the average “global topic” are represented in a N
            dimensional space of words. The angle α between the sentence −
            s→
            µ and the global
            →
            −
            topic b is processed as follow:
            →
            − −
            b × s→
            µ
            cos(α) = →
            −
            −
            →
            || b ||.||sµ ||

            (1)

            VSM of words
            W1

            b Global topic

            S1

            Sμ

            α
            S2

            Sentence
            WN

            Sp
            Wj

            Fig. 3. The “global topic” in a Vector Space Model of N words.

            Next, a weight for each sentence is calculated using their proximity with the
            “global topic” and their “lexical weight”. In Figure 4, the “lexical weight” is
            represented in a VSM of p sentences. Narrow angles indicate that words closest
            to the “lexical weight” should be important. Finally, the summary is generated
            concatenating the sentences with the highest scores following their order in the
            original document. Formally, ARTEX algorithm computes the score of each sentence by calculating the inner
            product between a sentence vector, an average
            pseudo-sentence vector (the “global topic”) and an average pseudo-word vector(the“lexical weight”). Once the
            pre-processing is complete, a matrix S[pN ] (N
            words and p sentences) is created. Let −
            s→
            µ = (sµ,1 , sµ,2 , ..., sµ,N ) be a vector of the
            −
            sentence µ = 1, 2, ..., p. The average pseudo-word vector →
            a = [aµ] was defined
            as the average number of occurrences of N words used in the sentence −
            s→
            µ:
            aµ =

            1 X
            sµ,j
            N j

            (2)

            →
            −
            and the average pseudo-sentence vector b = [bj ] as the average number of
            occurrences of each word j used through the p sentences:
            bj =

            1X
            sµ,j
            p µ

            (3)

            The weight of a sentence −
            s→
            µ is calculated as follows:
            −
            →
            − →
            →
            −
            w(−
            s→
            µ) = ( s × b ) × a
            
            
            N
            1 X
            sµ,j × bj  × aµ ; µ = 1, 2, . . . , p
            =
            Np j=1

            (4)

            The w(•) computed by Equation 4 must be normalized between the interval
            →
            −
            −
            [0, 1]. The calculation of (→
            s × b ) indicates the proximity between the sentence

            VSM of sentences
            Lexical weight
            a

            S1
            wN

            α

            w1

            Word
            wj

            Sp

            w2

            Sμ

            Fig. 4. The “lexical weight” in a Vector Space Model of p sentences.

            →
            −
            −
            −
            →
            − →
            →
            −
            s→
            µ and the average pseudo-sentence b . The product ( s × b ) × a weight this
            →
            −
            →
            −
            −
            →
            proximity using the average pseudo-word a . If a sentence sµ is near b and their
            −
            →
            corresponding element aµ has a high value, therefore sµ will have a high score.
            −
            −
            → →
            Moreover, a sentence −
            s→
            µ far from a main topic (i.e. sµ × b is near 0) or their
            corresponding element am u has a low value, (i.e. am u are near 0), therefore −
            s→
            µ
            will have a low score.
            It is not really necessary to divide the scalar product by the constant N1p ,
            →
            −0
            P
            →
            −
            →
            −
            because the angle α between b and −
            s→
            µ is the same if b = b =
            µ sµ,j . The
            element aµ is only a scale factor that does not modify α [22]:
            

            N
            X

            

            1
            
            w(−
            s→
            sµ,j × bj  × aµ ; µ = 1, 2, . . . , p
            µ )∗ = p
            N 5 p3 j=1

            (5)

            p
            The term 1/ N 5 p3 is a constant value, and then w(•) (Equation 4) and
            w(•)∗ (Equation 5) are both equivalent.
            This summarization system outperforms the CORTEX [26] one with the
            FRESA [15, 14] measure. ARTEX is evaluated with several corpus such as the
            Medecina Clinica [22]. ARTEX performance is then better than CORTEX on
            English, Spanish or French, which are the targeted languages in this study.

            4

            Evaluation of LDA model quality

            The previous section described different summarization systems to reduce the
            size of train corpus and to retain only relevant information contained into the
            train documents. This section proposes a set of metrics to evaluate the quality
            of topic spaces generated from summaries of the train documents. The first one
            is the perplexity. This score is the most popular one. We also propose to study

            another measure to evaluate the dispersion of each word into a given topic space.
            This measure is called the Jensen-Shannon (J S) divergence.
            4.1

            Perplexity

            Perplexity is a standard measure to evaluate topic spaces, and more generally
            a probabilistic model. A topic model Z is effective if it can correctly predict
            an unseen document from the test collection. The perplexity used in language
            modeling is monotonically decreasing in the likelihood of the test data, and is
            algebraically equivalent to the inverse of the geometric mean per-word likelihood.
            A lower perplexity score indicates better generalization performance [2]:
            )
            (
            M
            1 X
            log P (w)
            (6)
            perplexity(B) = exp −
            NB
            d=1

            with
            NB =

            M
            X

            Nd

            (7)

            d=1

            where NB is the combined length of all M testing terms and Nd is the number
            of words in the document d; P (w) is the likelihood that the generative model
            will be assigned to an unseen word w of a document d in the test collection.
            The quantity inside the exponent is called the entropy of the test collection. The
            logarithm enables to interpret the entropy in terms of bits of information.
            4.2

            Jensen-Shannon (J S) divergence

            The perplexity evaluates the performance of a topic space. Another important
            information is the distribution of words in each topic. The Kullback-Leibler
            divergence (KL) estimates how much a topic is different from the N topics
            contained in the topic model. This distribution is defined hereafter:
            X
            pi
            (8)
            KL(zi , zj ) =
            pi log
            pj
            w∈A

            where pi = P (w|zi ) and pj = P (w|zj ) are the probabilities that the word w is
            generated by the topic zi or zj . Thus, the symmetric KL divergence is named
            Jensen-Shannon (J S) divergence metric. It is the mid-point measure between
            KL(zi , zj ) and KL(zj , zi ). J S is then defined with equation 8 as the mean of
            the divergences between (zi , zj ) and (zj , zi ) as:
            1
            (KL(zi , zj ) + KL(zj , zi ))
            2
            
            
            1 X
            pi
            pj
            =
            pi log
            + pj log
            .
            2
            pj
            pi

            J S(zi , zj ) =

            w∈A

            (9)

            The J S divergence for the entire topic space is then defined as the divergence
            between each pair of topics composing the topic model Z, defined in equation 9
            as:
            J S(Z) =

            X X

            J S(zi , zj )

            zi ∈Z zj ∈Z

            =

            1 X X X
            pi
            pj
            pi log
            + pj log .
            2
            pj
            pi

            (10)

            zi ∈Z zj ∈Z w∈A

            p

            if i = j ⇒ log pji = 0 (log1 = 0). After defining the metrics to evaluate the
            quality of the model, the next section describes the experiment data sets and
            the experimental protocol.

            5

            Experiments

            These summarization systems are used to compress and retain only relevant
            information into train text collection in each language. This section presents
            the experiments processed to evaluate the relevance and the effectiveness of
            the proposed system of fast and robust topic space building. First of all, the
            experimental protocol is presented, and then a qualitative analysis of obtained
            results is performed using evaluation metrics described in Section 4.
            5.1

            Experimental protocol

            In order to train topic spaces, a large corpus of documents is required. Three
            corpus was used. Each corpus C is in a particular language (English, Spanish and
            French), and is composed of a training set A and a testing set B. The corpus
            are composed of articles from Wikipedia. Thus, for each of the three languages,
            a set of 100,000 documents is collected. 90% of the corpus is summarized and
            used to build topic spaces, while 10% is used to evaluate each model (no need
            to be summarized).
            Table 1 shows that the latin languages (French and Spanish) have a similar
            size (a difference of less than 4% is observed), while the English one is bigger
            than the others (English text corpus is 1.37 times bigger than French or Spanish
            corpus). In spite of the size difference of corpus, both of them have more or less
            the same number of words and sentences in an article. We can also note that
            the English vocabulary size is roughly the same (15%) than the latin languages.
            Same observations can be made in Table 2, that presents statistics at document
            level (mean on the whole corpus). In next section, the outcome of this fact is
            seen during the perplexity evaluation of topic spaces built from English train
            text collection.
            As set of topic spaces is trained to evaluate the perplexity and the JensenShannon (J S) scores for each
            language, as well as the processing time to summarize and compress documents from the train corpus.
            Following a classical

            Table 1. Dataset statistics of the Wikipedia corpus.
            Language #Words #Unique Words #Sentences
            English 30,506,196
            2,133,055
            7,271,971
            Spanish 23,742,681
            1,808,828
            5,245,507
            French 25,545,425
            1,724,189
            5,364,825

            Table 2. Dataset statistics per document of the Wikipedia corpus.
            Language #Words #Unique Words #Sentences
            English
            339
            24
            81
            Spanish
            264
            20
            58
            French
            284
            19
            56

            study of LDA topic spaces quality [27], the number of topics by model is fixed
            to {5, 10, 50, 100, 200, 400}. These topic spaces are built with the MALLET
            toolkit [28].
            5.2

            Results

            The experiments conducted in this paper are topic-based concern. Thus, each
            metric proposed in Section 4 (Perplexity and J S) are applied for each language
            (English, Spanish and French), for each topic space size ({5, 10, 50, 100, 200, 400}),
            and finally, for each compression rate during the summarization process (10%
            to 50% of the original size of the documents). Figures 5 and 6 present results
            obtained by varying the number of topics (Figure (a) to (c)) and the percentage
            of summary (Figure 6), respectively for perplexity and Jensen-Shannon (J S)
            measures. Results are computed with a mean among the various topic spaces
            size and a mean among the different reduced summaries size. Moreover, each
            language was study separately to point out differences of topic spaces quality
            depending on the language.
            7
            20
            6
            15

            7
            Full text

            ARTEX

            BF

            BR

            6

            5

            5

            4

            4

            10
            3
            5 10
            50 100 200 400
            number of topics (log scale)
            (a) English

            3
            5 10
            50 100 200 400
            number of topics (log scale)
            (b) Spanish

            5 10
            50 100 200 400
            number of topics (log scale)
            (c) French

            Fig. 5. Perplexity (×10−3 ) by varying the number of topics for each corpus.

            7

            7

            6

            6

            5

            5

            20

            15

            4
            10

            Full text

            ARTEX

            BF

            BR

            3
            10

            20
            30 40 50
            % summary (log scale)
            (a) English

            4
            3

            10

            20
            30 40 50
            % summary (log scale)
            (b) Spanish

            10

            20
            30 40 50
            % summary (log scale)
            (c) French

            Fig. 6. Perplexity (×10−3 ) by varying the % summary for each corpus.
        </corps>
        <conclusion>
            In this paper, a qualitative study of the impact of documents summarization in
            topic space learning is proposed. The basic idea that learning topic spaces from
            compressed documents is less time consuming than learning topic spaces from
            the full documents is noted. The main advantage to use the full text document
            in text corpus to build topic space is to move up the semantic variability into
            each topic, and then increase the divergence between these ones. Experiments
            show that topic spaces with enough topics size have more or less (roughly) the
            same divergence.
            Thus, topic spaces with a large number of topics, i.e. suitable knowing the
            size of the corpus (more than 200 topics in our case), have a lower perplexity, a
            better divergence between topics and are less time consuming during the LDA
            learning process. The only drawback of topic spaces learned from text corpus of
            summarized documents disappear when the number of topics comes up suitable
            for the size of the corpus whatever the language considered.
        </conclusion>
        <discussion>
            The results reported in Figures 5 and 6 allow us to point out a first general
            remark, already observed in section 5.1: the two latin languages have more or
            less the same tendencies. This should be explained by the root of these languages,
            which are both latins.
            Figure 5 shows that the Spanish and French corpus obtain a perplexity between 3,000 and 6,100 when the
            number of classes in the topic space varies.
            Another observation is that, for these two languages, topic spaces obtained with
            summarized documents, outperform the ones obtained with complete documents
            when at least 50 topics are considered (Figures 5-b and -c). The best system for
            all languages is ordered in the same way. Systems are ordered from the best to
            the worst in this manner: ARTEX, BF (this fact is explained in the next part
            and is noted into J S measure curves in Figures 7 and 8), and then BR. If we
            considerer a number of topics up to 50, we can note that the topic spaces, from
            full text documents (i.e. not summarized) with an English text corpus, obtain
            a better perplexity (smaller) than documents processed with a summarization
            system, that is particularly visible into Figures 6.
            To address the shortcoming due to the size of the English corpus (1.37 times
            bigger than latin languages), the number of topics contained into the thematic
            space has to be increased to effectively disconnect words into topics. In spite of
            moving up, the number of topics move down the perplexity of topic spaces for all
            summarization systems (except random baseline (RB)), the perplexity obtained
            with the English corpus being higher than those obtained from the Spanish and
            French corpus.
            Among all summarization systems used to reduce the documents from the
            train corpus, the baseline first (BF) obtains good results for all languages. This
            performance is due to the fact that BF selects the first paragraph of the document as a summary: when a
            Wikipedia content provider writes a new article, he
            exposes the main idea of the article in the first sentences. Furthermore, the rest of
            the document relates different aspects of the article subject, such as historical or

            economical details, which are not useful to compose a relevant summary. Thus,
            this baseline is quite hard to outperform when the documents to summarize are
            from encyclopedia such as Wikipedia.

            2

            Full text

            ARTEX

            BF

            BR

            1

            0

            2

            2

            1

            1

            0
            5 10
            50 100 200 400
            number of topics (log scale)
            (a) English

            0
            5 10
            50 100 200 400
            number of topics (log scale)
            (b) Spanish

            5 10
            50 100 200 400
            number of topics (log scale)
            (c) French

            Fig. 7. Jensen-Shannon (×103 ) measure by varying the number of topics for each
            corpus.

            2

            Full text

            ARTEX

            BF

            BR

            1

            0

            2

            2

            1

            1

            0
            10

            20
            30 40 50
            % summary (log scale)
            (a) English

            0
            10

            20
            30 40 50
            % summary (log scale)
            (b) Spanish

            10

            20
            30 40 50
            % summary (log scale)
            (c) French

            Fig. 8. Jensen-Shannon (×103 ) measure by varying the % summary for each corpus.

            The random baseline (RB) composes its summary by randomly selecting a
            set of sentences in an article. This kind of system is particularly relevant when
            the main ideas are disseminated in the document such as a blog or a website.
            This is the main reason why this baseline did not obtain good results except
            for J S divergence measure (see Figures 7 and 8). This can be explained by
            the fact that this system selects sentences at different places, and then, selects
            a variable set of words. Thus, topic spaces from these documents contain a
            variable vocabulary. The J S divergence evaluates how much a word contained

            in a topic is discriminative, and allows to distinguish this topic from the others
            that compose the thematic representation.
            Figures 7 and 8 also show that Jensen-Shannon (J S) divergence scores between topics obtained a similar
            performance order of summarization systems
            for all languages corpus. Moreover, full text documents always outperform all
            topic spaces representation for all languages and all summary rates. The reason
            is that full text documents contain a larger vocabulary, and J S divergence is
            sensitive to the vocabulary size, especially when the number of topics is equal
            for summarized and full text documents. This observation is pointed out by Figures 8-b and -c where the
            means among topic spaces for each summary rate of
            full text documents are beyond all summarization systems. Last points of the
            curves show that topic spaces, with a high number of topics and estimated from
            summaries, do not outperform those estimated from full text documents, but
            become more and more closer to these ones: this confirms the original idea that
            have motivated this work.
            Tables 3 and 4 finally present the processing time, in seconds, by varying the
            number of topics for each language corpus, respectively with the full text and
            the summarized documents. We can see that processing time is saved when topic
            spaces are learned from summarized documents. Moreover, tables show that the
            processing times follow an exponential curve, especially for the full text context.
            For this reason, we can easily imagine the processing time that can be saved
            using summaries instead of the complete documents, which inevitably contain
            non informative and irrelevant terms.

            Table 3. Processing time (in seconds) by varying the number of topics for each corpus.
            System
            Language
            Full Text English Spanish French
            5
            1,861
            1,388
            1,208
            10
            2,127
            1,731
            1,362
            50
            4,194
            2,680
            2,319
            100
            5,288
            3,413
            3,323
            200
            6,364
            4,524
            4,667
            400
            8,654
            6,625
            6,751

            Table 4. Processing time (in seconds) by varying the number of topics for each corpus.
            System
            Language
            System
            Language
            System
            Language
            ARTEX English Spanish French
            BR English Spanish French
            BF English Spanish French
            5
            514
            448
            394
            5
            318
            265
            238
            5
            466
            301
            276
            10
            607
            521
            438
            10
            349
            298
            288
            10
            529
            348
            317
            50
            1,051
            804
            709
            50
            466
            418
            465
            50
            1031
            727
            459
            100
            1,565
            1,303
            1,039
            100
            652
            602
            548
            100
            1,614
            737
            680
            200
            2,536
            2,076
            1,573
            200
            919
            863
            838
            200
            2,115
            814
            985
            400
            3,404
            2,853
            2,073
            400
            1,081
            988
            978
            400
            2,784
            1,448
            988

            A general remark is that the best summarization system is ARTEX, but if
            we take into account the processing time during the topic space learning, the
            baseline first (BF) is the best agreement. Indeed, if one want to find a common
            ground between a low perplexity, a high J S divergence between topics and a
            fast learning process, the BF method should be chosen.
        </discussion>
        <biblio>
            1. Salton, G.: Automatic text processing: the transformation. Analysis and Retrieval
            of Information by Computer (1989)
            2. Blei, D., Ng, A., Jordan, M.: Latent dirichlet allocation. The Journal of Machine
            Learning Research 3 (2003) 993–1022
            3. Baeza-Yates, R., Ribeiro-Neto, B., et al.: Modern information retrieval. Volume
            463. ACM press New York (1999)
            4. Salton, G., McGill, M.J.: Introduction to modern information retrieval. (1983)
            5. Salton, G., Yang, C.S.: On the specification of term values in automatic indexing.
            Journal of documentation 29 (1973) 351–372
            6. Deerwester, S., Dumais, S., Furnas, G., Landauer, T., Harshman, R.: Indexing by
            latent semantic analysis. Journal of the American society for information science
            41 (1990) 391–407
            7. Bellegarda, J.: A latent semantic analysis framework for large-span language modeling. In: Fifth European
            Conference on Speech Communication and Technology.
            (1997)
            8. Hofmann, T.: Probabilistic latent semantic analysis. In: Proc. of Uncertainty in
            Artificial Intelligence, UAI ’ 99, Citeseer (1999) 21
            9. Bellegarda, J.: Exploiting latent semantic information in statistical language modeling. Proceedings of
            the IEEE 88 (2000) 1279–1296
            10. Suzuki, Y., Fukumoto, F., Sekiguchi, Y.: Keyword extraction using term-domain
            interdependence for dictation of radio news. In: 17th international conference on
            Computational linguistics. Volume 2., ACL (1998) 1272–1276

            11. Popescul, A., Pennock, D.M., Lawrence, S.: Probabilistic models for unified collaborative and
            content-based recommendation in sparse-data environments. In:
            Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,
            Morgan Kaufmann Publishers Inc. (2001) 437–444
            12. Louis, A., Nenkova, A.: Automatically Evaluating Content Selection in Summarization without Human
            Models. In: Empirical Methods in Natural Language
            Processing, Singapore (2009) 306–314
            13. Lin, J.: Divergence Measures based on the Shannon Entropy. IEEE Transactions
            on Information Theory 37 (1991)
            14. Saggion, H., Torres-Moreno, J.M., da Cunha, I., SanJuan, E.: Multilingual summarization evaluation
            without human models. In: 23rd Int. Conf. on Computational
            Linguistics. COLING ’10, Beijing, China, ACL (2010) 1059–1067
            15. Torres-Moreno, J.M., Saggion, H., Cunha, I.d., SanJuan, E., Velázquez-Morales,
            P.: Summary evaluation with and without references. Polibits (2010) 13–20
            16. Hofmann, T.: Unsupervised learning by probabilistic latent semantic analysis.
            Machine Learning 42 (2001) 177–196
            17. Minka, T., Lafferty, J.: Expectation-propagation for the generative aspect model.
            In: Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, Morgan Kaufmann
            Publishers Inc. (2002) 352–359
            18. Griffiths, T.L., Steyvers, M.: Finding scientific topics. Proceedings of the National
            academy of Sciences of the United States of America 101 (2004) 5228–5235
            19. Geman, S., Geman, D.: Stochastic relaxation, gibbs distributions, and the bayesian
            restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence (1984) 721–741
            20. Heinrich, G.: Parameter estimation for text analysis. Web: http://www. arbylon.
            net/publications/text-est. pdf (2005)
            21. Torres-Moreno, J.M.: Automatic Text Summarization. Wiley and Sons (2014)
            22. Torres-Moreno, J.M.: Artex is another text summarizer. arxiv:1210.3312 [cs.ir]
            (2012)
            23. Ledeneva, Y., Gelbukh, A., Garcı́a-Hernández, R.A.: Terms derived from frequent
            sequences for extractive text summarization. In: Computational Linguistics and
            Intelligent Text Processing. Springer (2008) 593–604
            24. Manning, C.D., Schütze, H.: Foundations of Statistical Natural Language Processing. The MIT Press,
            Cambridge, Massachusetts (1999)
            25. DUC: Document Understanding Conference. (2002)
            26. Torres-Moreno, J.M., Velázquez-Morales, P., Meunier, J.G.: Cortex : un algorithme
            pour la condensation automatique des textes. In: ARCo’01. Volume 2., Lyon,
            France (2001) 365–366
            27. Rosen-Zvi, M., Griffiths, T., Steyvers, M., Smyth, P.: The author-topic model for
            authors and documents. In: Proceedings of the 20th conference on Uncertainty in
            artificial intelligence, AUAI Press (2004) 487–494
            28. McCallum, A.K.:
            Mallet: A machine learning for language toolkit.
            http://mallet.cs.umass.edu (2002)
        </biblio>
    </article>
    <article>
        <preamble>Partitioning_large_signed_two-mode_networks__Problems_and_prospects.pdf</preamble>
        <titre>Partitioning Large Signed Two-Mode Networks: Problems and Prospects</titre>
        <auteur>
            Patrick Doreian a,b,∗ , Paulette Lloyd c , Andrej Mrvar b
            a
        </auteur>
        <abstract>
            While a substantial amount of attention within social network analysis (SNA) has been given to the study
            of one-mode networks, there is an increasing consideration of two-mode networks. Recent research on
            signed networks resulted in the relaxed structural balance (RSB) approach and its subsequent extension
            to signed two-mode networks involving social actors and social objects. We extend this approach to
            large signed two-mode networks, and address the methodological issues that arise. We develop tools to
            partition these types of networks and compare them with other approaches using a recently collected
            dataset of United Nations General Assembly roll call votes. Although our primary purpose is methodological,
            we take the ﬁrst step towards bridging Heider’s structural balance theory with recent theorizing in
            international relations on soft balancing of power processes.
            © 2012 Elsevier B.V. All rights reserved.
        </abstract>
        <introduction>
            1. Introduction
            While a substantial amount of attention within social network
            analysis (SNA) has been given to the study of one-mode networks,
            there is an increasing consideration of two-mode networks. It is
            now recognized that such data are particularly important (see
            Borgatti and Everett, 1992, 1997; Doreian et al., 2004; Latapy et al.,
            2008). Here we focus our attention on signed two-mode networks.
            While our primary focus here is methodological, we stress that the
            technical issues are driven by substantive concerns.
            The substantive problem described in Section 2.5 implies a need
            for tools designed to partition large1 two-mode signed networks.
            One source for such a partitioning tool is found in the work of
            Heider (1946, 1958). He focused on two types of triples involving
            signed relations: those involving three individuals and those of two
            individuals and a social object such as a belief. His work was foundational for structural balance theory. A
            central assumption of this

            ∗ Corresponding author at: Department of Sociology, 2602 WWPH, University of
            Pittsburgh, Pittsburgh, PA 16260, USA.
            E-mail address: pitpat@pitt.edu (P. Doreian).
            1
            Large is an inherently ambiguous term because it is a function of the size of the
            network, the algorithm used, the speed of a machine, and its memory. Intuitively, we
            think a network is ‘large’ when, for running a program, it ushers in signiﬁcant computational burdens.
            Direct blockmodeling is a computationally burdensome approach
            and we restrict the term ‘large’ for networks that greatly lengthen the running time
            of the relocation algorithm we use. Operationally, this can be speciﬁed as sizes above
            n1 > 100, n2 > 100 (using the notation introduced below) for two-mode networks. The
            islands technique (Zaveršnik and Batagelj, 2004) that we use in Section 6.3 can easily
            handle networks with millions of vertices because it is a linear-in-time algorithm.
            0378-8733/$ – see front matter © 2012 Elsevier B.V. All rights reserved.
            doi:10.1016/j.socnet.2012.01.002

            theory is that there is a tendency towards balance in these triples
            of relations. The outcomes of these tendencies for triples involving three actors are expressed in four folk
            aphorisms: “a friend
            of a friend is a friend”; “a friend of an enemy is an enemy”; “an
            enemy of a friend is an enemy”; and “an enemy of an enemy is a
            friend”. Such outcomes and the dynamics leading to network structures consistent with balance are for
            one-mode signed relations.
            Noting that many, if not most, signed networks do not have this
            exact form, Doreian and Mrvar (1996) proposed an algorithm for
            partitioning signed one-mode networks to obtain partition structures that were as close as possible to those
            predicted by structural
            balance theory. Recognizing that multiple processes can operate
            to generate signed structures, Doreian and Mrvar (2009) generalized structural balance to relaxed structural
            balance for one-mode
            networks to accommodate more complex signed block structures.
            Similar dynamics hold for networks involving social actors and
            social objects like beliefs or statements. These involve two-mode
            relations and, arguably, were more important in Heider’s formulation using unit formation relations (between
            social actors and social
            objects). This led to an expansion of the notion of relaxed structural balance (Mrvar and Doreian, 2009)
            through the development
            of a method for delineating the partition structure of two-mode
            signed networks. This paper extends that approach to address problems that may be encountered when
            partitioning signed two-mode
            data. We illustrate our approach with a recently collected dataset
            of United Nations General Assembly (UNGA) roll call votes, as they
            provide a natural example of this signed data type with states (as
            social actors) voting for or against resolutions (as social objects). In
            addition, the diversity among states and resolutions means countries are likely to have overlapping and even
            conﬂicting loyalties

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            that lead to more complex processes and outcomes—items that
            tend not to be considered within structural balance approaches.
            Typically, UNGA voting data have been analyzed using methods
            that locate primary divisions among states in an attempt to identify potential fault lines for conﬂict. Our
            approach to these data is
            unique in that we exploit their structural characteristics to further
            explore structural balance theory and the tools needed to partition
            large signed two-mode networks.
            The rest of this paper is organized as follows: Section 2 elaborates relaxed structural balance theory, its
            natural application to
            international relations data, and explores its complementarity with
            balance of power ideas. We describe the data in Section 3, elaborate
            our primary methodological approach in Section 4, and we provide
            the results from this approach in Section 5. We then explore the
            data using other partitioning approaches and compare the results
            with our own in Section 6. In Section 7 we discuss some of the
            methodological problems with using a blockmodeling approach to
            partition large signed two-mode data, as well as the potential for
            its use in exploring soft balancing of power processes. We conclude
            with recommendations for further methodological development of
            blockmodeling approaches to signed data.
        </introduction>
        <corps>
            2. Applying structural balance theory to international
            relations data
            We couple relaxed structural balance to balance of power ideas
            to provide a substantive foundation for the partitioning methods
            we consider here.
            2.1. Relaxed structural balance
            Heider (1946, 1958) provided the foundational statements for
            a major approach to signed networks known as structural balance
            theory. He focused on signed relations between people, signed relations between people and social objects,
            and the implications of
            social structures involving these signed relations. These structures
            of relations are in the form of particular triples. Triples involving
            three social actors (p, o, and q) are poq-triples and triples involving
            two social actors, p and o, and a social object, x, are pox-triples.
            Beliefs and ideas as examples of social objects are particularly
            salient for our consideration—and extension—of structural balance.
            Relations between actors are social relations and Heider labeled
            the ties between actors and social objects as unit formation relations. Examples of signed social relations
            between social actors
            are like/dislike, love/hate and respect/disrespect. For the unit formation relations, examples include
            accepting/rejecting beliefs and
            supporting/opposing ideas.
            There are 8 types of triples for a set of three relations between
            three actors (each tie can be positive or negative). Heider divided
            these triples into two types: balanced and unbalanced. Denoting
            positive ties by 1 and negative ties by −1, the sign of a triple is
            the product of the signs of the ties in the triple. A triple is deﬁned
            as balanced if the product of the signs in the triple is 1 and imbalanced if this product is −1. According
            to Heider, imbalanced triples
            are a source of strain for the individuals in them and the individuals will attempt to move from having
            imbalanced triples to having
            balanced triples through changes in the sign(s) of relations. For p
            in a poq-triple, if p has positive ties to o and q but knows that there
            is a negative tie between o and q, the sign of the triple is −1 and
            is imbalanced. According to Heider, p will attempt to balance the
            triple by changing the sign of a tie. However, if there are imbalanced
            triples in sets of either poq-triples or pox-triples then there will be
            many attempts, by different actors, to achieve balance. This makes
            achieving balance over a whole network of social actors a difﬁcult
            and error prone process (Hummon and Doreian, 2003).

            179

            Cartwright and Harary (1956) formalized Heider’s theory and
            focused on signed ties between social actors and, in effect, discarded
            unit formation relations. A binary signed network is an ordered pair
            (G, ), where:
            1. G = (U, A) is a digraph, without loops, having a set of vertices, U,
            and a set of arcs, A, where A is a subset of U × U; and
            2.  : A → {+1, −1} is a sign function where positive arcs have the
            sign +1 and negative arcs have the sign −1.
            If a network has multiple weak components, these components
            can be considered separately as distinct networks. Here, we assume
            that the digraph is weakly connected.2 Cartwright and Harary
            (1956) proved the following:
            Theorem 1. For a balanced signed network, (G, ), the vertices in
            U can be partitioned into two subsets3 such that each positive arc
            joins vertices in the same subset and each negative arc joins vertices
            in different subsets.
            Harary et al. (1965, pp. 342–3) prove that for all pairs of vertices
            in a balanced network, all semi-paths joining them have the same
            sign. Davis (1967) observed that, despite the appeal of Theorem
            1, there are social groups where there are more than two subsets
            of mutually hostile groups of social actors. He proposed that the all
            negative triple (with the sign of −1) not be classiﬁed as imbalanced.
            Consistent with this, he deﬁned a signed network as clusterable if
            it contains no semi-cycle with exactly one negative tie. He then
            proved:
            Theorem 2. For a clusterable signed network,4 the set of vertices, U,
            can be partitioned into two or more subsets such that every positive arc
            joins vertices in the same subset and every negative arc joins vertices
            in different subsets.
            The network structure implied by the above (structure) theorems can be described also in terms of
            blockmodeling (see Doreian
            et al., 2005: Chapter 10). We use the term position for a cluster of
            vertices (representing actors) and, given a set of positions, a block
            is a set of ties between positions. The diagonal blocks are positive
            (with only positive ties between vertices within subsets) and the
            off-diagonal blocks will be negative (with the negative ties between
            vertices in different subsets). A positive block is one that has only
            positive or null ties and a negative block has only negative or null
            ties. Rather than use both balanced and clusterable as terms, we
            use k-balance to cover both of them.5
            However, most empirical signed networks do not have a structure consistent with either of these two
            theorems. Put differently,
            most empirical signed networks are not k-balanced.6 Yet, if there
            are balance processes that are operative, they ought to leave

            2
            The presence of multiple weak components requires some mild restatement of
            the results but do not alter their intrinsic content.
            3
            If every tie in the network is positive then one of the two subsets is empty.
            4
            By deﬁnition, a signed network must contain at least one negative tie: if a network contains no negative
            ties it is an unsigned network. None of the balance
            theoretic processes involving both positive and negative ties have relevance for
            unsigned networks. By the same token, attempts to assess structural balance theory
            using unsigned data cannot provide a useful assessment. The only occasion when
            an unsigned network is relevant with regard to structural balance is when it is the
            outcome of balancing processes.
            5
            If there are only two subsets then we are dealing with balance (k-balance, k = 2)
            and if there are more than two subsets then we are dealing with clusterability (kbalance, k > 2).
            6
            This calls into question the validity of a general claim of a tendency towards
            balance. Doreian and Krackhardt (2001) examined the trajectories of the eight types
            of triples over time in the Newcomb data (Newcomb, 1961, Nordlie, 1958). Two of
            the balanced triples increased in number of time, consistent with Heider’s theory,
            but the other two balanced triples decreased in frequency over time. Two types of
            imbalanced triples decreased in frequency over time, also consistent with Heider,

            180

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            some structural traces. Instead of asking if signed networks are kbalanced or not, it is more fruitful,
            empirically, to seek partitions of
            signed networks that are as close to being k-balanced as possible. If an
            empirical network is k-balanced then there will be no inconsistent
            ties within either type of block. But if the network is not k-balanced
            then there will be some positive ties in negative blocks, or some
            negative ties in positive blocks, or both. We denote the number of
            negative inconsistencies by N and the number of positive inconsistencies by P. To locate partitions as close
            as possible to a perfect
            balanced partition, it is necessary to measure the extent to which
            a partition departs from zero inconsistencies. A simple and direct
            measure of this is (˛N + (1 − ˛)P) with 0
            <
            ˛
            <
            1 (varying ˛ permits
            a differential weighting of N and P). All that is needed is an algorithm to locate a partition(s) that
            minimizes this measure. Doreian
            and Mrvar (1996) proposed a partitioning algorithm—described in
            Section 4.1—that does exactly this.
            Doreian and Mrvar (2009) reconsidered the problem that most
            empirical signed networks are not exactly balanced. While balance theoretic mechanisms may be operative,
            they need not be
            the only mechanisms in play. Some members of a signed network may be universally regarded in positive terms
            even though
            they may belong to different positions with negative ties between
            the positions. If present, such actors imply positive blocks off the
            main diagonal. Expressed differently, differential popularity (Feld
            and Elmore, 1982) implies positive blocks off the main diagonal.
            Actors who play a mediating role between mutually hostile subsets also imply off-diagonal positive blocks
            and sets of mutually
            hostile actors (not blocks) imply negative blocks on the diagonal.
            In response to these considerations, Doreian and Mrvar (2009) proposed the notion of relaxed structural
            balance (RSB). This idea has
            three components: (i) the idea of positive and negative blocks is
            retained; (ii) these blocks can appear anywhere in the blockmodel;
            and (iii) the measure of inconsistency is retained. For such a blockmodel, then if (˛N + (1 − ˛)P) = 0 for a
            signed network then the
            network is a relaxed structurally balanced network. They show that
            RSB is a formal generalization of structural balance where the block
            structures anticipated by Theorems 1 and 2 are special cases. On ﬁtting the RSB model to some of the
            classical signed networks in the
            literature, they obtained better ﬁtting models that permitted more
            nuanced interpretations of the prior partitions of these signed networks. More importantly, thinking of
            positive and negative block
            types being located anywhere in a blockmodel opens the way to
            partitioning signed two-mode networks.
            2.2. Relaxed structural balance for signed two-mode networks
            For two-mode networks (also called afﬁliation networks), there
            are two sets of social actors. Typical examples include people attending events, individuals sitting on
            organizational boards
            of directors, and people belonging to multiple social or recreational organizations. Mrvar and Doreian
            (2009) extended relaxed
            structural balance to signed two-mode networks to consider US
            Supreme Court Justices (as social actors) supporting or dissenting
            from Supreme Court decisions (as social objects). In doing so, they
            returned to Heider’s unit formation relations and used signed twomode networks to formalize that aspect of
            Heider’s theory. The
            general formalism extends straightforwardly.
            Let U = (u1 , u2 , . . . , un1 ) denotes the set of social actors (represented as vertices) and V = (v1 , v2
            , . . . , vn2 ) denotes the set of social
            objects (represented as vertices). The number of vertices in U, is
            denoted by n1 and the number of vertices in V, is denoted by n2 .

            but the other two types of imbalanced triples increased and became more frequent
            over time.

            An undirected binary signed two-mode network is an ordered pair,
            (G, ), where:
            1. G = (U, V, E) is a bipartite network having two sets of vertices,
            U and V, and a set of edges,7 E, where E ⊆ U × V (where U ∩ V is
            empty); and
            2.  : E → {+1, −1} is a sign function where positive edges have the
            sign +1 and negative edges have the sign −1.
            Clearly, with two-mode networks, the idea of diagonal and offdiagonal blocks does not apply and having
            positive and negative
            blocks appearing anywhere follows naturally. All of the formal
            development for one-mode networks extends straightforwardly to
            two-mode networks. Instead of partitioning a one-mode network
            into k clusters, a two-mode network is partitioned into k1 clusters
            of rows (social actors) and k2 clusters of columns (social objects).
            We deﬁne a (k1 , k2 ) partition of (G, ) as one where there are k1
            clusters in the partition of U and k2 clusters in the partitions of V.
            Let C = (C1 , C2 ) denote a (k1 , k2 ) partition of (G, ) where C1 is a partition of V and C2 is a
            partition of V. The measure of inconsistency for
            the (k1 , k2 ) partition of (G, ) remains (˛N + (1 − ˛)P). Mrvar and
            Doreian (2009) adapted their one-mode algorithm to identify partitions of two-mode signed networks that
            minimize this measure
            of inconsistency.
            2.3. Coupling relaxed structural balance with balance of power
            processes
            Heider’s (1946) approach to structural balance theory is social
            psychological which, at face value, is quite different from the ideas
            in social network analysis. Yet, they can be coupled in fruitful
            ways (Robins and Kashima, 2008). There are micro-level processes
            operating at the social actor (vertex) level—what an actor does
            in speciﬁc situations—and macro-level structural processes affecting the structure as a whole. The two
            processes act to constrain
            each other: while actors are free to do whatever they want to
            do (given their interests), they need to be mindful of what others are doing and the nature of the social
            relations within which
            they are located.8 Robins and Kashima point out that accepting
            this approach is to accept also a dynamic view of these structural
            processes. We note that Heider’s theory about empirical triples
            embraces a very dynamic approach featuring change as an essential
            part of generating structural outcomes.
            We explore the macro-level implications of Heider’s structural
            balance theory in two ways. First, we extend Heider’s assumption
            of a tendency towards balance among multiple actors to balance
            among state actors in the international system of states. Balance of
            power theories in International Relations research share assumptions of balancing processes among states,
            and alliance formation
            implied by balancing processes evokes the same four folk aphorisms (e.g., “a friend of an enemy is an
            enemy”. . .). In the post-Cold
            War period of unipolar military power, international relations theorists have proposed the idea of “soft
            power” balancing through

            7
            Our choice of using edges rather than arcs is driven primarily by the empirical
            example. With resolutions and states, there is no obvious way of determining the
            arcs linking them. If (the representatives of) states cast their votes about the resolution the arcs could
            go from states to resolutions. But if resolutions prompt the states
            to vote in certain ways the arcs go from resolutions to states. It is simpler to couple
            states and resolutions by edges where the signs indicating how they voted. There
            are no edges between states and none between resolutions. The formalization goes
            through with little change if arcs are used instead of edges.
            8
            As individuals, Romeo and Juliet were free to fall in love with each other despite
            belonging to two different mutually hostile families. This macro-feature was a powerful constraint on their
            choices through the actions of others with whom they were
            linked. The imbalance implied by their love drives the drama of Shakespeare’s Romeo
            and Juliet.

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            norms. We explore this idea using UN General Assembly (UNGA)
            military resolutions as they contain evidence of both hard (decisions to go to war) and soft (norms and laws
            to constrain military
            power) balancing of power processes.
            Second, we show how the relaxed structural balance approach
            can be used to explore balancing of power processes. We do this
            in the course of developing the methodological contribution of this
            paper, namely, identiﬁcation of and solutions for problems with
            applying the relaxed structural balance approach to large signed
            two-mode data.
            2.4. Balance of power and international relations
            Balance of power is an International Relations (IR) theory that
            addresses the process of building coalitions among states to prevent
            one state from becoming too powerful. There are two variants of
            this theory—‘hard’ and ‘soft’. Hard balance of power theory is rooted
            in military alliances and arms build-ups associated with them. Soft
            balancing behavior, in contrast, is considered a state strategy, or
            foreign policy conduct, to prevent a rising power from assuming
            hegemony but without a primary emphasis on force. Some balance of power is the outcome at the systemic or
            sub-systemic
            levels that result in power equilibria among key states. Indeed,
            recent debates in the IR literature question whether in an age of a
            unipolar (U.S. hegemonic) power, states are currently engaging in
            traditional hard balancing behavior. Instead, some have argued that
            diplomatic coalitions and norms building (as soft balancing) deﬁne
            much of state behavior.9 Soft balancing strategies can include informal alliances, collaboration in regional
            or international institutions,
            or voting and veto power in international organizations such as
            the United Nations (UN) (Art, 2005/2006; Brooks and Wohlforth,
            2005/2006; Paul, 2004). Hard balancing for social relations in onemode networks leads to states within an
            alliance having positive
            ties to other states in the alliance and negative ties to states in
            other alliances. The core idea is consistency in the relations formed.
            Soft balancing also involves consistency in the sense of states in
            blocs voting in the same way on resolutions of central concern to
            bloc members and voting against resolutions supported by (some)
            other blocs of states.
            It is generally accepted that the UN is both the pre-eminent
            international organization (IO), and that it plays a central role in
            the maintenance of international security. Although the UN Security Council (UNSC) is the UN organ most
            associated with the use
            of, or constraints on, the exercise of military power, the UNGA has
            also played an important, if less publicly visible, role. The First Committee, one of the six permanent
            committees of the UNGA, has
            worked to develop international norms and laws aimed at restricting the exercise of military power. Although
            its resolutions are
            non-binding, many have crystallized into customary law while others have become the basis for treaties that
            limit military power
            either by prohibiting the use of some weapons (e.g., anti-personnel
            mines, chemical or biological weapons) to efforts to create zones of
            peace that prohibit nuclear weapons altogether.
            Voting analyses on military resolutions typically reveal voting
            similarity across key issues by states who share alliances or key
            attributes and afﬁliations. Consequently, these data lend themselves to an exploration of the relaxed
            structural balance approach,
            and provide an opportunity to explore the relationship of Heider’s
            structural balance theory with primarily soft balancing of power
            processes theorized in IR theory. We compare two time periods

            9
            See for example, World Politics 61 (1), January 2009, and International Security
            30 (1), Summer 2005; both are special issues devoted to examining the continual
            utility of the balance of power concept in international relations given the characteristics of current
            world politics.

            181

            in order to delineate the structure of the voting array at each
            time point and to assess change between them. The ﬁrst is in the
            late Cold War period characterized by ideologically and militarily
            opposed groups of states. The second time period examines the
            5–10 years after the end of the Cold War. This period includes the
            dissolution of the Warsaw Pact and the re-alignment of many former Soviet bloc countries with NATO and
            European Union member
            states. We expect empirically to ﬁnd similar clustering of states
            as prior studies as far as the large clusters of states (often seen
            as voting blocs) are concerned,10 but also evidence of additional
            sub-clustering within the major blocs that can be exploited for our
            theoretical purposes. Part of this further exploration concerns the
            clusters of resolutions in relation to the voting blocs. To the extent
            that hard balancing involves military issues with a real possibility
            of warfare, states ‘choosing’ sides face huge consequences for their
            choices. Voting on military resolutions before UNGA, in terms of
            committed resources, is less consequential but far more subtle in
            its implications. The notion of being consistent for states within
            voting blocs remains in place but permits greater ﬂexibility when
            there are cross-cutting issues.
            2.5. Empirical implications
            Balance of power arguments are made in terms of blocs (clusters
            as positions) of states allied with each other and mobilized against
            blocs of other states. With alliance-based sets of interests, members
            of an alliance are expected to act together in opposition to members of other alliances. Members of the
            other alliances also act in
            concert. If the interests of alliance members are activated by resolutions before the UNGA, then these
            members are expected to vote
            similarly to other members on these resolutions. This can take the
            form of all voting in support of some resolutions and voting against
            other resolutions. During the era of the Cold War this idealized voting pattern could be illustrated by NATO
            states voting together and
            members of the Warsaw Pact voting together and in opposition to
            NATO.
            However, members of one alliance can have interests beyond
            those implied by membership in a particular military or political
            alliance—or in an international organization. Some speciﬁc issues
            could divide alliance members so that members of an alliance do
            not vote in the same fashion across all issues. Generally, we expect
            patterns in voting behavior to reveal long-standing coalitions of
            states that are similar on attributes or afﬁliations, but we also
            expect a certain amount of self-interest, logrolling, and bargaining to result in other temporary
            coalitions. Our aim is to show how
            the RSB approach can be used to locate balancing of power processes. Speciﬁcally, we expect that
            simultaneously partitioning of
            both states and resolutions will highlight broad patterns of joint
            voting and also lesser patterns of temporary coalition formation.
            We illustrate our approach in Fig. 1.
            Fig. 1 shows 12 states and 13 resolutions in a hypothetical voting array. The top panel shows a two-mode
            network composed of
            states, s1 through s12 , that are represented by circles, and resolutions, r1 through r13 , represented by
            squares. Solid lines between
            states and resolutions represent votes in favor of resolutions. In
            contrast, the dashed lines represent votes against resolutions. The
            states have been placed into three subsets that can be labeled as S1 ,
            S2 and S3 . The states in S1 are s1 through s7 ; the states in S2 are s6
            though s9 ; and S3 is composed of s10 through s12 . The resolutions
            have also been placed in three subsets: R1 contains r1 through r7 ;
            R2 contains r8 through r10 ; and R3 contains r11 through r13 . The
            structure of this network can be described simply. States in S1 tend

            10

            See Kim and Russett (1996) and Voeten (2000) for example.

            182

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            Fig. 1. Illustrative network and formatted array for states and voting on resolutions.

            to vote in support of all resolutions; states in S2 vote in support
            of resolutions in R1 and R2 but tend to vote against resolutions in
            R3 ; and states in S3 tend support only resolutions in R1 and tend
            to oppose resolutions in both R2 and R3 . There are also some votes
            not ﬁtting with this general description.
            The lower panel of Fig. 1 contains an alternative representation
            of exactly the same information but in a formatted array. Black

            squares represent votes in support of resolutions and red diamonds
            represent votes against resolutions. The array has been formatted
            to that members of the above clusters of states and resolutions
            are placed together. Solid lines mark the boundaries between the
            clusters. The names of the resolutions are written across the top
            and the names of states are on the right side of the array. The signed
            block structure for this hypothetical example, using the labeling of

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            P for positive blocks and N for negative blocks, is as follows: in the
            top row, the blocks are PPP; the blocks in the second row are PPN;
            and the blocks in the bottom row are PNN. When the two-mode
            networks are large, the network diagrams get to be unwieldy and
            difﬁcult to read. For large datasets, the formatted arrays are simpler
            to read and form a more compact representation.
            The real data for the UNGA voting are more complex in two
            ways: (i) trivially, they are much larger and (ii) exceptions are far
            more likely to occur than revealed in the hypothetical motivating
            example. In short, the real data are messy but, we expect blockmodeling to reveal distinct patterns in the
            form of positive and
            negative blocks, despite expected exceptions. A disadvantage is
            that the exceptions will make the network diagrams challenging
            to read in the standard blockmodeling format; therefore we will
            report the majority of our graphical results using the formatted
            (blockmodeling) arrays.
            3. Data
            We use a dataset of roll call votes for the United Nations General Assembly (UNGA) covering the decades
            before and after the
            end of the Cold War and the dissolution of the Warsaw Pact.11 The
            data were divided into four time periods: 1981–1985, late Cold
            War; 1986–1990, a ﬁrst transition period; 1991–1995, a second
            transition period; and 1996–2001, a “settled” period, albeit at the
            juncture of 9/11. As noted above, we select the ﬁrst and last periods
            which we label as Time 1 and Time 2, respectively.
            We focus on military resolutions because this subset of resolutions is the most substantively appropriate
            for testing our
            coupling of balance of power ideas with relaxed structural balance
            theory.12 Given the type of data—international voting data—where
            states have overlapping and even conﬂicting loyalties (Turkey
            with NATO and the Organization of Islamic Conference (OIC) for
            example)—voting blocs are likely to shift by issue areas. The potential advantage of blockmodeling is that
            it can identify and deﬁne
            blocs by the resolutions favored by the states in those blocs. Other
            methods can also accomplish this but blockmodeling has a direct
            way of grouping these together for further exploration by representing the underlying network structure in
            image matrices using
            rows of block types (PPP, PPN, etc.).
            The Time 1 data set has 141 states and 276 resolutions. While
            there were 159 UN member states during this time period, we
            deleted 18 states because they were absent for 25% or more of the
            votes during this time period. Our second time period (Time 2) had
            153 states and 150 resolutions.13 There were 189 UN member states

            11
            The data were collected by the second author from 2000 to 2002. Roll call votes
            from 1983 to 2001 had been digitized. A summary of all resolutions on roll call votes
            is found at http://www.un.org/documents/resga.htm and speciﬁcs on roll call votes
            are found at: http://unbisnet.un.org/. Roll call votes from 1981 to 1983 were found in
            hard copy at Yale University Social Science Library, 140 Prospect Street, Government
            Documents collection, United Nations Collection; the Government Librarian was
            very helpful. Assistance was also provided by the United Nations Dag Hammarskjold
            Library. UN Librarians and Personnel in the Department of Public Information (UN
            Headquarters, 1st Avenue between 45th and 46th Street, New York City) were very
            helpful in obtaining and verifying information.
            12
            See http://www.un.org/en/ga/maincommittees/index.shtml (accessed 7/4/11)
            for information on the main committees and http://www.un.org/documents/
            resga.htm to access the resolutions. Most years contain a summary of the resolutions deliberated upon that
            year, including information on where the resolution
            was debated, e.g., one of the permanent committees or the Plenary; information on
            the voting; draft documents that includes sponsorship information and the general
            topic of the resolution.
            13
            There were 129 military resolutions for the period 1996–2000 and 150 if we
            included the 21 resolutions for 2001. We experimented with both in case there
            were 9/11 effects but found the output to be substantially similar. We report the
            129 resolutions for the blockmodeling results because we matched 5 year periods
            for both time points.

            183

            as of 2001; we removed 34 states that were absent for 25% or more
            of the votes during this time period. A full list of the names is provided in Appendix A and a summary of
            the resolutions is provided in
            Appendix B. The continual process of decolonization and increase in
            membership of newly independent states, including those resulting
            from the dissolution of the Soviet Union resulted in an increase of
            roughly 30 new member states for our second time period. Because
            our focus is on balancing processes and changing alliances, we think
            it is important to include all of the new states rather than focus on
            the same subset of states for both time periods. This is particularly
            important given the enlargement of NATO and the EU with former
            Soviet bloc states. The number of resolutions decreased because of
            a deliberate effort by the UNGA to reach consensus on resolutions
            once the Cold War impediment to international policy-making had
            ended. This results in a signiﬁcant reduction in roll call votes. There
            were a total of 725 roll call votes during our ﬁrst time period and
            406 in our second. Of these, military resolutions were the most
            numerous (38% and 37% respectively), highlighting the importance
            of these issues to the UN mandate to promote international peace
            and security.
            Coding the votes cast on military (and other types of) resolutions
            is not straightforward. Ideally, there are only votes for a resolution and votes against a resolution.
            However, states can abstain
            from voting on certain resolutions and their representatives may
            choose to be absent when a vote is taken on other resolutions. These
            absences could be coded as 0, but this decision would obscure deliberate absences. Interviews with UN
            delegates and permanent UNGA
            personnel14 indicate that frequently absences can be grouped with
            abstentions as a “weak no” vote. Therefore we made the choice of
            ﬁrst removing all countries that were absent 25% or more of the
            time for a given subset of roll call votes. We then recoded the few
            remaining absences as a “−1”. We coded votes in support of a resolution as “+1”. We believe this coding
            accurately captures state
            voting patterns.
            4. Methods
            4.1. Direct signed blockmodeling
            Signed blockmodeling is a label for the partitioning of signed networks and direct signed blockmodeling is
            the approach based on the
            ideas described in Sections 2.1–2.4 and used here. The Doreian and
            Mrvar algorithm for doing this has the following features:
            1. The measure of inconsistency described earlier can be used as
            a criterion function (Doreian et al., 2005) measuring departures
            from exact balance. Departures from exact balance take only two
            forms: positive ties in negative blocks and negative ties in positive blocks. As before, N denotes the
            number of negative ties in
            positive blocks and P denote the number of positive ties in negative blocks. Let C denote a clustering of
            the vertices into mutually
            exclusive subsets and let P(C) denote the value of the criterion
            function for that clustering, C, then, P(C) = ˛N + (1 − ˛)P where
            0
            <
            ˛
            <
            1.
            2. Two ranges and one value for ␣ can be distinguished: (i)
            0
            <
            ˛
            <
            0.5 (where positive inconsistencies are weighted more

            14
            The second author interviewed a number of UN delegates and permanent personnel when she collected her voting
            data during the 2001 UNGA session. Without
            exception, interviewees noted that both an “abstain” and frequently an absence was
            a “weak no” vote. Similarly, Voeten (2000, p. 193) argues that there is little practical
            difference between a “no” vote and an abstention. What matters is the willingness
            of a state to go on record for supporting a resolution. To separate out a situation of
            high absenteeism from a voting choice, we omitted all states absent for 25% of the
            time. This took care of the vast majority of absences that would be coded as a “no”
            vote.

            184

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            heavily); (ii) ˛ = 0.5 (where positive and negative inconsistencies are weighted equally); and iii) 0.5
            <
            ˛
            <
            1 (where negative
            inconsistencies are weighted more heavily).
            3. There is a relocation algorithm that is based on a neighborhood of
            a clustering, C, deﬁned by two transformations: (i) the movement
            of one vertex from one plus-set to another plus-set in C; and (ii)
            the interchange of two vertices between two plus sets in C. Given
            a partition, C, this leads to a local optimization procedure where
            these two types of transformations are the only ones considered.
            4. Given a partition, C, the effect of these local transformations is
            examined in terms of their impact on P(C). If a transformation
            leads to another clustering, C , such that P(C) > P(C ) for C in the
            neighborhood of C, then C replaces C as the active clustering.
            This is continued until no further drop in the value of criterion
            function is possible.
            5. The whole procedure is repeated many times15 (because it is a
            local optimization procedure) until no further improvement is
            possible. The ﬁnal partition(s) is (are) the optimal16 partition(s)
            where optimal means only that the partition the minimal value
            of the criterion function.
            For structural balance, the existence of a globally minimized partition is guaranteed, regardless of the
            size of the network. We use k
            to denote the number of subsets in a partition (clustering) C where
            1 ≤ k ≤ n and n denotes the number of vertices in the network. A
            partition with (k + 1) clusters is an adjacent partition. Doreian et al.
            (2005, p. 305) provide:
            Theorem 3. For any signed network (G, ), there will be a unique
            lowest value of the criterion function, denoted by P(Cmin ), that occurs
            for partitions with k subsets or adjacent partitions.17
            Note that a unique minimum value, P(Cmin ), does not imply that
            there is one unique partition with this optimal value. Theorems 1
            and 2 formed a major development for the formulation of structural balance and Theorem 3 provides a clear
            statement about the
            behavior of the criterion function. Brusco et al. (2011) showed
            that, for networks where n ≤ 40, the local optimization procedure locates the optimal partition(s) of signed
            one-mode networks.
            However, for relaxed structural balance, while the criterion function is deﬁned in the same fashion, its
            behavior as the number of
            clusters, k, is increased is different. Denoting the value of the optimal value of the criterion function
            for partitions with k clusters by
            P(Ck ), Doreian and Mrvar (2009) prove:
            Theorem 4. For establishing optimal partitions using the relaxed
            structural balance blockmodel, the values of P(Ck ) decline monotonically as k increases.
            The nice behavior of the criterion function for structural balance
            (Theorem 3) is lost when relaxed structural balance is considered
            (unless an empirical network conforms to, or closely approximates,
            structural balance).18 The declining monotonic property of the

            15

            At a minimum, we recommend using many thousands of repetitions.
            There are two meanings for this term. One is the best that was found by using the
            local optimization method (thus far). A more general meaning for optimal partition
            is for the globally optimal partition(s)—which might not be obtained in a speciﬁc
            analysis. All theorems regarding optimal partitions apply for the latter meaning of
            this term.
            17
            The result is intuitively reasonable. If k = 1, then every negative tie is an inconsistency and, at the
            other extreme, if k = n, then every positive tie is an inconsistency. As
            k increases from 1, in general, P(C) decreases monotonically until (P(Cmin )) is reached
            and as k is increased further then the value of P(C) increases. However, it is possible
            that for adjacent partitions the value of P(C) remains at the optimal value and the
            plot of P(C) against k is ﬂat for these values before increasing as k is increased.
            18
            Given a partition whose criterion function is optimal value is 0 under structural
            balance; increases in the value of k, under relaxed structural balance will produce
            more ﬁne grained partitions with the same value of the criterion function. If the
            16

            criterion function for relaxed structural balance for signed onemode networks holds also for signed two-mode
            networks. Mrvar
            and Doreian (2009) prove:
            Theorem 5. Given a signed two-mode network G = (U, V, E) and a
            set of optimal partitions for the (k1 , k2 )-partitions of G, with 1 ≤ k1 ≤ n1
            and with 1 ≤ k2 ≤ n2 , the optimal values of P(C) decline monotonically
            with k1 for each value of k2 and monotonically with k2 for each value
            of k1 .
            While relaxed structural balance, as a generalization of structural balance, has useful properties that
            include being the
            foundation of a method for partitioning signed two-mode networks, the behavior of the criterion function in
            relation to k1 and
            k2 , as described in Theorem 5, ushers in some serious problems.
            These problems are the methodological focus of this paper. To state
            the ﬁrst problem, we deﬁne the grain of a partition in terms of the
            number of clusters in a partition (for both one-mode and two-mode
            networks). Loosely, partitions having many clusters are ﬁne-grained
            and partitions with few clusters are coarse-grained. The minimum
            value of the criterion function for relaxed structural balance is 0
            and this value must occur when every vertex is a singleton in a
            cluster. This is the most ﬁne-grained partition possible but is useless as a blockmodel. Empirically, it is
            possible that this value is
            reached for extremely ﬁne grained partitions that also have little
            utility for blockmodeling. This implies that to establish a reasonable coarse-grained partition, some
            judgment is required.19 When
            the implicit 3D plot (of the criterion function against k1 and k2 )
            becomes ﬂatter it suggests that the coarsest-grained partition for
            this ﬂattened curve is an appropriate partition. Second, for signed
            networks that have the size of the UNGA voting arrays considered
            here, the guarantee offered by the results of Brusco et al. (2011) no
            longer holds.20 It follows that the resulting partitions be subject to
            additional scrutiny in an effort to ensure a useful partition. In general, this involves some consideration
            of alternative partitions near
            a candidate partition. Finally, using the local optimization algorithm includes some searching over
            different values of k1 and k2 is
            computationally demanding when k1 and k2 are large.21 It follows
            that partitioning over large ranges of k1 and k2 would be extremely
            time consuming. Indirect blockmodeling may be a way of seeking
            guidance as to where to focus attention in terms of k1 and k2 .
            4.2. Indirect signed blockmodeling
            Doreian et al. (2005) distinguish direct blockmodeling, as
            described above, from indirect blockmodeling where summaries
            of the data, in the form of (dis)similarity measures constructed
            from a network, are used to partition the network data with some
            standard clustering method (Doreian et al., 2005, pp. 177–84).

            optimized value of the criterion function under structural balance is not 0 then,
            under relaxed structural balance, the value of P(C) will decline monotonically, consistent with Theorem 4.
            19
            The same property holds for structural equivalence although this is seldom
            recognized.
            20
            For the smaller networks that Brusco et al. (2011) considered, the results of
            using the local optimization algorithm were compared with those obtained from
            an exact (branch-and-bound) algorithm guaranteed to identify the optimal partitions. In all of the networks
            they considered, the returned partitions from both
            algorithms were identical. Alas, the exact algorithm is completely impractical for
            large networks and such comparisons cannot be made. While this suggests another
            deﬁnition of ‘large’—a signed network is large (n > 40 for one-mode networks) when
            this guarantee is no longer available—we think that this is overly conservative with
            ‘large’ being used for rather small networks.
            21
            For the P4 data with k1 = 7 and k2 = 7 100,000 repetitions, using an HP HDX18
            Notebook PC with an Intel Core 2 CPU Q9000 running at 2.00 GHz and with installed
            memory of 4.96 GB, a single run took about 4 h and 40 min. Seeking more efﬁcient
            methods seems merited provided that the resulting partitions are the best possible
            partitions.

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            Doreian et al. (1994) compared the direct and indirect approaches
            for some well known small networks. They report that, most of
            the time, the direct approach produced better ﬁtting partitions,
            in terms of the values of the criterion function reported for each
            method, than those obtained by the indirect approach. Further,
            the indirect approach never outperformed the direct approach for
            these small networks. However, the direct blockmodeling approach
            can be extremely burdensome computationally and this problem
            becomes acute as network sizes increases. Given that the signed
            UNGA voting data set that we use is much larger than the signed
            networks considered hitherto, it seems prudent to consider indirect
            signed blockmodeling—blockmodeling signed data using indirect
            methods—as well.
            The data described in Section 3 are very close to being complete
            (having zero or near-zero null ties). One faster indirect way of partitioning the rows and columns is to
            partition them separately and
            then fuse the two partitions to form a joint partition of the twomode signed network. For the rows, we
            computed the Euclidean
            distance22 for the vectors of each row. Letting gik denote the (i, k)
            element of G, the Euclidean
            distance between ui and uj , d(ui , uj ), is
            obtained from d2 (ui , uj ) = (gik – gjk )2 and the Euclidean
             distance
            between vi and vj , d(vi , vj ), is obtained from d2 (vi , uj ) = (gki – gkj )2
            in separate computations. These were used as input for a standard
            hierarchical clustering program where we used the Ward clustering
            method (Ward, 1963). The two hierarchical clustering algorithms
            each produce a dendrogram that can be visually inspected to determine the number of clusters (k1 for rows
            and k2 for columns). Of
            course, there is ﬂexibility here and this introduces another element
            of judgment.
            4.3. Comparing partitions
            Our primary goal is to establish blockmodels of signed twomode data that are meaningful and well deﬁned.
            This implies that
            comparisons of partitions have to be made. These comparisons arise
            in three ways. First, given the establishment of a blockmodeling
            partition, it is necessary to establish that it is better than a random partition with the same numbers of
            clusters. Second, given the
            result of Theorem 5, and the implication that an element of judgment is needed to choose from among ﬁtted
            blockmodels (with
            different values of k1 and k2 ). Given a move from a coarse-grained
            partition to a more ﬁne-grained partition—where Theorem 5 points
            to a smaller value of the criterion function for the ﬁner-grained
            partition—it is necessary to check that the difference is large enough
            to justify the move. Third, as we do use other partitioning methods, we need a way of comparing partitions
            obtained from using
            different methods.
            Direct blockmodeling has a trio of criteria for evaluating partitions: (i) the form of the blockmodel in
            terms of its block structure;
            (ii) the agreement (or not) of the composition of the clusters; and
            (iii) the value of the criterion function implied by each partition.
            The blockmodeling approach is designed to use all of the signed
            two-mode data directly and does not use a low(er) dimensional
            representation. The expectation is that blockmodeling will produce better partitions in the sense of: (i)
            having a lower value of
            the criterion function, P(C); (ii) will have a cleaner block structure (more blocks in their correct
            location); and (iii) will have
            more coherent—internally, more consistent—blocks in the returned
            blockmodel. However, given the problems outlined in Section 4.1,
            there is no guarantee that this will be the case. The size of the data
            sets and the computational burden that they imply for the blockmodeling approach can mean that the optimal
            partition(s) are not

            22
            Because the data are so close to being complete, Euclidean distance can be used
            even though for most incomplete signed data sets it cannot be used fruitfully.

            185

            reached. The value of P(C) for all of the partitions that we report is
            one criterion for evaluating the reported partitions. The smaller the
            value of P(C), the better is the partition with that value. A second
            criterion measures the consistency of the composition of the clusters in a partition. The gold standard for
            this is the Adjusted Rand
            Index (ARI): see Hubert and Arabie, 1985; Saporta and Youness,
            2002; Warrens, 2008; and Santos and Embrechts, 2009. Here we
            use the ARI to measure the correspondence of two partitions of the
            rows and columns of a partition of a two-mode structure.
            Steinley (2004), based on an extended simulation study, has provided a set of guidelines for assessing the
            correspondence (or not)
            of two partitions of a set of vertices. For ARI ≥ 0.9, the correspondence between partitions is deemed
            excellent. When 0.9 > ARI > 0.8,
            the correspondence is said to be acceptable. For this range of the values of ARI, the memberships of the two
            partitions are deemed to
            be close enough to be considered the same. For lower values of ARI
            (ARI ≤ 0.8) the correspondence of the two partitions is unacceptable.
            The blockmodeling approach starts with a random partition of
            the vertices in a two-mode network and proceeds from there. An
            obvious question for any established partition is simple to state:
            does it differ from a random partition of the vertices? In Pajek, a
            random partition of the vertices into k1 clusters of rows and k2
            clusters of columns produces k1 clusters of about the same size
            (n1 /k1 ) of the rows and k2 clusters of columns of about the same
            size (n2 /k2 ) of the columns. The ARI can be used to measure the
            difference in the composition of the (starting) clusters and the ones
            established by blockmodeling. In general, the sizes of row clusters
            for the optimal partitions differ from the sizes of the random row
            clusters, as is the case for columns. Another measure of departures
            from randomness is to use randomly generated clusters having the
            same sizes as were established for the optimal partition.23
            In the blockmodeling procedure, a value of the criterion function for the initial random partition is
            computed. We denote this
            value by P(Cr ) and the value of the optimal criterion function by
            P(Co ). A simple measure of the difference between these two values of the criterion function is a
            proportional reduction of error
            measure: PRE = (P(Cr ) − P(Co ))/P(Cr ). Following experiments with
            random networks, values of PRE ≥ 0.2 indicate departures from randomness and are therefore noteworthy. When
            viable partitions
            were obtained using other methods either ARI or P(Cr ) are used
            as evaluative criteria.
            5. Results
            Here, we present our blockmodeling results and stress two features of this approach. One is that
            blockmodeling is a full information
            approach in the sense that all of the data are analyzed all of the time.
            No data reduction occurs when partitioning because the method
            uses no summaries of the data. Second, as used here, the focus is
            placed on the data being two-mode data. Both clusters of states and
            clusters of resolutions are important: the two partitions (of states
            and resolutions) get their full coherence when they are considered
            together.
            5.1. Time 1: Late cold war period
            5.1.1. Partitioning the full array of states and resolutions
            Fitting blockmodels to a network array can be done within
            two distinct strategies. One is purely inductive and the other uses
            pre-speciﬁcation. An inductive speciﬁcation for signed two-mode

            23
            For example, if there are 100 vertices and a random partition is created, the
            sizes of the clusters are 34, 33 and 33. But if the blockmodeling partition returned
            clusters of sizes 60, 30 and 10, this alternative value of ARI is computed with a
            randomly created partition into clusters with sizes of 60, 30 and 10.

            186

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            networks states only that there will be positive or negative blocks.
            In contrast, pre-speciﬁcation can be used when analysts possess substantive or empirical knowledge (Doreian
            et al., 2005, pp.
            233–5) regarding the network to which blockmodels can be ﬁtted.
            Using pre-speciﬁcation requires, ahead of analysis, the statement of a partial or complete blockmodel. A
            complete blockmodel
            gives the distribution of all block types in their locations within
            the blockmodel. Although the term model based is often used to
            describe only statistical modeling where one or more equations or
            some probability structure are used, blockmodeling is model based
            when a complete blockmodel is pre-speciﬁed. The nature of UNGA
            voting dictates that some form of pre-speciﬁcation be used. The
            reason is simple. For Time 1 there are 30,468 positive votes and
            8,448 negative votes implying that the proportion of positive votes
            is 0.783. The inductive use of blockmodeling (with only P and N
            blocks speciﬁed) leads to a blockmodel with only P blocks.
            Of course, the form of the actual blockmodel becomes important and formulating one ahead of time is
            difﬁcult. We know that
            some states tend to vote in support of the majority of resolutions
            while no states vote against all resolutions. Other states vote in
            ways that span a range between these extremes. This leads to a
            simple speciﬁcation that takes the following form when expressed
            in block types:
            S1
            S2
            S3
            S4
            S5
            S6
            S7

            R1

            R2

            R3

            R4

            R5

            R6

            R7

            P
            P
            P
            P
            P
            P
            P

            P
            P
            P
            P
            P
            P
            N

            P
            P
            P
            P
            P
            N
            N

            P
            P
            P
            P
            N
            N
            N

            P
            P
            P
            N
            N
            N
            N

            P
            P
            N
            N
            N
            N
            N

            P
            N
            N
            N
            N
            N
            N

            We call this a generic blockmodel form regardless of the number
            of clusters. To simplify notation, we write k = k1 = k2 . In the above
            illustration, k = 7. The ﬁrst row of positive (P) blocks is for states
            in cluster S1 that tended to support all resolutions. In contrast,
            the row corresponding to cluster S7 is for states voting against all
            resolutions except those in cluster R1 . The remaining rows have systematic differences regarding the number
            of resolutions that are
            supported and opposed by the states in each of the row clusters.
            Similarly, resolutions differ systematically in the ways that states
            support or oppose them.
            The results from using the generic pre-speciﬁed blockmodel are
            shown in Table 1A. Six variants of the generic pre-speciﬁcation are
            shown. The number of partitions, the values of the criterion function, and the values of the ﬁt indices (PRE
            and ARI) are shown. The
            decline in the values of the criterion function is consistent with
            Theorem 5 and the declines show diminishing returns from increasing the number of clusters.24 Also, the
            behavior of PRE and ARI make
            it clear that the obtained partitions are far from random partitions.
            All looks good with this information until we compare the values
            of the criterion function for the indirect approach using Euclidean
            distance whose implied criterion function values are shown in the
            last column. In the main, the performance using Euclidean distance
            is poor. Not only are the implied values of the criterion function
            higher, the blockmodel structures have duplicate rows and columns
            of block types which imply that there are many other partitions
            with equally well ﬁtting blockmodels. The surprise comes with the
            (7, 7) partition where the criterion function is lower than for using
            the generic blockmodel.
            Table 1B shows the implied blockmodel for the Euclidean (7, 7)
            partition where, in contrast to the generic blockmodel, there are
            some P blocks among the N blocks in the lower right of the array

            24
            The values of the criterion function for the (8, 8), (9, 9) and (10, 10) partitions
            are 2148, 2137.5 and 2124 respectively.

            of blocks. The generic blockmodel was quite good but not good
            enough for the Time 1 voting array. As a result, we used it as an
            empirically based pre-speciﬁed blockmodel and ﬁtted that to the
            data. The results are shown in Table 1C. The partition is unique
            and the criterion function has dropped even further to 1881.5 and,
            again, the PRE and ARI indices conform that the partition is far
            from random. Further, the value of the ARI when this partition is
            compared with the partition from the generic blockmodel is 0.552
            indicating that the two are different partitions.
            We show the partition of the full set of states and military resolutions for Time 1 in Appendix C (Fig. C1)
            to demonstrate how the
            empirically informed pre-speciﬁcation in Table 1 is used. It shows the
            nature of the partition even though the labels for states and resolutions cannot be read. The black squares
            represent positive votes
            and the red diamonds represent negative votes. There is a clear
            patterning where there are concentrations of ties of a type within
            blocks. Table 2 gives the partition of the states25 and Table C1 gives
            the partition of resolutions. In short, this gives the ‘big picture’ of
            key voting clusters. This overall partition provides the departure
            point for focusing on parts within the big picture.
            We note that the clusters of states labeled S5 and S6 feature
            primarily the Industrialized (and mainly Western) states. Most
            members of NATO are in S6 including the US. The cluster labeled
            S7 has the (former) Soviet Union, members of the Warsaw Pact and
            other leftist/communist states. The remaining clusters have the rest
            of the world’s states that are in the dataset. The clusters of resolutions reported in Table 3 are labeled
            R1 through R7 . The states in S6
            solidly oppose the resolutions in R3 , R4 , R6 , and R7 . The four states
            in S5 have the same pattern except for tending to support the resolutions in R3 . The states in S7 and S6
            tend to vote in support of
            resolutions in R1 —as do all states—and against those in R6 but for
            resolutions on all of the other ﬁve clusters of resolutions their voting is diametrically opposed. The
            states in S4 tend to support all
            resolutions except those in R5 (although the resolutions in R2 draw
            a mixed response from this set of states). States in S2 tend to oppose
            resolutions in R7 and support those in the remaining clusters while
            states in S1 tend to support all resolutions.
            In addition to obtaining clusters of states as potential blocs (not
            blocks), and the clusters of the resolutions, this two-mode partition opens the way to examining
            simultaneously the states and
            resolutions to facilitate the exploration of the exact nature of the
            resolutions that distinguished the states that oppose or support
            them.26
            We noted earlier that 78% of the votes cast at Time 1 were positive. Blocks of a signed blockmodel are
            distinguished as positive
            or null blocks. Well ﬁtting blockmodels ought to have blocks that
            are ﬁlled with ties having the same sign: positive blocks have primarily positive ties and negative blocks
            primarily have negative
            ties. Table 3 presents the block densities of the ties consistent with
            the block type for each block together with a label for its sign. The
            higher the density of the correctly signed ties in a block, the fewer
            inconsistencies this block contributes to the criterion function. For
            a well ﬁtting blockmodel, positive blocks must have densities well
            above 0.78 and negative blocks must have densities well above 0.22.
            There are 28 (bolded) blocks having densities above 0.9. Of these,
            20 of them are positive blocks and 8 are negative blocks. They provide evidence that the blockmodel
            partition in Fig. C1 has created
            many blocks with ties largely of the same sign. The 20 P blocks have

            25
            The partitioning returned Papua New Guinea as a singleton in a cluster. We do
            not take this seriously as a real cluster and treat it as not belonging to any potential
            voting bloc. However, we take some comfort in noting that the methods we use did
            distinguish it from all of the identiﬁed voting blocs.
            26
            This is done elsewhere because the focus here is on the methods that facilitate
            such comparisons.

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            187

            Table 1
            Summary of the blockmodeling partitioning for Time 1.
            Partition

            Number of partitions

            Value of criterion function

            (A) Summary output and ﬁt indices for Time 1
            1
            (2, 2)
            (3, 3)
            1
            (4, 4)
            1
            (5, 5)
            4
            4
            (6, 6)
            2
            (7, 7)
            R1
            (B) Inductively established blockmodel
            S1
            P
            S2
            P
            S3
            P
            S4
            P
            S5
            P
            S6
            P
            S7
            P
            Partition

            Fit indices

            2743.5
            2452.0
            2353.0
            2262.5
            2197.5
            2164.0

            Euclidean criterion function values

            PRE

            ARI

            0.611
            0.686
            0.717
            0.734
            0.762
            0.753

            0.344
            0.204
            0.734
            0.111
            0.096
            0.080

            2934.0
            2648.0
            2353.0
            2284.0
            2284.0
            2117.5

            R2

            R3

            R4

            R5

            R6

            R7

            P
            P
            P
            P
            P
            P
            N

            P
            P
            P
            P
            P
            N
            P

            P
            P
            P
            P
            N
            N
            P

            P
            P
            P
            N
            P
            P
            N

            P
            P
            N
            P
            N
            N
            N

            P
            N
            N
            P
            N
            N
            P

            Number of partitions

            (C) The ﬁnal ﬁtted blockmodel for Time 1
            1
            (7, 7)

            Value of criterion function

            Fit indices

            1881.5

            densities well above the overall density of 0.78 for positive ties and
            the 8 N blocks are well above the overall density of 0.22 for negative ties. The concentration of negative
            ties primarily into only 8 N
            blocks is the most interesting because they show joint opposition to
            resolutions supported by most other states.
            While Fig. 2 presents the big picture of alliances, blockmodeling also permits “drilling down” to examine
            what issues cause
            particular states to depart from the voting patterns of other bloc
            members. This examination could be driven by an interest in particular blocs, interest in speciﬁc states, or
            voting cohesion of regional,

            PRE

            ARI

            0.748

            0.456

            international, or cross-regional organizations such as the EU, NATO,
            or the OIC.
            5.1.2. Some detailed further partitions
            One notable feature of Table 3 is that six of the blocks for
            the Leftist or Communist states of the era (in the cluster labeled
            S7 in Table 3) have block densities above 0.9 (and the seventh
            has a density of 0.869) showing highly consistent bloc voting.
            Indeed, these densities point to this bloc has having the most consistently coherent voting pattern
            illustrative of Cold War power

            Table 2
            Seven clusters of states from Fig. 2.
            S1
            Bahrain
            Bangladesh
            Barbados
            Bolivia
            Botswana
            Burundi
            Cameroon
            CenAfrRep
            Chad
            Djibouti
            Ecuador
            Egypt
            Gabon
            Ghana
            Guinea
            Guyana
            Iran
            Iraq
            Jordan
            Kenya
            Kuwait
            Lebanon
            Lesotho
            Malaysia
            Maldives
            Mali

            Malta
            Mauritania
            Mauritius
            Morocco
            Niger
            Nigeria
            Oman
            Pakistan
            Panama
            Peru
            Qatar
            Romania
            Rwanda
            SaudiArabia
            Senegal
            SierraLeone
            Somalia
            SriLanka
            Sudan
            Thailand
            Togo
            TrinidadTobago
            Tunisia
            UAE
            URTanzania
            Venezuela
            Zambia

            S2

            S4

            S5

            S6

            S7

            Bahamas
            BurmaMyanmar
            Chile
            China
            Colombia
            CostaRica
            CotedIvoire
            DKCambodia
            DominicanRep
            ElSalvador
            Fiji
            Guatemala
            Haiti
            Honduras
            Jamaica
            Liberia
            Malawi
            Nepal
            Paraguay
            Philippines
            Singapore
            StLucia
            Suriname
            Uruguay
            ZaireDRC

            Algeria
            Angola
            Argentina
            Benin
            Bhutan
            Brazil
            CapeVerde
            Congo
            Cyprus
            Ethiopia
            GuineaBissau
            Indonesia
            LibyanAJ
            Madagascar
            Mexico
            Nicaragua
            SaoTomePrinc
            SyrianArabRe
            UVBurkinoFas
            Uganda
            Yemen
            Yugoslavia

            Austria
            Finland
            Greece
            Ireland
            Sweden

            Australia
            Belgium
            Canada
            Denmark
            France
            GerFedRep
            Iceland
            Israel
            Italy
            Japan
            Luxembourg
            Netherlands
            NewZealand
            Norway
            Portugal
            Spain
            Turkey
            UK
            US

            Afghanistan
            Bulgaria
            ByeloBelarus
            Cuba
            Czech
            DemYemen
            GerDemRep
            Hungary
            India
            Laos
            Mongolia
            Mozambique
            Poland
            USSRussianFe
            Ukraine
            Vietnam

            Note: A singleton cluster S3 (with Papua New Guinea) was identiﬁed but excluded from this and subsequent
            tables.

            188

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            Table 3
            Block densities for the 42 blocks in Fig. 2.

            S1
            S2
            S4
            S5
            S6
            S7

            R1

            R2

            R3

            R4

            R5

            P
            0.958
            P
            0.898
            P
            0.931
            P
            0.983
            P
            0.848
            P
            0.951

            P
            0.929
            P
            0.918
            P
            0.778
            P
            0.992
            P
            0.905
            N
            0.913

            P
            0.961
            P
            0.845
            P
            0.966
            P
            0.904
            N
            0.856
            P
            0.986

            P
            0.932
            P
            0.789
            P
            0.954
            N
            0.833
            N
            0.952
            P
            0.992

            P
            0.838
            P
            0.863
            N
            0.770
            P
            0.789
            P
            0.889
            N
            0.977

            The singleton cluster (S3 ) was removed but the labels for Clusters S4 , S5 , S6 and S7 were retained.
            Bolded densities are all above 0.9.

            Fig. 2. Negative blocks for communist and leftist states voting against resolutions for Time 1.

            R6

            R7

            P
            0.947
            P
            0.836
            P
            0.914
            N
            0.680
            N
            0.952
            N
            0.869

            P
            0.769
            N
            0.761
            P
            0.868
            N
            0.790
            N
            0.955
            P
            0.982

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            189

            Fig. 3. Details Fig. 4 negative voting blocks for the western and industrial states for Time 1.

            struggles. Although the Western and/or Industrial states generally
            vote against military resolutions, particularly those that impact
            their military power, there are resolutions that the Communist
            bloc also votes against in consistent fashion. This is shown in
            Fig. 3 for the three negative blocks for these states. The ﬁrst two
            blocks in Fig. 2 are reproduced as is from Fig. C1, but the third
            has been reﬁned slightly (via blockmodeling) to show that both
            India and the Democratic Republic of the Yemen are slightly different in their detailed voting patterns. The
            resolutions identifying

            this block deﬁne the issues where these two states depart from
            the other members of the larger bloc. These two states tended
            to support issues related to disarmament and development and
            complete disarmament. We include this to show that, if needed
            and relevant, more detailed selective partitioning is possible to
            identify both the clusters of states and resolutions for further
            study, including any implications for balancing processes involving military resolutions as the soft
            balancing counterpart to hard
            balancing.

            190

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            Fig. 4. Partition of one bloc of states from the non-western states and resolutions in R7.

            Fig. 3 presents some corresponding formatted arrays for the
            Western, NATO and/or Industrial states for three of the clusters
            of resolutions that they vote against as a voting bloc. In the big
            picture of Fig. C1, these are simply negative blocks of votes illustrating voting patterns typical of Cold
            War voting where alliances
            oppose one another on military policy issues. As noted above, the
            majority of Western states oppose the majority of military resolutions (particularly those on disarmament—a
            direct threat to their

            military power). Yet the Western bloc—including NATO memberstates—are divided on several resolutions,
            illustrating that states
            voted for other reasons than standing solidly with alliance members. For example, the block of resolutions
            deﬁned by the cluster
            R3 shows a subset of 23 resolutions for which 5 states (Australia,
            Denmark, Iceland, Norway and Spain—all but Australia are NATO
            member states) tend to support. In the second panel of Fig. 3, there
            are 7 resolutions supported by Portugal, Spain and Turkey that

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            other members of the EU and NATO vote against. And in the ﬁnal
            panel of Fig. 4, there are 17 resolutions where Turkey votes in opposition to the majority of Western
            states. The resolutions involving
            these positive votes by Turkey reﬂect a conﬂict between NATO and
            OIC interests. In each of these three examples, the joint partition
            of states and resolutions provide important information on voting
            (dis)similarity. We expected on the whole during this period to ﬁnd
            voting deﬁned largely by the two Cold War military alliances—and
            this was conﬁrmed. We also found that some issues divided alliance
            members and the full information provided by our approach allows
            the interested researcher to explore the factors that lead to this
            divide.
            Examining the top row of Table 3, for a cluster of non-aligned
            member states suggests that the block deﬁned by S1 and R7 (with a
            positive tie density of 0.769) has enough negative ties (467 of them)
            that merit further attention. The blockmodel in Fig. 4 shows that
            there is some pattern to these ties (PRE = 0.626 and ARI = 0.162) in
            the sense of there being clear N blocks in an otherwise large P block
            for a coarser-grained partition. Again, the clusters of resolutions
            and states can be examined further to determine what links these
            states to oppose resolutions together that are supported by most
            other non-aligned member states.
            5.1.3. Balancing of power revealed
            As we noted earlier, the blockmodeling provides full information (on states and resolutions) which provides
            a basis for further
            exploration, including locating clear balancing processes among rival
            alliances. Fig. 6 provides an example of doing this. We look more
            closely at the following clusters: S6 (the primary Western bloc containing most of the NATO members) and S7
            (contains Warsaw Pact
            Soviet bloc members and allies) from Table 2 and R7 (the resolutions that the Western bloc opposes and the
            Soviet bloc supports)
            from Table 3. These are highlighted in the lower right part of the
            overall partitioned array in Fig. C1. Denoting this two-mode subnetwork by G, we multiply it by its
            transpose to obtain GGT which
            we depict as a valued and signed one-mode network27 for states.
            This is drawn in the top panel of Fig. 5 with the S6 (Western, largely
            NATO member) states depicted by circles on the right and S7 (Soviet
            bloc and allied) states depicted as squares in the left—a clear reﬂection of the Cold War political divide
            that was present at Time 1. All
            but one of the positive ties are within these two clusters of states.
            The only exception is a positive tie between India and Turkey. The
            lower panel of Fig. 6 shows how this positive tie was generated
            (e.g., the pattern of agreement on some resolutions).
            Turkey votes with the ‘NATO bloc’ on most issues (25 of 39 resolutions); but diverges on a number of others
            for this subset of
            resolutions.28 Similarly, India votes largely in alignment with Soviet
            bloc and allied states (29 of 39 resolutions).29 India and Turkey vote
            identically on the 24 (14 for Turkey and 10 for India) resolutions for
            which they vote differently than their respective bloc members. An
            advantage of blockmodeling is the ability to identify speciﬁc resolutions which both identify bloc voting
            and the issues that divide
            bloc members. Once identiﬁed, the content of the resolutions and
            the temporary coalitions supporting them can be explored to pinpoint the reasons for the divergence and to
            identify issues that can
            potentially fracture alliances.

            27
            We do not use the values in drawing Fig. 6 but they are useful for subsequent
            analyses.
            28
            Turkey votes differently from its NATO and Western allies on the following
            14 resolutions: 40/6, 40/93, 40/96D, 40/168A, 40/168B, 39/14, 38/69, 38/180A,
            38/180D, 38/180E, 37/82, 36/27, 36/98 and 36/226A.
            29
            India votes differently from Soviet and allied states on the following 10 resolutions: 40/18, 40/85, 40/92A,
            40/94I, 39/57, 39/65B, 39/151I, 38/187A, 38/188F and
            36/92J.

            191

            5.2. Time 2: Post cold war period
            Fig. 6 contains the detailed unique (7, 7) partition for Time 2
            that was obtained using the generic pre-speciﬁed blockmodel, and
            Table 4 lists the seven clusters of states. The pattern is similar to
            that of the Cold War period in identifying clear blocks of states
            that voted highly similar (as well as the divergences from bloc voting). It is also similar in that the
            Western or developed states are
            more likely to vote against military resolutions in the UNGA and to
            vote as a block. There are a number of important differences, however. First, S6 is now composed of both
            Western European states
            and newly independent Eastern and Central European states that
            formed after the dissolution of the Soviet Union and Yugoslavia.
            Many of these states have now been incorporated into both the EU
            and NATO. Interestingly, we also see a distinct voting pattern of the
            US, which votes more independently from the rest of the developed
            and Western states and quite similarly with Israel (in S7 ). In some
            ways, the blocks reﬂect Huntington’s “clash of civilizations” thesis
            in that there is a clearly expanded group of largely Western states
            that vote quite distinctly from “the rest” on these military issues.
            Blockmodeling allows further drilling down to identify the distinct cluster of resolutions that divide these
            Western/developed
            states from each other and from the rest of UN member-states. This
            feature is useful for identifying the issues for which NATO members
            are united and those which may suggest some balancing between
            the US (as the sole military hegemon) and EU-members states. A
            more in-depth exploration of this balancing process is beyond the
            scope of this paper but we have provided the tools by which this
            can be pursued.
            Table 5 contains the summary of ﬁtting this pre-speciﬁed blockmodel. As was the case for Time 1, the
            criterion function drops in a
            fashion consistent with Theorem 5. The ﬁt statistics show that each
            obtained partition for the grains shown in the table is far from being
            random: these blocks of voting ties represent real differences in the
            patterns of voting by states within clusters. Again, there are diminishing returns for increasing the number
            of clusters and further
            partitioning of selected blocks is a better strategy than increasing
            the number of clusters for the full partitioning. In contrast to Time
            1, the indirect blockmodels perform poorly for all grains (2 ≤ k ≤ 7).
            The values of the criterion function are higher and there are identical rows and columns of block types.30
            The computed value for
            the second ARI is 0.118 for the two (7, 7) partitions, an extremely
            low value indicating that the indirect and direct (7, 7) partitions are
            very different.
            5.3. Partial summary
            A theoretical goal of our paper was to demonstrate the utility of
            the relaxed structural balance approach to the analysis of UNGA
            roll call votes, in potentially coupling balance of power theory
            in international relations with Heider’s structural balance theory.
            Both theories share an assumption of a tendency towards balance
            among multiple actors resulting in an alliance formation. International relations scholars have proposed the
            idea of “soft power”
            balancing through norms, and adopted resolutions represent a consensus on norms underlying particular policy
            recommendations
            regarding issues of interest to the international community. Voting
            on these military resolutions provide information on the coalitions
            that support or oppose particular norms about the use of military
            power by UN member-states. Examining the roll call votes on military issues at two time points spanning the
            Cold War political

            30
            This helps account for the criterion function being unchanged for the (3, 3), (4,
            4), and (5, 5) partitions because splitting a block into blocks with the same sign will
            not change the value of the criterion function.

            192

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            Fig. 5. The one-mode network for S6 and S7 based on R7 and the two-mode network for S6 , S7 and R7 .

            ideological divide illustrated changing coalitions after a major
            shock to the international system. The overall structure of state
            relations impacted the relations among coalition members and the
            balance of power among them. As might be expected, the US and EU
            member states vote more similarly than other UN-member states
            overall, but also less similarly than they did during the Cold War
            when they shared a common enemy. We argue that our technique

            and results provide an important ﬁrst step in exploring the coupling
            of these two important theories, and encourage other scholars to
            continue in this direction.
            We also progressed in our methodological goals, namely, identiﬁcation of and solutions for problems with
            applying the relaxed
            structural balance approach to large signed two-mode data. We
            demonstrated that direct signed blockmodeling is very useful for

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            193

            Fig. 6. A partition of the full set of military resolutions for Time 2.

            partitioning larger signed two-mode networks, but we also identiﬁed several methodological issues that must
            be resolved. We
            suggested several solutions, and found that despite the problems highlighted, the results produced are
            coherent, and reﬂect
            divisions located in prior UNGA voting analyses for both time periods examined. However, despite having
            provided a substantively
            driven method that produces coherent and useful partitions of
            signed two-mode data, and illustrating how blockmodeling can
            identify smaller clusters that may be fruitful for examining the
            implications of temporary or more permanent coalitions, several problems remain. First, when the data arrays
            are large, the

            partitions obtained by using direct blockmodeling methods
            required long computational times, which is problematic in two
            ways: the length of time involved, and the difﬁculty with obtaining
            optimal partitions if either the time for the partitioning is reduced
            or the size of the two-mode array is raised too much. Second, we
            were surprised to ﬁnd that the result for the (indirect) Euclidean
            distance (7, 7) partition was superior to the direct partition for the
            Time 1. This was a salutary reminder that the direct blockmodeling
            method need not always locate the best partition. In this instance,
            using indirect blockmodeling had great value in not only providing a better ﬁtting partition but also in
            providing a pre-speciﬁed

            194

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            Table 4
            Seven Clusters of States corresponding to Fig. 6.
            S1 (N = 35)

            S2 (N = 28)

            Algeria
            Angola
            Barbados
            Benin
            Bhutan
            Bolivia
            BruneiDar
            BurmaMyanmar
            CapeVerde
            China
            Cuba
            Egypt
            EquatorialGuinea
            Fiji
            Guinea
            Haiti
            Indonesia
            Iran

            Lebanon
            LibyanAJ
            Malaysia
            Mexico
            Namibia
            Nepal
            Oman
            Pakistan
            PapuaNewGuinea
            StLucia
            Sudan
            Suriname
            Swaziland
            SyrianArabRep
            Vietnam
            Yemen
            Zimbabwe

            S3 (N = 29)

            AntiguaBarbuda
            Bangladesh
            Belize
            Botswana
            UVBurkinoFaso
            Cameroon
            Colombia
            CotedIvoire
            Ecuador
            Ethiopia
            Gabon
            Grenada
            Guyana
            India
            Jamaica
            Kenya
            Laos
            Madagascar

            S4 (N = 6)

            S5 (N = 9)

            S6 (N = 44)

            Brazil
            Paraguay
            Samoa
            SolomonIslands
            SouthAfrica
            Uruguay

            Australia
            Azerbaijan
            ByeloBelarus
            Georgia
            Japan
            Kazakhstan
            NewZealand
            Tajikistan
            Ukraine

            Albania
            Andorra
            Argentina
            Armenia
            Austria
            Belgium
            Bulgaria
            Canada
            Croatia
            Cyprus
            Czech
            Denmark
            Estonia
            Finland

            Mongolia
            Mozambique
            Senegal
            Singapore
            SriLanka
            Thailand
            Togo
            Uganda
            URTanzania
            Zambia

            Bahamas
            Bahrain
            Chile
            CostaRica
            Djibouti
            DominicanRep
            ElSalvador
            Eritrea
            Ghana
            Guatemala
            Honduras
            Jordan
            Kuwait
            Maldives
            Mali
            Mauritius
            Morocco
            Nicaragua

            Nigeria
            Panama
            Peru
            Philippines
            Qatar
            SaudiArabia
            SierraLeone
            TrinidadTobago
            Tunisia
            UAE
            Venezuela

            S7 (N = 2)
            France
            GerFedRep
            Greece
            Hungary
            Iceland
            Ireland
            Italy
            Latvia
            Liechtenstein
            Lithuania
            Luxembourg
            Malta
            Monaco
            Netherlands

            blockmodel as the start point for getting an even better ﬁtting
            partitions.
            It is possible that using methods having a very different rationale
            could be useful. Alternative methods also may provide different but
            complementary information to help understand bloc voting in the
            UNGA. For these reasons, we make a preliminary exploration of
            several alternative approaches. We do not claim to take a complete
            survey of alternative methods because they are too numerous and a
            single paper could not do justice to what they have to offer. Nevertheless, we think it instructional to
            compare our results with that of
            some alternative popular approaches. Not all of these will advance
            directly our theoretical interest but they may have the potential to
            do so.
            6. Alternative clustering approaches
            We consider brieﬂy alternative tools for analyzing these data
            that have been used in UNGA voting studies or were developed
            as different ways of partitioning or representing two-mode data.
            However, this not intended as a complete survey.

            Norway
            Poland
            Portugal
            RepKorea
            RepMoldova
            Romania
            USSRussianFed
            SanMarino
            Slovakia
            Slovenia
            Spain
            Sweden
            TFYRM
            Turkey

            UK
            Uzbekistan

            Israel
            US

            6.1. Geometric data analysis
            Spatial models of multivariate data produce graphical geometric displays that facilitate the exploration of
            multivariate structure.
            For example, principal component analysis (PCA) produces a lowdimensional summary of a high-dimensional
            Euclidean space,
            multidimensional scaling (MDS) constructs a low-dimensional
            Euclidean representation of dissimilarity data, and multiple correspondence analysis (MCA) constructs a
            low-dimensional Euclidean
            representation of categorical data. Gower and Hand (1995) and
            Gower et al. (2011) provide a uniﬁed treatment of these techniques
            within the rubric of geometric data analysis.
            Spatial models of voting data were introduced and popularized
            by Downs (1957) who applied Hotelling’s (1929) model of spatial competition. Such spatial models represent
            both voters and
            candidates as ideal points in a low-dimensional Euclidean space
            and posit that voters tend to prefer candidates who are closer
            over candidates who are further away. Mapping the ideal points
            exempliﬁes an approach called “unfolding” in the MDS literature.
            See Groenen (2005: Part III) for an introduction to the notion of

            Table 5
            Summary of full partitioning of all military resolutions for Time 2.
            Partition

            (2, 2)
            (3, 3)
            (4, 4)
            (5, 5)
            (6, 6)
            (7, 7)

            Number of partitions

            1
            1
            1
            1
            1
            1

            Value of the second ARI for the (7, 7)-partition is 0.118.

            Value of criterion function

            972.0
            932.0
            902.0
            873.5
            859.5
            848.0

            Fit indices

            Euclidean criterion function values

            PRE

            ARI

            0.732
            0.758
            0.778
            0.785
            0.807
            0.804

            0.362
            0.245
            0.164
            0.128
            0.102
            0.081

            972.0
            974.5
            974.5
            974.5
            947.0
            947.5

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            unfolding. Cahoon (1975) developed an early example of an unfolding method for voting data.
            For analysis of legislative voting, candidates are replaced by policy positions—for example, Yea or Nay on a
            resolution. An unfolding
            analysis of legislative voting data represents each legislative representative (voter) and each policy
            position as an ideal point in a
            low-dimensional Euclidean space. The premise is that a voter will
            tend to vote Yea if s/he is closer to the ideal point for Yea than s/he
            is to the ideal point for Nay. We adopt this approach here.
            Not all ideal-point spatial models of legislative voting assign an
            ideal point to each policy position. The popular NOMINATE procedure of Poole and Rosenthal, described in
            detail by Poole (2005),
            assigns an ideal point to Yea and posits that voters will to tend to
            vote Yea if they are sufﬁciently close to the ideal point and Nay if
            they are sufﬁciently far from it. Erik Voeten (2000) introduces this
            method in his analysis of UNGA voting.
            For this paper, we used the R package HOMALS (De Leeuw and
            Mair, 2007, 2009) to perform MCA. In the context of HOMALS,
            MCA is called a simple homogeneity analysis. See Giﬁ (1990)
            and Michailidis and de Leeuw (1998) for detailed descriptions of
            homogeneity analysis. Previous applications of MCA/homogeneity
            analysis to legislative voting data include Desposato’s (1997) study
            of party switching in Brazil and de Leeuw’s (2005) analysis of Senate
            voting patterns.31
            These methods permit the simultaneous spatial locations for, in
            this case, states and resolutions. Fig. 8 shows the results for our ﬁrst
            (late Cold War) period, using a two-dimensional space and plotting
            the coordinates estimated using the Homals method.32 For visual
            reasons we only display the locations of states. The main clusters
            are highly similar to the blockmodeling results for this time period.
            Although the resolutions are suppressed, Homals locates countries
            near the resolutions for which they vote yes. Cluster 1 contains
            members of the NATO alliance and their allies; cluster 4 (bottom
            right hand side of graph) contains the Soviet bloc (Warsaw Pact
            member-) states and their allies. A large group of leftist states is
            located just above this group (cluster 3). The large group of largely
            non-aligned member states (with China nearby) is in and around
            cluster 2. The classic horseshoe pattern positions countries most
            dissimilar from one another at opposite ends, with the Western
            bloc opposite the Soviet bloc countries on both dimensions. The
            non-aligned member states are between these two ideologically
            opposed blocs. Interestingly, the three Western states P5 (UNSC
            Permanent 5 members) are clustered more closely than the rest of
            the Western states (China and the Soviet Union are the other two
            P5 states) (Fig. 7).
            The two-dimensional Homals plot for Time 2 (Fig. 8) reveals a
            change in the voting structures beyond the addition of about 30
            new UN member-states. The graph indicates that most of the difference in voting is along the ﬁrst dimension
            where an expanded
            “West” (Cluster 1) containing many of the former Soviet Union
            bloc states—and representing an expanded NATO and EU—is distinct from “the rest” of the states. As in the
            blockmodeling analysis,
            the US and Israel are distinct from the other Western states on
            the second dimension. The ﬁve remaining communist states—Cuba,
            China, Laos, North Korea (omitted due to high absenteeism), and
            Vietnam—are located within or just below cluster 3 and distinct
            from the large group of developing states. Not surprisingly, the

            31
            We also used the R package WNOMINATE (Poole et al., 2011) to perform the
            NOMINATE procedure and compare the results from using it with results with our
            Homals output but do not provide the results here because of space constraints. The
            results are available upon request.
            32
            The NOMINATE results produced similar clusters to that of Homals; the plots
            look different because of how the ideal points are plotted but the correlation
            between clusters are high.

            195

            Fig. 7. Homals voting map, Time 1: 1981–1985.

            Fig. 8. Homals voting map, Time 2: 1996–2001.

            distinct pattern of Western states (including differences within this
            bloc), and the separation of states described as “counterhegemonic”
            (remaining communist states and those with which the US and
            other Western states have been in conﬂict)33 from the rest of the
            non-Western states was consistent across analyses.
            There is no exact way to fully compare the results for using
            different methods. However, we can compare some of the results
            from using other methods with our own in order to independently
            evaluate the blocs produced by blockmodeling. We found that the
            results across methods were highly similar providing some validation for the clusters we identiﬁed. One
            advantage of the geographic
            methods was the production of coordinates for both resolutions and
            states that can be used as variables in statistical models. However,
            voting bloc memberships as identiﬁed by blockmodeling also can
            be used as dummy variables in the same fashion as done in Snyder
            and Kick (1979).

            33

            See Voeten (2000) for similar ﬁndings using NOMINATE.

            196

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            6.2. Negative eigenvector centrality
            An approach that can be used to both identify cliques of states
            voting similarly, and to examine balancing behavior, is the negative eigenvector centrality approach created
            by Bonacich and Lloyd
            (2004).34 This approach illustrates competing alliances through the
            use of positive and negative ties. The clustering is highly similar
            to that found in the blockmodel results. The two approaches can
            be complementary in two ways. First, blockmodeling provides full
            information results which can be used to explore the key issues
            that deﬁne the divide among opposing groups of states. Second,
            the eigenvector centrality approach can be used to explore subsets
            of states. We provide an example using a subset of “European” (or
            Western) states.35
            Fig. 9 illustrates voting similarity at Time 1 among European and
            other Western states and clearly reﬂects what would be expected
            by balancing processes described in Theorems 1 and 2 above. Note
            that all of the positive ties (solid lines) are among countries within
            each cluster, and all of the negative ties are between the group of
            Western (and NATO member) states on the right and Warsaw Pact
            members and their allies on the left.
            Fig. 10 illustrates voting similarity in Time 2, the period after the
            Cold War has ended and states have re-aligned. The graph reveals
            ﬁrst, a much denser graph as there are a number of newly independent states; and second, a largely cohesive
            Europe (and “West”).
            It is not surprising that the incorporation of many former Soviet
            bloc Eastern European states into NATO and EU would result in the
            peripheral status of Russia, Belarus and former Soviet bloc Central
            Asian states. Blockmodeling can be explored to locate the key issues
            that keep these states distinct from the rest.
            6.3. Islands
            Similar to the Eigenvector Centrality approach, the islands
            technique (Zaveršnik and Batagelj, 2004) begins with the transformation of the data from a two-mode to a
            one-mode network.
            As noted in that section, we begin with a matrix G that denotes
            the array of ties for a signed two-mode network G = (U, V, E). The
            matrix GGT is a valued network whose vertices are the elements in
            U and the values on the ties are integers (that can be positive or
            negative). More formally, the network created by forming GGT is
            M = (U, Em , w) where Em is the set of edges and w maps these edges
            to the set of integers. A visual imagery for the concept of islands is
            to think of a network where the lines differ in height according to
            the values of the edges in Em . Imagine the whole network is covered
            by water in a tank. As the level of water drops, the highest valued
            edges appear ﬁrst (along with the vertices directly linked by them)
            above the level of the water. If the level drops further, more valued
            edges appear and they form one of more islands visible above the
            threshold.
            Let t be a threshold (the level of the water in the visual imagery).
            Let e denote an element of Em (e ε Em ) and let w(e) denote the value
            of an edge, e. A line cut of M = (U, Em ) is a subnetwork, M(t) = (U(t),
            Em (t)) where the elements of Em (t) are such that e ε Em (t) if and
            only if w(e) ≥ t and U(t) is the set of elements of U having some
            edges, e, incident at them where w(e) ≥ t. The vertices U(t) that are

            34
            Lloyd (2007) also developed the application of this approach to international
            relations data.
            35
            The UN has two regional classiﬁcations: one is deﬁned by the ECOSOC regional
            commissions and includes all of Europe plus the US, Canada, and Israel; the other
            is the regional caucusing group which separates Eastern and Western Europe and
            places countries in the latter category (WEOG—Western European and Other Group)
            along with the U.S., Canada, Israel, Australia, New Zealand, Denmark, Finland, and
            Norway. We chose to use the ECOSOC European category that included both Eastern
            and Western Europe.

            linked by valued edges in Em (t) form islands. Given a threshold t,
            there can be one or many islands that are determined solely by
            being connected by the edges of Em (t). Note that this criterion for
            island membership is stringent. The set of vertices is edge island if
            the corresponding induced subgraph is connected and there exists a
            spanning tree, such that the values of ties with exactly one endpoint
            in island are less than or equal to the values of ties of the tree in the
            island (see Zaveršnik and Batagelj, 2004 and De Nooy et al., 2011,
            pp. 124–32). Island detection is implemented in Pajek (Batagelj and
            Mrvar, 1998).
            In terms of the UNGA voting arrays, the values of the ties in GGT
            capture the net number of times that two states vote the same way
            over the set of resolutions in the array. The higher the values in GGT ,
            the more often the states vote in the same fashion overall. An analysis using islands seeks to locate those
            islands composed of states
            that vote together the most often. In practice in Pajek, the threshold
            t is left implicit by specifying the minimum and maximum sizes of
            islands. Determining island sizes is done by inspecting the distribution of their sizes under different
            pairs of minimum and maximum
            sizes. The algorithm is extremely fast and islands analyses done
            on the UNGA voting matrices are completed within seconds rather
            than hours for many of the blockmodeling analyses.
            Because of space constraints, we summarize our results here.
            The analysis in terms of islands produced partitions that differ in
            two ways from those obtained by using blockmodeling. First, the
            islands procedure allows for vertices to be unclassiﬁed. For Time
            1, there were 5 identiﬁed islands and one residual cluster of states
            that were not classiﬁed. And for resolutions there were four islands
            of classiﬁed resolutions and one residual cluster. The implied value
            of the criterion function for this partition is 3130, a value well above
            all values of the criterion function reported in Table 1. The values
            of ARI when the islands partition is compared with the (5, 5), (6,
            6) and (7, 7) partitions are 0.148, 0.262, and 0.235 respectively.
            The partition provided by the island detection method differs from
            all of the blockmodeling partitions. While it does cluster all of the
            Western and/or Industrial states together, it is less nuanced in the
            clustering of the remaining states.
            An analysis using Islands for Time 2 led to a partition with 8
            clusters of states and a partition with 7 clusters of resolutions.36
            The implied value of the criterion function was 1016.5, a value well
            above the values reported in Table 5. When this partition was compared with the (7, 7) and (8, 8)
            partitions, the values of the ARI
            measure were 0.455 and 0.416, respectively. The clusters detected
            by the islands method were very different to those obtained from
            blockmodeling and inferior in terms of the criterion function used
            to evaluate joint partitions.
            6.4. Community detection
            At face value, there is a potential connection to be made
            with the community detection literature developed primarily by
            researchers coming from physics. See, for example, Fortunado
            (2009), Girvan and Newman (2002) and Newman (2006). The
            essential idea of community detection for one-mode networks is
            to locate one or more subsets of vertices such that the ties between
            vertices inside the subsets are larger (if valued) and/or more dense
            than the ties from these vertices to the rest of the network. The
            potential connection is that within voting blocs there will be dense
            ties of a particular type. We explore the popular Girvan–Newman
            (2002) algorithm37 implemented in UCInet (Borgatti et al., 2002).

            36
            As noted, we do not report the full results because of space constraints but they
            are available upon request from the authors.
            37
            We thank Steve Borgatti for implementing this algorithm into Ucinet (Borgatti
            et al., 2002).

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            197

            Fig. 9. Eigenvector centrality voting map for “Europe”, Time 1 (1981–1985).

            This also required conversion of our two-mode data into two onemode networks.
            When the Girvan–Newman CD algorithm is used for the onemode network GGT constructed from the two-mode
            network G,
            there is one large cluster with a few (Western) states not in the large
            cluster. The reason for this is straightforward: the counts of states
            voting together on resolutions ranges from 276 (for states that supported all resolutions) to −222 (for 4
            pairs of states whose net joint
            voting together was against military resolutions considered in the
            UNGA). The majority of values in this one-mode matrix are large
            and there is no real surprise with the detection of a single ‘community’. For Time 2, this algorithm
            detected one large community
            containing all states except a few non-Western states.
            We note two things. First, the Girvan–Newman algorithm is formulated for binary networks and it is
            straightforward to convert the
            valued network into a binary one. Some threshold is selected and
            values above this threshold are coded “1” while values below it are
            coded “0.” The problem lies in selecting the threshold and different thresholds lead to detecting different
            communities. This is an
            option that could be explored. Second, and perhaps more importantly, this algorithm was designed for sparse
            networks and the
            UNGA voting arrays are far from sparse. Indeed, they are close
            to being complete. Indeed, the near complete nature of the data
            used here are not appropriate for using this algorithm. A better
            option might be in Newman (2006). Many community detection
            methods are covered by Fortunado (2009) and some could be useful here. Also, Tragg and Bruggeman (2009)
            propose an approach
            for signed networks. These are among many potential algorithms
            that could prove useful. Certainly, the literature on community

            detection could be very useful. We return to this in our ﬁnal
            section.
            7. Summary
            We have presented results stemming from the partitioning of
            signed two-mode data in the form of states voting on military resolutions that came before the UNGA for two
            time periods (1981–85
            and 1996–2001). One period was well before the dissolution of the
            Soviet Union and the other period was deﬁned for a time period
            that was as far after this dissolution as the ﬁrst period was before
            it. Unsurprisingly, the voting structure of states differed considerably for the two time periods. But a
            focus on voting analysis
            was not our primary intent. Rather, our purpose was to consider
            seriously some of the methodological problems of a blockmodeling approach to partitioning large signed
            two-mode data. While
            the UNGA voting data for military resolutions could be seen as no
            more than providing a ‘convenient’ data set for doing this, these
            data are far more than a demonstration database. In addition to
            being important in their own right, these data exemplify the inherent problems stemming from moving from
            small data sets to data
            that stretch the bounds of what can be done with blockmodeling.
            Hence, the preliminary effort to evaluate our results using alternative approaches. A second motivation was
            to evaluate what can
            be gained in the trade-off of using an approach with high computational times for larger datasets. Our
            desire was to ‘bring back in’
            the less studied triple of two actors and a social object that blockmodeling allows in order to explore
            balancing of power processes
            using IR data.

            198

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            Fig. 10. Eigenvector centrality voting map for “Europe”, Time 2 (1996–2001).

            We also considered some other approaches to these data,
            and presented comparisons of the different partitions that are
            identiﬁed using these alternative methods. Direct comparisons,
            however, were difﬁcult because of one fundamental difference in
            the methodological approaches used in the analysis of signed twomode data. Blockmodeling is a full
            information approach that deals
            directly with all of the data and does not use any summarization
            of them for obtaining partitions or any general conclusions about
            these data. The cost of doing this comes in the form of increased
            computational times for completing the analyses of data. Even so,
            for the UNGA voting data for military resolutions, the more extensive analyses can be done. Of course, this
            ﬂies in the face of a drive
            to analyze very large data sets in a very fast fashion and one of the
            large questions is whether or not the extra time is worth it.
            In terms of characterizing the big picture, the blockmodeling
            approach considered here appears to provide the same general
            characterization of the structure of UNGA voting as some of the
            other approaches. The potential contribution of blockmodeling is to
            go beyond this type of characterization to probe for further details
            within these broad characterizations. While we think that the more
            detailed partitions that we have provided as examples do lay the
            foundations for doing this, a decision about the value of examining
            these ﬁne-grained details is up to individual researchers.
            We think that the joint partition of states and resolutions has
            intrinsic value because when considering the blocs of states with
            regard to voting, a deeper understanding comes from knowing
            where the exact differences are located. Blockmodeling provides
            a coherent way of doing this. For example, it is one thing to note

            that the states belonging to different broad alliances face competing pressures to vote according to the
            multiple memberships that
            they have. It is another thing to identify responses to these kinds of
            pressures. We think that the reﬁned partition in the bottom panel
            of Fig. 4 takes a useful step in this direction. This sets up a more
            demanding task to see exactly what it is in the content of the resolutions that leads states to vote the way
            they do, especially when
            subject to conﬂicting pressures, in the UNGA. But as illustrated in
            Fig. 5, it provides the means to explore soft balancing processes, e.g.,
            efforts by states to exert their policy preferences. The drilling down
            feature of blockmodeling allows for the identiﬁcation of the complete set of resolutions that distinguish
            the voting among states and
            to identify the issues that distinguish typical voting blocs and that
            may contribute to balancing processes among competing states or
            alliances. For example, the complete information analysis provides
            the information needed to determine whether voting differences
            are due to a particular conﬂict (differences in how to resolve the
            ongoing Middle East conﬂict for example); or reﬂect more general
            challenges to a state or an alliances’ military power by constraining
            its use, e.g., through promotion of norms that prohibit the possession or use of particular weaponry.
            8. Discussion
            During the analyses that led to the results shown here, a number of difﬁcult issues emerged. Key to ﬁtting
            blockmodels is the
            distinction between inductive and deductive blockmodeling. The
            former forms a strategy that can be construed as based on ignorance

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            because all that is expressed is that certain blocks could be present.
            Of course, there is a role for inductive blockmodeling, especially
            when it is applied in new empirical contexts. Yet, we often know
            more about empirical phenomena than the relevance of particular block types. This knowledge can be expressed
            in a pre-speciﬁed
            blockmodel. Our experience with the generic blockmodel described
            in Section 4 is a sobering reminder that a prespeciﬁed model, while
            immensely plausible, can never be the end of a structural story.
            The use of what we thought would be an inferior indirect approach
            using Euclidean distance, while much inferior most of the time, did
            surprise us once in Time 1. It suggested an alternative, and better,
            pre-speciﬁed blockmodel that we were compelled to use. This suggests that if there are multiple approaches
            to obtaining a partition
            of two-mode signed (or any) structural data, there is a beneﬁt to
            moving between the different methods rather than sticking to one
            preferred approach.38
            One little explored feature of the blockmodeling of signed data,
            regardless of whether they are one-mode or two-mode, is found
            in the criterion function, ˛N + (1 − ˛)P that we have used. Nearly
            all empirical applications have featured partitioning with ˛ = 0.5
            because it is safe and there is no obvious reason for weighting N
            and P differently. Yet, when there is a large imbalance between the
            number of positive and negative ties in a signed data set, it seems
            reasonable to explore different values for ˛. Indeed, some of the
            reﬁned partitions that we reported for partitioning within previously established blocks were done with
            varying ˛. There are at
            least two compelling reasons for doing this. One is pragmatic in
            that values of ˛ that depart from 0.5 are much more likely to produce unique partitions. More importantly,
            when seeking ‘minority’
            patterns within large blocks, it is important to make sure that the
            sub-blocks are really homogeneous. So, when the minority different blocks are expected to be N blocks then
            positive inconsistencies
            were weighted more heavily and, when the minority blocks were
            expected to be P blocks, the negative inconsistencies were weighted
            more heavily. The detailed impact of differing values of ␣ is an issue
            that merits exploration.
            As data sets considered by network analysts expand in size,
            and blockmodeling two-mode data structures is seen as having
            value, there is a need for faster algorithms. In terms of speed, the
            approaches considered here range from the blazingly fast Islands
            algorithm to the heavy computational demands of direct blockmodeling. Choosing a preferred method(s) will be
            the decision of
            individual researchers according to their overall objectives. Blockmodeling locates the same general
            clustering and also provides
            the means for drilling down into ﬁne-grained details with greater
            ease or clarity than some other approaches. On the other hand,
            blockmodeling may lead to extremely ﬁne-grained partitions with
            the potential to overwhelm the researcher with this detail. Still,
            this detail may be of value for those interested in the position of
            particular states or groups of states on particular policies.
            We emphasize that our comparisons with other methods are
            far from complete. Space constraints prevented us from fully comparing alternative approaches. The fact that
            these methods did not
            work as well in the data considered here, in terms of poor ARI scores
            or not providing partitions has no implications for their general utility. In the RSB approach to
            partitioning 2-mode networks, the primary evaluative emphasis is on the value of the criterion functions
            for established partitions, something that is absent from the other
            approaches that we have considered. We acknowledge that given
            the presence of many other algorithms, such as used in the community detection approach, there may be more
            efﬁcient methods

            38
            It raises also the daunting problem of specifying the form of two-mode data
            structures where an indirect approach, such as the one using Euclidean distance,
            performs very well rather than very poorly.

            199

            to identify clusters in large datasets. But our motivation for writing
            this paper was twofold: to explore solutions for problems that occur
            when applying the relaxed structural balance approach to large
            signed two-mode data, and to contribute to recent applications of
            social network analysis to IR research by highlighting its potential
            to uncover soft balancing of power processes. Soft balancing implies
            alliances with consistency with regard to joint voting behavior and,
            to capture this, the signed blockmodeling of two-mode data has
            been operationalized in a speciﬁc and substantive driven way.

            Appendix A. Countries included in the analyses
            T1 countries, N = 141
            DKCambodia
            Afghanistan
            DemYemen
            Algeria
            Denmark
            Angola
            Djibouti
            Argentina
            Australia
            DominicanRep
            Ecuador
            Austria
            Bahamas
            Egypt
            ElSalvador
            Bahrain
            Ethiopia
            Bangladesh
            Fiji
            Barbados
            Finland
            Byelorussia
            Belgium
            France
            Gabon
            Benin
            GerDemRep
            Bhutan
            GerFedRep
            Bolivia
            Ghana
            Botswana
            Brazil
            Greece
            Guatemala
            Bulgaria
            Guinea
            UVBurkinoFaso
            GuineaBissau
            BurmaMyanmar
            Burundi
            Guyana
            Haiti
            Cameroon
            Honduras
            Canada
            CapeVerde
            Hungary
            Iceland
            CenAfrRep
            Chad
            India
            Indonesia
            Chile
            Iran
            China
            Iraq
            Colombia
            Ireland
            Congo
            CostaRica
            Israel
            Italy
            CotedIvoire
            Cuba
            Jamaica
            Japan
            Cyprus
            Jordan
            Czechoslovakia

            Kenya
            Kuwait
            Laos
            Lebanon
            Lesotho
            Liberia
            Libya
            Luxembourg
            Madagascar
            Malawi
            Malaysia
            Maldives
            Mali
            Malta
            Mauritania
            Mauritius
            Mexico
            Mongolia
            Morocco
            Mozambique
            Nepal
            Netherlands
            NewZealand
            Nicaragua
            Niger
            Nigeria
            Norway
            Oman
            Pakistan
            Panama
            PapuaNewGuinea
            Paraguay
            Peru
            Philippines
            Poland

            Portugal
            Qatar
            Romania
            USSR
            Rwanda
            StLucia
            SaoTomePrincipe
            SaudiArabia
            Senegal
            SierraLeone
            Singapore
            Somalia
            Spain
            SriLanka
            Sudan
            Suriname
            Sweden
            SyrianArabRep
            Thailand
            Togo
            TrinidadTobago
            Tunisia
            Turkey
            Uganda
            Ukraine
            UAE
            UK
            URTanzania
            US
            Uruguay
            Venezuela
            Vietnam
            Yemen
            Yugoslavia
            ZaireDRC
            Zambia

            T2 Countries, N = 153
            Djibouti
            Albania
            DominicanRep
            Algeria
            Ecuador
            Andorra
            Egypt
            Angola
            ElSalvador
            AntiguaBarbuda
            EquatorialGuinea
            Argentina
            Eritrea
            Armenia
            Estonia
            Australia
            Ethiopia
            Austria
            Fiji
            Azerbaijan
            Finland
            Bahamas
            France
            Bahrain
            Gabon
            Bangladesh
            Georgia
            Barbados
            GerFedRep
            Belarus
            Belgium
            Ghana
            Greece
            Belize
            Grenada
            Benin
            Guatemala
            Bhutan
            Guinea
            Bolivia
            Botswana
            Guyana
            Haiti
            Brazil
            Honduras
            BruneiDar
            Hungary
            Bulgaria
            UVBurkinoFaso
            Iceland

            Lebanon
            LibyanAJ
            Liechtenstein
            Lithuania
            Luxembourg
            Madagascar
            Malaysia
            Maldives
            Mali
            Malta
            Mauritius
            Mexico
            Monaco
            Mongolia
            Morocco
            Mozambique
            Namibia
            Nepal
            Netherlands
            NewZealand
            Nicaragua
            Nigeria
            Norway
            Oman
            Pakistan

            StLucia
            Samoa
            SanMarino
            SaudiArabia
            Senegal
            SierraLeone
            Singapore
            Slovakia
            Slovenia
            SolomonIslands
            SouthAfrica
            Spain
            SriLanka
            Sudan
            Suriname
            Swaziland
            Sweden
            SyrianArabRep
            Tajikistan
            Thailand
            TFYRM
            Togo
            TrinidadTobago
            Tunisia
            Turkey

            200

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            Appendix A (Continued )
            BurmaMyanmar
            Cameroon
            Canada
            CapeVerde
            Chile
            China
            Colombia
            CostaRica
            CotedIvoire
            Croatia
            Cuba
            Cyprus
            CzechRep
            Denmark

            India
            Indonesia
            Iran
            Ireland
            Israel
            Italy
            Jamaica
            Japan
            Jordan
            Kazakhstan
            Kenya
            Kuwait
            Laos
            Latvia

            Panama
            PapuaNewGuinea
            Paraguay
            Peru
            Philippines
            Poland
            Portugal
            Qatar
            RepKorea
            RepMoldova
            Romania
            RussianFed
            Monaco
            Mongolia

            Uganda
            Ukraine
            UAE
            UK
            URTanzania
            US
            Uruguay
            Uzbekistan
            Venezuela
            Vietnam
            Yemen
            Zambia
            Zimbabwe

            Appendix B. UNGA military resolutions for each time period

            resolutions concerning ratiﬁcation of Additional Protocol I
            of the Treaty for the Prohibition of nuclear weapons in Latin
            America; 1 on declaration to prevent nuclear catastrophe; 1 on
            comprehensive review of all peacekeeping operations.
            (4) Resolutions from the 10th and 12th special session on disarmament: 55 and 23 resolutions, respectively;
            these address
            multiple issues from general disarmament to chemical, biological and small arm weapons, and the need for
            conventions.
            (5) Special reports on disarmament and security: 5 (one a year) on
            enhancing the effectiveness of the principle of non-use of force
            in international relations.
            (6) UN conferences: 1 on promotion of international cooperation
            in peaceful uses of nuclear energy.
            (7) Normative and rights issues: 2 relating disarmament and
            development; 2 on right of people to peace.

            Time 1: 1981–1985, N = 276 resolutions
            (1) National and regional concerns: a number of resolutions
            addressed apartheid. South Africa was under apartheid from
            1948 to early 1994 and it was sanctioned from voting in the UN
            from September 1974 until June 1994. The issue of apartheid
            was raised in the ﬁrst UNGA session in 1946 but it was seen as
            an internal issue until the 1960 Sharpeville massacre brought
            greater international attention to the issue. It was not until
            1974, that the UNGA voted to expel South Africa from the UN.
            The UNSC 418 in 1977 created a mandatory arms embargo.
            The following resolutions focused on national and regional concerns: South Africa apartheid regime against
            Angola and other
            independent African states (36/172C, 1981); arms embargo
            against South Africa (36/172E; 40/6, 1985); condemnation of
            military and nuclear collaboration with South Africa (37/69D);
            armed conﬂict between Iran and Iraq (37/3); Israeli aggression against Iraqi nuclear installations (36/27,
            1981; 38/9,
            1983; 39/14, 1984); Israeli nuclear armament (36/98, 1981;
            37/82, 1982; 38/69, 1983; 39/147, 1984; 40/93, 1985); using
            Antarctica for peaceful purposes (3 in 1985); Falkland islands
            (37/9, 1982; 38/12, 1983; 39/6, 1984; 40/21, 1985); 13 addressing Palestine; 13 addressing the Middle East
            situation; 3 on
            Afghanistan (very divisive (38/29, 1983; 39/13, 1984; 40/12,
            1985); 2 to establish nuclear-weapon-free-zone in the Middle
            East; 4 to establish a nuclear-weapon-free-zone in South Asia.
            (2) Resolutions related to conventions: chemical and biological
            weapons (the following resolutions revealed a strong division
            in voting: 36/96 A, B, C; 37/98 A, C, D, E; 38/187 A, C; 36/65 A, B,
            E; 40/92 A, C). The Convention on Biological Weapons opened
            for signature April 1972; entered into force March 1975; 171
            signatories and 155 ratiﬁcations; Israel is one of the states that
            have not ratiﬁed or signed; 4 resolutions address the “urgent
            need” for a comprehensive nuclear-test-ban treaty, 1982–1985
            (a Convention was opened for signature in 1996 but is not yet
            in force; the US signed it but did has not ratiﬁed it as of yet;
            see http://www.ctbto.org/faqs/?uid=3&cHash=11241d850); 10
            resolutions call for the creation of an international convention
            to assure non-nuclear weapon states against the use or threat
            of use of nuclear weapons; 1 resolutions calls for a treaty to
            prohibit stationing weapons of any kind in outer space.
            (3) Resolutions addressing particular security issues and policies:
            36 resolutions focus on general and complete disarmament
            with multiple related issues; 4 on removing landmines as
            remnants of war; 4 on reducing military budgets; 2 reviewing
            multilateral treaty-making process; 6 prohibiting the development and manufacture of new types of weapons of
            mass
            destruction; 4 on preventing an arms race in outer space; 1
            on nuclear weapon freeze; 3 calling for an immediate end
            to testing nuclear weapons; 4 calling for implementation of

            Time 2: 1996–2001, N = 150 resolutions

            (1) National and regional concerns: 2 resolutions focus on ﬁnancing of a UN interim force in Lebanon; 1 on
            the ME peace
            process; 6 on the risk of nuclear proliferation in the ME; 1
            on establishing nuclear-weapon-free-zone in the ME region;
            1 on maintenance of international security in the development of South-Eastern Europe; 1 on Bosnia and
            Herzegovina;
            6 to establish a nuclear-weapon-free-zone in South Atlantic;
            1 to establish a nuclear-weapon-free-zone in South Asia; 4 on
            implementing declaration of Indian Ocean as a zone of peace.
            (2) Resolutions related to conventions: these are all addressed under
            the general and complete disarmament resolutions detailed in
            #4 below. The Convention on Chemical Weapons opened for
            signature January 1993; entered into force April 1997; as of May
            2009, 188 signatures and 186 ratiﬁcations; Israel and Myanmar
            have not signed; Angola, N Korea, Egypt, Somalia, Syria have not
            signed or ratiﬁed it.
            (3) Resolutions addressing particular security issues and policies: 82
            resolutions focused on general and complete disarmament
            with multiple issues. Notably, this is a 4-fold increase; several
            resolutions addressed the ICJ advisory opinion on the Legality of the threat or use of nuclear weapons and
            reveal very
            divisive voting; other issues covered a wide range of disarmament issues including the Nuclear
            non-Proliferation Treaty;
            conventional arms control at the regional level; transparency
            in armaments; environmental norms in disarmaments; small
            arms; nuclear testing; 1 on arms race prevention in outer
            space; 1 on comprehensive review of peacekeeping operations;
            1 on maintenance of international security; 7 on scientiﬁc and
            technological developments and their impact on international
            security; 5 on preventing an arms race in outer space; prevent violent disintegration of states; 1 on
            measures to eliminate
            international terrorism.
            (4) Resolutions from the 10th and 12th special session on disarmament: 2 and 6 resolutions respectively
            (continual reduction);
            5 from the 12th session is on a nuclear weapons convention;
            other issues discuss disarmament generally.
            (5) Special reports on disarmament and security: 4 on the International Atomic Energy Agency (IAEA); 1 on
            the report of the
            Security Council.
            (6) Cooperation with other agencies: 6 on cooperation between the
            UN and the Organization for Security and Cooperation in Europe
            (OSCE); 1 on cooperation between the UN and the Preparatory Commission for the Comprehensive nuclear test
            ban treaty
            organization.

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            201

            Fig. C1. A partition of the full set of states and military resolutions for Time 1.

            Appendix C. Ancillary ﬁgures and tables
            Fig. C1 and Table C1.
            Table C1
            Seven clusters of resolutions from Fig. C1.
            R1

            R2

            R3

            36/104
            36/172C
            36/172F
            36/25
            36/92C
            36/92F
            36/95
            36/96A
            37/3
            37/71
            37/81
            37/83
            37/99F
            37/99I
            37/99J
            38/180C
            38/181A
            38/188A
            38/188I
            38/58D
            38/61
            38/68
            38/70
            38/74
            38/9
            39/146C
            39/148H
            39/151A

            36/112
            36/97A
            36/97C
            36/97G
            36/97J
            37/100E
            37/73
            37/78K
            37/84
            37/98C
            37/99D
            37/99E
            37/99G
            38/183P
            38/184
            38/188C
            38/188E
            38/63
            38/71A
            38/81
            39/53
            39/64B
            39/90
            40/81
            40/91B

            36/102
            36/106
            36/84
            36/86A
            36/86B
            36/92E
            36/92H
            36/97K
            36/99
            37/100A
            37/100B
            37/102
            37/118
            37/72
            37/74A
            37/74B
            37/77B
            37/78C
            37/78F
            37/78G
            37/78I
            37/78J
            37/85
            38/132
            38/181B
            38/183B
            38/183D
            38/183G

            R4
            38/73E
            38/73H
            38/76
            38/80
            39/148A
            39/148C
            39/148F
            39/148G
            39/148J
            39/148K
            39/148L
            39/148N
            39/148O
            39/148P
            39/155
            39/49C
            39/49D
            39/52
            39/60
            39/61B
            39/63C
            39/63D
            39/63G
            39/80
            40/151B
            40/151C
            40/151E
            40/152A

            36/172E
            36/188
            36/226B
            36/31
            36/87B
            36/89
            36/92D
            36/92I
            36/94
            36/96B
            37/100C
            37/100H
            37/105
            37/215
            37/69D
            37/77A
            37/78B
            37/80
            37/9
            38/12
            38/133
            38/162
            38/180B
            38/182
            38/183F
            38/188J
            38/58A
            38/58B

            40/11
            40/151A
            40/151D
            40/151F
            40/152I
            40/158
            40/197
            40/70
            40/80B
            40/96A

            R5

            R6

            R7

            36/88
            36/96C
            37/76
            37/95B
            37/98D
            37/98E
            38/187C
            38/29
            38/65
            39/13
            39/148B
            39/55
            39/65A
            39/65E
            40/12
            40/152B
            40/83
            40/92C
            40/94N

            37/167
            37/78A
            38/183A
            38/188G
            38/191
            39/158
            40/156A
            40/156B
            40/156C
            40/159

            36/100
            36/226A
            36/27
            36/92J
            36/92K
            36/97E
            36/98
            37/78E
            37/82
            37/98A
            37/99A
            38/180A
            38/180D
            38/180E
            38/183C
            38/187A
            38/188F
            38/69
            38/75
            39/
            39/6A
            39/6B
            39/7
            39/8D
            39/8E
            39/151D
            39/151I
            39/57

            202

            P. Doreian et al. / Social Networks 35 (2013) 178–203

            Table C1 (Continued)
            R1
            39/151B
            39/151F
            39/151H
            39/51
            39/58
            39/59
            39/61A
            40/150
            40/151H
            40/152J
            40/168C
            40/79
            40/86
            40/87
            40/89A
            40/94A
            40/94F
            40/94G
            40/94M

            R2

            R3
            38/183H
            38/183I
            38/183J
            38/183L
            38/183M
            38/183N
            38/188H
            38/190
            38/58C
            38/58E
            38/62
            38/72
            38/73B

            R4
            40/152E
            40/152G
            40/152M
            40/152N
            40/152P
            40/21
            40/80A
            40/88
            40/89B
            40/90
            40/94H
            40/96C
        </corps>
        <conclusion>
            Aucune conclusion trouvée.
        </conclusion>
        <discussion>
            Aucune discussion trouvée.
        </discussion>
        <biblio>
            References
            Art, R.J., 2005/2006. Correspondence: striking the balance. International Security 30
            (Winter (3)), 177–196.
            Batagelj, V., Mrvar, A., 1998. Pajek—program for large network analysis. Connections
            21, 47–57.
            Bonacich, P., Lloyd, P., 2004. Calculating status with negative relations. Social Networks 26 (4), 331–339.
            Borg, I., Groenen, P.J.F., 2005. Modern Multidimensional Scaling: Theory and Applications, second ed.
            Springer, New York.
            Borgatti, S.P., Everett, M.J., 1992. Regular blockmodels of multiway, multimode
            matrices. Social Networks 14, 91–120.
            Borgatti, S.P., Everett, M.G., 1997. Network analysis of 2-mode data. Social Networks
            19, 243–269.
            Borgatti, S.P., Everett, M.G., Freeman, L., 2002. UCInet for Windows: Software for
            Social Network Analysis. Analytic Technologies, Harvard, MA.
            Brooks, S.G., Wohlforth, W.C., 2005/2006. Correspondence: Striking the Balance.
            International Security 30 (Winter (3)), 177–196.
            Brusco, M., Doreian, P., Mrvar, A., Steinley, D., 2011. Two algorithms for relaxed
            structural balance partitioning: linking theory, models, and data to understand
            social network phenomena. Sociological Methods and Research 40, 57–87.
            Cahoon, L.J. 1975. Locating a set of points using range information only. Unpublished
            PhD Dissertation in Statistics. Carnegie Mellon University, Pittsburgh.
            Cartwright, D., Harary, F., 1956. Structural balance: a generalization of Heider’s theory. Psychological
            Review 63, 277–293.
            Davis, J.A., 1967. Clustering and structural balance in graphs. Human Relations 20,
            181–187.
            De Leeuw, J., 2005. Principal component analysis of Senate voting patterns. In: Sawilowski, S. (Ed.), Real
            Data Analysis. Information Age Publishing, North Carolina,
            pp. 405–411.
            De Leeuw, J., Mair, P., 2007. Homogeneity analysis. In: R: The Package Homals.
            Preprint 525, Department of Statistics, UCLA.
            De Leeuw, J., Mair, P., 2009. Giﬁ methods for optimal scaling in r: the package homals.
            Journal of Statistical Software 31 (4), 1–20, http://www.jstatsoft.org/v31/i04/.
            De Nooy, Wouter, Andrej, Mrvar, Vladimir, Batagelj, 2011. Exploratory Social Network Analysis with Pajek:
            Revised and Expanded, 2nd ed. Cambridge University
            Press, New York.
            Doreian, P., Batagelj, V., Ferligoj, A., 1994. Partitioning networks based on generalized
            concepts of equivalence. Journal of Mathematical Sociology 19, 1–27.
            Doreian, P., Batagelj, V., Ferligoj, A., 2004. Generalized blockmodeling of two-mode
            network data. Social Networks 26, 29–53.
            Doreian, P., Batagelj, V., Ferligoj, A., 2005. Generalized Blockmodeling. Cambridge
            University Press, Cambridge, England.
            Doreian, P., Krackhardt, D., 2001. Pre-transitive balance mechanisms for signed networks. Journal of
            Mathematical Sociology 25, 43–67.
            Doreian, P., Mrvar, A., 1996. A partitioning approach to structural balance. Social
            Networks 18, 149–168.
            Doreian, P., Mrvar, A., 2009. Partitioning signed social networks. Social Networks 31,
            1–11.
            Desposato, S.W., 1997. Party Switching and Democratization in Brazil. In:
            Presented at the 1997 Meeting of the Latin American Studies Association in Guadalajara, Mexico, April 17–20,
            1997. Available from:
            http://lasa.international.pitt.edu/LASA97/desposato.pdf.
            Downs, A., 1957. An Economic Theory of Democracy. Harper, New York.
            Feld, S., Elmore, R., 1982. Patterns of sociometric choices: patterns of transitivity
            reconsidered. Social Psychology Quarterly 45 (2), 77–85.
            Fortunado, S., 2009. Community detection in graphs. Physics Reports 486, 75–174.

            38/67
            38/73F
            38/73G
            39/11
            39/148M
            39/151E
            39/157
            39/167
            39/49A
            39/49B
            39/6
            39/62
            39/63A
            39/63H
            39/63K
            39/81

            R5

            R6

            R7
            39/65B
            40/152H
            40/168A
            40/168B
            40/18
            40/6
            40/85
            40/92A
            40/93
            40/94I
            40/96D

            Giﬁ, A., 1990. Nonlinear Multivariate Analysis. John Wiley & Sons, New York.
            Girvan, M., Newman, M.E.J., 2002. Community structure in social and biological networks. Proceedings of the
            National Academy of Sciences 99,
            7821–7826.
            Gower, J.C., Hand, D.J., 1995. Biplots. Chapman & Hall/CRC, Boca Raton, FL.
            Gower, J.C., Lubbe, S.G., le Roux, N., 2011. Understanding Biplots. John Wiley & Sons,
            New York.
            Harary, F., Norman, R.Z., Cartwright, D., 1965. Structural Models: An Introduction to
            the Theory of Directed Graphs. John Wiley, New York.
            Heider, F., 1946. Attitudes and cognitive organization. Journal of Psychology 21,
            107–112.
            Heider, F., 1958. The Psychology of Interpersonal Relations. John Wiley and Sons,
            New York.
            Hotelling, H., 1929. Stability and competition. Economic Journal 39 (1), 41–57.
            Hubert, L., Arabie, P., 1985. Comparing partitions. Journal of Classiﬁcation 2,
            193–218.
            Hummon, N.P., Doreian, P., 2003. Some dynamics of social balance processes: bringing Heider back into
            balance theory. Social Networks 25, 17–49.
            Huntington, Samuel P., 1996. The Clash of Civilizations and the Remaking of World
            Order. Simon and Schuster, New York.
            Kim, S.Y., Russett, B., 1996. The new politics of voting alignments in the
            United Nations General Assembly. International Organization 50 (4), 629–
            652.
            Latapy, M., Magnien, C., Vecchio, N.D., 2008. Basic notions for the analysis of large
            two-mode networks. Social Networks 30, 31–48.
            Lloyd, Paulette, 2007. Mapping the World Order. Unpublished manuscript.
            Michailidis, G., de Leeuw, J., 1998. The Giﬁ system of descriptive multivariate analysis. Statistical
            Science 13, 307–336.
            Mrvar, A., Doreian, P., 2009. Partitioning signed two-mode networks. Journal of
            Mathematical Sociology 33, 196–221.
            Newcomb, T.N., 1961. The Acquaintance Process. Holt Rinehart and Winston, New
            York.
            Newman, M.E.J., 2006. Finding community structure in networks using the eigenvectors of matrices. Physical
            Review E 74, 036104.
            Nordlie, P.H., 1958. A Longitudinal Study of Interpersonal Attraction in a Natural
            Setting, Unpublished Ph.D. thesis, University of Michigan.
            Paul, T.V., 2004. The enduring axioms of balance of power theory. In: Paul, T.V.,
            Wirtz, J.J., Fortman, M. (Eds.), Balance of Power: Theory and Practice in the 21st
            Century. Stanford University Press, Stanford, pp. 1–25.
            Poole, K.T., 2005. Spatial Models of Parliamentary Voting. Cambridge University
            Press, Cambridge, UK.
            Poole, K.T., Lewis, j., Lo, J., Carroll, R., 2011. Scaling roll call votes with nominate.
            Journal of Statistical Software 42 (14), 1–21, http://www.jstatsoft.org/v42/i14/.
            Robins, G., Kashima, Y., 2008. Social psychology and social networks: individuals
            and social systems. Asian Journal of Social Psychology 11, 1–12.
            Santos, J.M., Embrechts, M.,2009. On the use of the Adjusted Rand Index as a metric
            for evaluating supervised classiﬁcation. In: ICANN ‘09: Proceedings of the 19th
            International Conference on Artiﬁcial Neural Networks. Springer-Verlag, Berlin,
            Heidelberg, pp. 175–184.
            Saporta, G., Youness, G., 2002. Comparing two partitions: some proposals and experiments. In: Hardle, W.,
            Ronz, B. (Eds.), Proceedings in Computational Statistics.
            Physica Verlag, Berlin, pp. 243–248.
            Snyder, D., Kick, E.L., 1979. Structural position in the world system and economic
            growth, 1955–1970: a multiple-network analysis of transnational interactions.
            American Journal of Sociology 84 (5), 1096–1126.
            Steinley, D., 2004. Properties of the Hubert–Arabie Adjusted Rand Index. Psychological Methods 9 (3),
            386–396.

            P. Doreian et al. / Social Networks 35 (2013) 178–203
            Tragg, V.A., Bruggeman, J., 2009. Community detection in networks with positive
            and negative links. Physical Review E (Statistical, Nonlinear, and Soft Matter
            Physics) 80 (3), 036115.
            Voeten, E., 2000. Clashes in the assembly. International Organization 54 (2),
            185–217.
            Warrens, M., 2008. On the equivalence of Cohen’s Kappa and the Hubert–Arabie
            Adjusted Rand Index. Journal of Classiﬁcation 25, 177–183.

            203

            Ward, J.H., 1963. Hierarchical grouping to optimize an objective function. Journal of
            the American Statistical Association 58, 236–244.
            Zaveršnik, M., Batagelj, V., 2004. Islands, XXIV International Sunbelt Social Network Conference,
            Portorož, Slovenia, May 12–16, 2004. Available from:
            http://vlado.fmf.uni-lj.si/pub/networks/Doc/Sunbelt/islands.pdf.


        </biblio>
    </article>
    <article>
        <preamble>Polibits_42_02.pdf</preamble>
        <titre>Summary Evaluation with and without References</titre>
        <auteur>
            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales
        </auteur>
        <abstract>
            Abstract—We study a new content-based method for
            the evaluation of text summarization systems without
            human models which is used to produce system rankings.
            The research is carried out using a new content-based
            evaluation framework called F RESA to compute a variety of
            divergences among probability distributions. We apply our
            comparison framework to various well-established content-based
            evaluation measures in text summarization such as C OVERAGE,
            R ESPONSIVENESS, P YRAMIDS and ROUGE studying their
            associations in various text summarization tasks including
            generic multi-document summarization in English and French,
            focus-based multi-document summarization in English and
            generic single-document summarization in French and Spanish.
            Index Terms—Text summarization evaluation, content-based
            evaluation measures, divergences.
        </abstract>
        <introduction>
            I. I NTRODUCTION

            T

            EXT summarization evaluation has always been a
            complex and controversial issue in computational
            linguistics. In the last decade, significant advances have been
            made in this field as well as various evaluation measures have
            been designed. Two evaluation campaigns have been led by
            the U.S. agence DARPA. The first one, SUMMAC, ran from
            1996 to 1998 under the auspices of the Tipster program [1],
            and the second one, entitled DUC (Document Understanding
            Conference) [2], was the main evaluation forum from 2000
            until 2007. Nowadays, the Text Analysis Conference (TAC)
            [3] provides a forum for assessment of different information
            access technologies including text summarization.
            Evaluation in text summarization can be extrinsic or
            intrinsic [4]. In an extrinsic evaluation, the summaries are
            assessed in the context of an specific task carried out by a
            human or a machine. In an intrinsic evaluation, the summaries
            are evaluated in reference to some ideal model. SUMMAC
            was mainly extrinsic while DUC and TAC followed an
            intrinsic evaluation paradigm. In an intrinsic evaluation, an
            Manuscript received June 8, 2010. Manuscript accepted for publication July
            25, 2010.
            Juan-Manuel Torres-Moreno is with LIA/Université d’Avignon,
            France
            and
            École
            Polytechnique
            de
            Montréal,
            Canada
            (juan-manuel.torres@univ-avignon.fr).
            Eric
            SanJuan
            is
            with
            LIA/Université
            d’Avignon,
            France
            (eric.sanjuan@univ-avignon.fr).
            Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain
            (horacio.saggion@upf.edu).
            Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain;
            LIA/Université d’Avignon, France and Instituto de Ingenierı́a/UNAM, Mexico
            (iria.dacunha@upf.edu).
            Patricia
            Velázquez-Morales
            is
            with
            VM
            Labs,
            France
            (patricia velazquez@yahoo.com).

            automatically generated summary (peer) has to be compared
            with one or more reference summaries (models). DUC used
            an interface called SEE to allow human judges to compare
            a peer with a model. Thus, judges give a C OVERAGE score
            to each peer produced by a system and the final system
            C OVERAGE score is the average of the C OVERAGE’s scores
            asigned. These system’s C OVERAGE scores can then be used
            to rank summarization systems. In the case of query-focused
            summarization (e.g. when the summary should answer a
            question or series of questions) a R ESPONSIVENESS score
            is also assigned to each summary, which indicates how
            responsive the summary is to the question(s).
            Because manual comparison of peer summaries with model
            summaries is an arduous and costly process, a body of
            research has been produced in the last decade on automatic
            content-based evaluation procedures. Early studies used text
            similarity measures such as cosine similarity (with or without
            weighting schema) to compare peer and model summaries
            [5]. Various vocabulary overlap measures such as n-grams
            overlap or longest common subsequence between peer and
            model have also been proposed [6], [7]. The B LEU machine
            translation evaluation measure [8] has also been tested in
            summarization [9]. The DUC conferences adopted the ROUGE
            package for content-based evaluation [10]. ROUGE implements
            a series of recall measures based on n-gram co-occurrence
            between a peer summary and a set of model summaries. These
            measures are used to produce systems’ rank. It has been shown
            that system rankings, produced by some ROUGE measures
            (e.g., ROUGE-2, which uses 2-grams), have a correlation with
            rankings produced using C OVERAGE.
            In recent years the P YRAMIDS evaluation method [11] has
            been introduced. It is based on the distribution of “content”
            of a set of model summaries. Summary Content Units (SCUs)
            are first identified in the model summaries, then each SCU
            receives a weight which is the number of models containing
            or expressing the same unit. Peer SCUs are identified in the
            peer, matched against model SCUs, and weighted accordingly.
            The P YRAMIDS score given to a peer is the ratio of the sum
            of the weights of its units and the sum of the weights of the
            best possible ideal summary with the same number of SCUs as
            the peer. The P YRAMIDS scores can be also used for ranking
            summarization systems. [11] showed that P YRAMIDS scores
            produced reliable system rankings when multiple (4 or more)
            models were used and that P YRAMIDS rankings correlate with
            rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE
            with skip 2-grams). However, this method requires the creation

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            of models and the identification, matching, and weighting of
            SCUs in both: models and peers.
            [12] evaluated the effectiveness of the Jensen-Shannon
            (J S) [13] theoretic measure in predicting systems ranks
            in two summarization tasks: query-focused and update
            summarization. They have shown that ranks produced
            by P YRAMIDS and those produced by J S measure
            correlate. However, they did not investigate the effect
            of the measure in summarization tasks such as generic
            multi-document summarization (DUC 2004 Task 2),
            biographical summarization (DUC 2004 Task 5), opinion
            summarization (TAC 2008 OS), and summarization in
            languages other than English.
            In this paper we present a series of experiments aimed at
            a better understanding of the value of the J S divergence
            for ranking summarization systems. We have carried out
            experimentation with the proposed measure and we have
            verified that in certain tasks (such as those studied by
            [12]) there is a strong correlation among P YRAMIDS,
            R ESPONSIVENESS and the J S divergence, but as we will
            show in this paper, there are datasets in which the correlation
            is not so strong. We also present experiments in Spanish
            and French showing positive correlation between the J S
            and ROUGE which is the de facto evaluation measure used
            in evaluation of non-English summarization. To the best of
            our knowledge this is the more extensive set of experiments
            interpreting the value of evaluation without human models.
            The rest of the paper is organized in the following way:
            First in Section II we introduce related work in the area of
            content-based evaluation identifying the departing point for
            our inquiry; then in Section III we explain the methodology
            adopted in our work and the tools and resources used for
            experimentation. In Section IV we present the experiments
            carried out together with the results. Section V discusses the
            results and Section VI concludes the paper and identifies future
            work.
        </introduction>
        <corps>
            II. R ELATED W ORK
            One of the first works to use content-based measures in
            text summarization evaluation is due to [5], who presented an
            evaluation framework to compare rankings of summarization
            systems produced by recall and cosine-based measures. They
            showed that there was weak correlation among rankings
            produced by recall, but that content-based measures produce
            rankings which were strongly correlated. This put forward
            the idea of using directly the full document for comparison
            purposes in text summarization evaluation. [6] presented a
            set of evaluation measures based on the notion of vocabulary
            overlap including n-gram overlap, cosine similarity, and
            longest common subsequence, and they applied them to
            multi-document summarization in English and Chinese.
            However, they did not evaluate the performance of the
            measures in different summarization tasks. [7] also compared
            various evaluation measures based on vocabulary overlap.
            Although these measures were able to separate random from

            non-random systems, no clear conclusion was reached on the
            value of each of the studied measures.
            Nowadays, a widespread summarization evaluation
            framework is ROUGE [14], which offers a set of statistics
            that compare peer summaries with models. It counts
            co-occurrences of n-grams in peer and models to derive a
            score. There are several statistics depending on the used
            n-grams and the text processing applied to the input texts
            (e.g., lemmatization, stop-word removal).
            [15] proposed a method of evaluation based on the
            use of “distances” or divergences between two probability
            distributions (the distribution of units in the automatic
            summary and the distribution of units in the model
            summary). They studied two different Information Theoretic
            measures of divergence: the Kullback-Leibler (KL) [16] and
            Jensen-Shannon (J S) [13] divergences. KL computes the
            divergence between probability distributions P and Q in the
            following way:
            Pw
            1X
            Pw log2
            (1)
            DKL (P ||Q) =
            2 w
            Qw
            While J S divergence is defined as follows:
            1X
            2Pw
            2Qw
            DJ S (P ||Q) =
            Pw log2
            + Qw log2
            2 w
            Pw + Qw
            Pw + Qw
            (2)
            These measures can be applied to the distribution of units in
            system summaries P and reference summaries Q. The value
            obtained may be used as a score for the system summary. The
            method has been tested by [15] over the DUC 2002 corpus for
            single and multi-document summarization tasks showing good
            correlation among divergence measures and both coverage and
            ROUGE rankings.
            [12] went even further and, as in [5], they proposed to
            compare directly the distribution of words in full documents
            with the distribution of words in automatic summaries to
            derive a content-based evaluation measure. They found a
            high correlation between rankings produced using models
            and rankings produced without models. This last work is the
            departing point for our inquiry into the value of measures that
            do not rely on human models.
            III. M ETHODOLOGY
            The followed methodology in this paper mirrors the one
            adopted in past work (e.g. [5], [7], [12]). Given a particular
            summarization task T , p data points to be summarized
            p−1
            with input material {Ii }i=0
            (e.g. document(s), question(s),
            s−1
            topic(s)), s peer summaries {SUMi,k }k=0
            for input i, and
            m−1
            m model summaries {MODELi,j }j=0 for input i, we will
            compare rankings of the s peer summaries produced by various
            evaluation measures. Some measures that we use compare
            summaries with n of the m models:
            MEASUREM (SUMi,k , {MODELi,j }n−1
            j=0 )

            (3)

            Summary Evaluation with and without References

            while other measures compare peers with all or some of the
            input material:
            MEASUREM (SUMi,k , Ii0 )

            (4)

            where Ii0 is some subset of input Ii . The values produced
            by the measures for each summary SUMi,k are averaged
            for each system k = 0, . . . , s − 1 and these averages are
            used to produce a ranking. Rankings are then compared
            using Spearman Rank correlation [17] which is used to
            measure the degree of association between two variables
            whose values are used to rank objects. We have chosen
            to use this correlation to compare directly results to those
            presented in [12]. Computation of correlations is done using
            the Statistics-RankCorrelation-0.12 package1 , which computes
            the rank correlation between two vectors. We also verified
            the good conformity of the results with the correlation test
            of Kendall τ calculated with the statistical software R. The
            two nonparametric tests of Spearman and Kendall do not
            really stand out as the treatment of ex-æquo. The good
            correspondence between the two tests shows that they do not
            introduce bias in our analysis. Subsequently will mention only
            the ρ of Sperman more widely used in this field.
            A. Tools
            We carry out experimentation using a new summarization
            evaluation framework: F RESA –FRamework for Evaluating
            Summaries Automatically–, which includes document-based
            summary evaluation measures based on probabilities
            distribution2 . As in the ROUGE package, F RESA supports
            different n-grams and skip n-grams probability distributions.
            The F RESA environment can be used in the evaluation of
            summaries in English, French, Spanish and Catalan, and it
            integrates filtering and lemmatization in the treatment of
            summaries and documents. It is developed in Perl and will
            be made publicly available. We also use the ROUGE package
            [10] to compute various ROUGE statistics in new datasets.
            B. Summarization Tasks and Data Sets
            We have conducted our experimentation with the following
            summarization tasks and data sets:
            1) Generic multi-document-summarization in English
            (production of a short summary of a cluster of related
            documents) using data from DUC’043 , task 2: 50
            clusters, 10 documents each – 294,636 words.
            2) Focused-based summarization in English (production of
            a short focused multi-document summary focused on the
            question “who is X?”, where X is a person’s name) using
            data from the DUC’04 task 5: 50 clusters, 10 documents
            each plus a target person name – 284,440 words.
            1 http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/
            2 F RESA

            is available at: http://lia.univavignon.fr/fileadmin/axes/TALNE/
            Ressources.html
            3 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html

            3) Update-summarization task that consists of creating a
            summary out of a cluster of documents and a topic. Two
            sub-tasks are considered here: A) an initial summary has
            to be produced based on an initial set of documents and
            topic; B) an update summary has to be produced from
            a different (but related) cluster assuming documents
            used in A) are known. The English TAC’08 Update
            Summarization dataset is used, which consists of 48
            topics with 20 documents each – 36,911 words.
            4) Opinion summarization where systems have to analyze
            a set of blog articles and summarize the opinions
            about a target in the articles. The TAC’08 Opinion
            Summarization in English4 data set (taken from the
            Blogs06 Text Collection) is used: 25 clusters and targets
            (i.e., target entity and questions) were used – 1,167,735
            words.
            5) Generic single-document summarization in Spanish
            using the Medicina Clı́nica5 corpus, which is composed
            of 50 medical articles in Spanish, each one with its
            corresponding author abstract – 124,929 words.
            6) Generic single document summarization in French using
            the “Canadien French Sociological Articles” corpus
            from the journal Perspectives interdisciplinaires sur le
            travail et la santé (PISTES)6 . It contains 50 sociological
            articles in French, each one with its corresponding
            author abstract – 381,039 words.
            7) Generic multi-document-summarization in French using
            data from the RPM27 corpus [18], 20 different themes
            consisting of 10 articles and 4 abstracts by reference
            thematic – 185,223 words.
            For experimentation in the TAC and the DUC datasets we use
            directly the peer summaries produced by systems participating
            in the evaluations. For experimentation in Spanish and French
            (single and multi-document summarization) we have created
            summaries at a similar ratio to those of reference using the
            following systems:
            – ENERTEX [19], a summarizer based on a theory of
            textual energy;
            – CORTEX [20], a single-document sentence extraction
            system for Spanish and French that combines various
            statistical measures of relevance (angle between sentence
            and topic, various Hamming weights for sentences, etc.)
            and applies an optimal decision algorithm for sentence
            selection;
            – SUMMTERM [21], a terminology-based summarizer that
            is used for summarization of medical articles and
            uses specialized terminology for scoring and ranking
            sentences;
            – REG [22], summarization system based on an greedy
            algorithm;
            4 http://www.nist.gov/tac/data/index.html
            5 http://www.elsevier.es/revistas/ctl servlet? f=7032&revistaid=2
            6 http://www.pistes.uqam.ca/
            7 http://www-labs.sinequa.com/rpm2

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            – J S summarizer, a summarization system that scores
            and ranks sentences according to their Jensen-Shannon
            divergence to the source document;
            – a lead-based summarization system that selects the lead
            sentences of the document;
            – a random-based summarization system that selects
            sentences at random;
            – Open Text Summarizer [23], a multi-lingual summarizer
            based on the frequency and
            – commercial systems: Word, SSSummarizer8 , Pertinence9
            and Copernic10 .
            C. Evaluation Measures
            The following measures derived from human assessment of
            the content of the summaries are used in our experiments:
            – C OVERAGE is understood as the degree to which one
            peer summary conveys the same information as a model
            summary [2]. C OVERAGE was used in DUC evaluations.
            This measure is used as indicated in equation 3 using
            human references or models.
            – R ESPONSIVENESS ranks summaries in a 5-point scale
            indicating how well the summary satisfied a given
            information need [2]. It is used in focused-based
            summarization tasks. This measure is used as indicated
            in equation 4 since a human judges the summary
            with respect to a given input “user need” (e.g., a
            question). R ESPONSIVENESS was used in DUC and TAC
            evaluations.
            – P YRAMIDS [11] is a content assessment measure which
            compares content units in a peer summary to weighted
            content units in a set of model summaries. This
            measure is used as indicated in equation 3 using human
            references or models. P YRAMIDS is the adopted metric
            for content-based evaluation in the TAC evaluations.
            For DUC and TAC datasets the values of these measures are
            available and we used them directly. We used the following
            automatic evaluation measures in our experiments:
            – ROUGE [14], which is a recall metric that takes into
            account n-grams as units of content for comparing peer
            and model summaries. The ROUGE formula specified in
            [10] is as follows:
            ROUGE-n(R, M ) =
            P
            ∈
            M
            count
            m
            n−gram∈P
            match (n − gram)
            P
            P
            count(n-gram)
            m ∈M

            P

            (5)

            where R is the summary to be evaluated, M is the set of
            model (human) summaries, countmatch is the number of
            common n-grams in m and P , and count is the number
            of n-grams in the model summaries. For the experiments
            8 http://www.kryltech.com/summarizer.htm

            presented here we used uni-grams, 2-grams, and the skip
            2-grams with maximum skip distance of 4 (ROUGE-1,
            ROUGE-2 and ROUGE-SU4). ROUGE is used to compare
            a peer summary to a set of model summaries in our
            framework (as indicated in equation 3).
            – Jensen-Shannon divergence formula given in Equation 2
            is implemented in our F RESA package with the following
            specification (Equation 6) for the probability distribution
            of words w.
            CT
            Pw = w
            N
            (
            S
            Cw
            if
            w
            ∈
            S
            NS
            Qw =
            (6)
            T
            Cw
            +δ
            otherwise
            N +δ∗B
            Where P is the probability distribution of words w in
            text T and Q is the probability distribution of words w
            in summary S; N is the number of words in text and
            T
            summary N = NT +NS , B = 1.5|V |, Cw
            is the number
            S
            of words in the text and Cw is the number of words in
            the summary. For smoothing the summary’s probabilities
            we have used δ = 0.005. We have also implemented
            other smoothing approaches (e.g. Good-Turing [24], that
            uses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2
            package11 ) in F RESA, but we do not use them in
            the experiments reported here. Following the ROUGE
            approach, in addition to word uni-grams we use 2-grams
            and skip n-grams computing divergences such as J S
            (using uni-grams) J S 2 (using 2-grams), J S 4 (using the
            skip n-grams of ROUGE-SU4), and J S M which is an
            average of the J S i . J Ss measures are used to compare a
            peer summary to its source document(s) in our framework
            (as indicated in equation 4). In the case of summarization
            of multiple documents, these are concatenated (in the
            given input order) to form a single input from which
            probabilities are computed.
            IV. E XPERIMENTS AND R ESULTS
            We first replicated the experiments presented in [12] to
            verify that our implementation of J S produced correlation
            results compatible with that work. We used the TAC’08
            Update Summarization data set and computed J S and
            ROUGE measures for each peer summary. We produced
            two system rankings (one for each measure), which were
            compared to rankings produced using the manual P YRAMIDS
            and R ESPONSIVENESS scores. Spearman correlations were
            computed among the different rankings. The results are
            presented in Table I. These results confirm a high correlation
            among P YRAMIDS, R ESPONSIVENESS and J S. We also
            verified high correlation between J S and ROUGE-2 (0.83
            Spearman correlation, not shown in the table) in this task and
            dataset.
            Then, we experimented with data from DUC’04, TAC’08
            Opinion Summarization pilot task as well as single and

            9 http://www.pertinence.net
            10 http://www.copernic.com/en/products/summarizer

            11 http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/

            Summary Evaluation with and without References

            TABLE I
            S PEARMAN CORRELATION OF CONTENT- BASED MEASURES IN TAC’08
            U PDATE S UMMARIZATION TASK

            Mesure
            ROUGE-2
            JS

            P YRAMIDS
            0.96
            0.85

            p-value
            p
            <
            0.005
            p
            <
            0.005

            R ESPONSIVENESS
            0.92
            0.74

            p-value
            p
            <
            0.005
            p
            <
            0.005

            multi-document summarization in Spanish and French. In spite
            of the fact that the experiments for French and Spanish corpora
            use less data points (i.e., less summarizers per task) than
            for English, results are still quite significant. For DUC’04,
            we computed the J S measure for each peer summary in
            tasks 2 and 5 and we used J S, ROUGE, C OVERAGE and
            R ESPONSIVENESS scores to produce systems’ rankings. The
            various Spearman’s rank correlation values for DUC’04 are
            presented in Tables II (for task 2) and III (for task 5).
            For task 2, we have verified a strong correlation between
            J S and C OVERAGE. For task 5, the correlation between
            J S and C OVERAGE is weak, and that between J S and
            R ESPONSIVENESS is weak and negative.
            Although the Opinion Summarization (OS) task is a new
            type of summarization task and its evaluation is a complicated
            issue, we have decided to compare J S rankings with those
            obtained using P YRAMIDS and R ESPONSIVENESS in TAC’08.
            Spearman’s correlation values are listed in Table IV. As it can
            be seen, there is weak and negative correlation of J S with
            both P YRAMIDS and R ESPONSIVENESS. Correlation between
            P YRAMIDS and R ESPONSIVENESS rankings is high for this
            task (0.71 Spearman’s correlation value).
            For experimentation in mono-document summarization
            in Spanish and French, we have run 11 multi-lingual
            summarization systems; for experimentation in French, we
            have run 12 systems. In both cases, we have produced
            summaries at a compression rate close to the compression rate
            of the authors’ provided abstracts. We have then computed J S
            and ROUGE measures for each summary and we have averaged
            the measure’s values for each system. These averages were
            used to produce rankings per each measure. We computed
            Spearman’s correlations for all pairs of rankings.
            Results are presented in Tables V, VI and VII. All results
            show medium to strong correlation between the J S measures
            and ROUGE measures. However the J S measure based on
            uni-grams has lower correlation than J Ss which use n-grams
            of higher order. Note that table VII presents results for
            generic multi-document summarization in French, in this
            case correlation scores are lower than correlation scores for
            single-document summarization in French, a result which may
            be expected given the diversity of input in multi-document
            summarization.
            V. D ISCUSSION
            The departing point for our inquiry into text summarization
            evaluation has been recent work on the use of content-based

            evaluation metrics that do not rely on human models but that
            compare summary content to input content directly [12]. We
            have some positive and some negative results regarding the
            direct use of the full document in content-based evaluation.
            We have verified that in both generic muti-document
            summarization
            and
            in
            topic-based
            multi-document
            summarization in English correlation among measures
            that use human models (P YRAMIDS, R ESPONSIVENESS
            and ROUGE) and a measure that does not use models
            (J S divergence) is strong. We have found that correlation
            among the same measures is weak for summarization of
            biographical information and summarization of opinions in
            blogs. We believe that in these cases content-based measures
            should be considered, in addition to the input document, the
            summarization task (i.e. text-based representation, description)
            to better assess the content of the peers [25], the task being a
            determinant factor in the selection of content for the summary.
            Our multi-lingual experiments in generic single-document
            summarization confirm a strong correlation among the
            J S divergence and ROUGE measures. It is worth noting
            that ROUGE is in general the chosen framework for
            presenting content-based evaluation results in non-English
            summarization.
            For the experiments in Spanish, we are conscious that we
            only have one model summary to compare with the peers.
            Nevertheless, these models are the corresponding abstracts
            written by the authors. As the experiments in [26] show, the
            professionals of a specialized domain (as, for example, the
            medical domain) adopt similar strategies to summarize their
            texts and they tend to choose roughly the same content chunks
            for their summaries. Previous studies have shown that author
            abstracts are able to reformulate content with fidelity [27] and
            these abstracts are ideal candidates for comparison purposes.
            Because of this, the summary of the author of a medical article
            can be taken as reference for summaries evaluation. It is worth
            noting that there is still debate on the number of models to be
            used in summarization evaluation [28]. In the French corpus
            PISTES, we suspect the situation is similar to the Spanish
            case.
        </corps>
        <conclusion>
            VI. C ONCLUSIONS AND F UTURE W ORK
            This paper has presented a series of experiments in
            content-based measures that do not rely on the use of model
            summaries for comparison purposes. We have carried out
            extensive experimentation with different summarization tasks
            drawing a clearer picture of tasks where the measures could
            be applied. This paper makes the following contributions:
            – We have shown that if we are only interested in ranking
            summarization systems according to the content of their
            automatic summaries, there are tasks were models could
            be subtituted by the full document in the computation of
            the J S measure obtaining reliable rankings. However,
            we have also found that the substitution of models
            by full-documents is not always advisable. We have

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            TABLE II
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC’04 TASK 2
            Mesure
            ROUGE-2
            JS

            C OVERAGE
            0.79
            0.68

            p-value
            p
            <
            0.0050
            p
            <
            0.0025

            TABLE III
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN DUC’04 TASK 5
            Mesure
            ROUGE-2
            JS

            C OVERAGE
            0.78
            0.40

            p-value
            p
            <
            0.001
            p
            <
            0.050

            R ESPONSIVENESS
            0.44
            -0.18

            p-value
            p
            <
            0.05
            p
            <
            0.25

            TABLE IV
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN TAC’08 OS TASK
            Mesure
            JS

            P YRAMIDS
            -0.13

            p-value
            p
            <
            0.25

            R ESPONSIVENESS
            -0.14

            p-value
            p
            <
            0.25

            TABLE V
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clı́nica C ORPUS (S PANISH )
            Mesure
            JS
            J S2
            J S4
            J SM

            ROUGE -1
            0.56
            0.88
            0.88
            0.82

            p-value
            p
            <
            0.100
            p
            <
            0.001
            p
            <
            0.001
            p
            <
            0.005

            ROUGE -2
            0.46
            0.80
            0.80
            0.71

            found weak correlation among different rankings in
            complex summarization tasks such as the summarization
            of biographical information and the summarization of
            opinions.
            – We have also carried out large-scale experiments in
            Spanish and French which show positive medium to
            strong correlation among system’s ranks produced by
            ROUGE and divergence measures that do not use the
            model summaries.
            – We have also presented a new framework, F RESA, for
            the computation of measures based on J S divergence.
            Following the ROUGE approach, F RESA package use
            word uni-grams, 2-grams and skip n-grams computing
            divergences. This framework will be available to the
            community for research purposes.
            Although we have made a number of contributions, this paper
            leaves many open questions than need to be addressed. In
            order to verify correlation between ROUGE and J S, in the
            short term we intend to extend our investigation to other
            languages such as Portuguese and Chinesse for which we
            have access to data and summarization technology. We also
            plan to apply F RESA to the rest of the DUC and TAC
            summarization tasks, by using several smoothing techniques.
            As a novel idea, we contemplate the possibility of adapting
            the evaluation framework for the phrase compression task
            [29], which, to our knowledge, does not have an efficient
            evaluation measure. The main idea is to calculate J S from
            an automatically-compressed sentence taking the complete
            sentence by reference. In the long term, we plan to incorporate

            p-value
            p
            <
            0.100
            p
            <
            0.002
            p
            <
            0.002
            p
            <
            0.020

            ROUGE -SU4
            0.45
            0.81
            0.81
            0.71

            p-value
            p
            <
            0.200
            p
            <
            0.005
            p
            <
            0.005
            p
            <
            0.010

            a representation of the task/topic in the calculation of
            measures. To carry out these comparisons, however, we are
            dependent on the existence of references.
            F RESA will also be used in the new question-answer task
            campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/
            qa.asp) for the evaluation of long answers. This task aims
            to answer a question by extraction and agglomeration of
            sentences in Wikipedia. This kind of task corresponds
            to those for which we have found a high correlation
            among the measures J S and evaluation methods with
            human intervention. Moreover, the J S calculation will be
            among the summaries produced and a representative set of
            relevant passages from Wikipedia. F RESA will be used to
            compare three types of systems, although different tasks: the
            multi-document summarizer guided by a query, the search
            systems targeted information (focused IR) and the question
            answering systems.
            ACKNOWLEDGMENT
            We are grateful to the Programa Ramón y Cajal from
            Ministerio de Ciencia e Innovación, Spain. This work is
            partially supported by: a postdoctoral grant from the National
            Program for Mobility of Research Human Resources (National
            Plan of Scientific Research, Development and Innovation
            2008-2011, Ministerio de Ciencia e Innovación, Spain); the
            research project CONACyT, number 82050, and the research
            project PAPIIT-DGAPA (Universidad Nacional Autónoma de
            México), number IN403108.

            Summary Evaluation with and without References

            TABLE VI
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE PISTES C ORPUS (F RENCH )
            Mesure
            JS
            J S2
            J S4
            J SM

            ROUGE -1
            0.70
            0.93
            0.83
            0.88

            p-value
            p
            <
            0.050
            p
            <
            0.002
            p
            <
            0.020
            p
            <
            0.010

            ROUGE -2
            0.73
            0.86
            0.76
            0.83

            p-value
            p
            <
            0.05
            p
            <
            0.01
            p
            <
            0.05
            p
            <
            0.02

            ROUGE -SU4
            0.73
            0.86
            0.76
            0.83

            p-value
            p
            <
            0.500
            p
            <
            0.005
            p
            <
            0.050
            p
            <
            0.010

            TABLE VII
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE RPM2 C ORPUS (F RENCH )
            Measure
            JS
            J S2
            J S4
            J SM

            ROUGE -1
            0.830
            0.800
            0.750
            0.850

            p-value
            p
            <
            0.002
            p
            <
            0.005
            p
            <
            0.010
            p
            <
            0.002

            ROUGE -2
            0.660
            0.590
            0.520
            0.640
        </conclusion>
        <discussion>
            Les corrélations JS/ROUGE sont élevées pour le résumé générique, mais faibles pour les tâches plus ciblées.
            L’absence de modèles peut parfois être compensée par une comparaison au texte source. Les auteurs soulignent
            l’importance du choix des tâches pour l’évaluation automatique. L’article met aussi en lumière les limites
            des évaluations sans référence humaine dans certains contextes.
        </discussion>
        <biblio>
            R EFERENCES
            [1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
            B. Sundheim, “Summac: a text summarization evaluation,” Natural
            Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.
            [2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,
            no. 6, pp. 1506–1520, 2007.
            [3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,
            USA: NIST, November 17-19 2008.
            [4] K. Spärck Jones and J. Galliers, Evaluating Natural Language
            Processing Systems, An Analysis and Review, ser. Lecture Notes in
            Computer Science. Springer, 1996, vol. 1083.
            [5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of
            rankings produced by summarization evaluation measures,” in NAACL
            Workshop on Automatic Summarization, 2000, pp. 69–78.
            [6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation
            of Summaries in a Cross-lingual Environment using Content-based
            Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.
            [7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,
            D. Liu, and E. Drábek, “Evaluation challenges in large-scale document
            summarization,” in ACL’03, 2003, pp. 375–382.
            [8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method
            for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
            311–318.
            [9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
            Initiatives in Natural Language Processing. Budapest, Hungary: EACL,
            14 April 2003.
            [10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
            Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,
            M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.
            [11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
            Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.
            145–152.
            [12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
            in Summarization without Human Models,” in Empirical Methods in
            Natural Language Processing, Singapore, August 2009, pp. 306–314.
            [Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
            [13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
            Transactions on Information Theory, vol. 37, no. 145-151, 1991.
            [14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
            N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
            USA: Association for Computational Linguistics, 2003, pp. 71–78.
            [15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
            approach to automatic evaluation of summaries,” in HLT-NAACL,
            Morristown, USA, 2006, pp. 463–470.
            [16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. of
            Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.
            [17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral
            Sciences. McGraw-Hill, 1998.

            p-value
            p
            <
            0.05
            p
            <
            0.05
            p
            <
            0.10
            p
            <
            0.05

            ROUGE -SU4
            0.741
            0.680
            0.620
            0.740

            p-value
            p
            <
            0.01
            p
            <
            0.02
            p
            <
            0.05
            p
            <
            0.01

            [18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
            “A French Human Reference Corpus for multi-documents
            summarization and sentence compression,” in LREC’10, vol. 2,
            Malta, 2010, p. In press.
            [19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy
            of Associative Memories: performants applications of Enertex algorithm
            in text summarization and topic segmentation,” in MICAI’07, 2007, pp.
            861–871.
            [20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G. Meunier,
            “Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2,
            St Malo, France, 2002, pp. 723–734.
            [21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales,
            “Automatic summarization using terminological and semantic
            resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.
            [22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton
            appliqué au résumé automatique de texte,” in JADT’10. Rome, 2010,
            p. In press.
            [23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern
            systems of automatic text summarization,” Automatic Documentation
            and Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.
            [24] C. D. Manning and H. Schütze, Foundations of Statistical Natural
            Language Processing.
            Cambridge, Massachusetts: The MIT Press,
            1999.
            [25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM,
            vol. 43, no. 6, pp. 1449–1481, 2007.
            [26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specialized
            discourse: The case of medical articles in spanish,” Terminology, vol. 13,
            no. 2, pp. 249–286, 2007.
            [27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL
            Student Research Workshop.
            Toulouse, France: Association for
            Computational Linguistics, 9-11 July 2001 2001, pp. 49–54.
            [28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:
            Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,
            Singapore, August 2009, pp. 23–30.
            [29] K. Knight and D. Marcu, “Statistics-based summarization-step one:
            Sentence compression,” in Proceedings of the National Conference on
            Artificial Intelligence. Menlo Park, CA; Cambridge, MA; London;
            AAAI Press; MIT Press; 1999, 2000, pp. 703–710.
        </biblio>
    </article>
    <articles>
