==============================
ğŸ—‚ Fichier        : ACL2004-HEADLINE.txt
ğŸ“ Titre          : Hybrid Headlines: Combining Topics and Sentence Compression David Zajic, Bonnie Dorr, Stacy President     Richard Schwartz Department of Computer Science        BBN Technologies
ğŸ“„ RÃ©sumÃ©         : . The topics and sentence compressions are combined in a This paper presents Topiary, a headline manner that preserves the advantages of each apgeneration system that creates very proach: the fluency and event-oriented informashort, informative summaries for news tion from the lead sentence with the broader covstories by combining sentence compres erage of the topic models. sion and unsupervised topic discovery. The next section presents previous work in the We will show that the combination of area of automatic summarization. Following this linguistically motivated sentence com we describe Hedge Trimmer and Unsupervised pression with statistically selected topic Topic Discovery in more detail, and describe the terms performs better than either alone, algorithm for combining sentence compression according to some automatic summary with topics. Next we show that Topiary scores evaluation measures. In addition we de higher than either Hedge Trimmer or Unsuperscribe experimental results establishing vised Topic Discovery alone according to certain an appropriate extrinsic task on which to automatic evaluation tools for summarization. Fimeasure the effect of summarization on nally we propose event tracking as an extrinsic human performance. We demonstrate task using automatic summarization for measurthe usefulness of headlines in compar ing how human performance is affected by autoison to full texts in the context of this matic summarization, and for correlating human extrinsic task. peformance with automatic evaluation tools. We describe an experiment that supports event track-
ğŸ“Š Lignes totales : 461
âœ‚ï¸ Lignes rÃ©sumÃ©  : 21
ğŸ”  Longueur texte : 1601 caractÃ¨res
â± Temps analyse  : 0 ms

==============================
ğŸ—‚ Fichier        : Boudin-Torres-2006.txt
ğŸ“ Titre          : A Scalable MMR Approach to Sentence Scoring for Multi-Document Update Summarization Florian Boudin \ and Marc El-BeÌ€ze \             Juan-Manuel Torres-Moreno \,[
ğŸ“„ RÃ©sumÃ©         : (history) has to be removed from the extract. We present S MMR, a scalable sentence scoring method for query-oriented up A natural way to go about update summarization date summarization. Sentences are scored would be extracting temporal tags (dates, elapsed thanks to a criterion combining query rele times, temporal expressions...) (Mani and Wilson, vance and dissimilarity with already read 2000) or to automatically construct the timeline documents (history). As the amount of from documents (Swan and Allan, 2000). These data in history increases, non-redundancy temporal marks could be used to focus extracts on is prioritized over query-relevance. We the most recently written facts. However, most reshow that S MMR achieves promising re cently written facts are not necessarily new facts. sults on the DUC 2007 update corpus. Machine Reading (MR) was used by (Hickl et al., 2007) to construct knowledge representations
ğŸ“Š Lignes totales : 279
âœ‚ï¸ Lignes rÃ©sumÃ©  : 13
ğŸ”  Longueur texte : 926 caractÃ¨res
â± Temps analyse  : 0 ms

==============================
ğŸ—‚ Fichier        : compression.txt
ğŸ“ Titre          : Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasksâˆ— David Zajic1 , Bonnie J. Dorr1 , Jimmy Lin1 , Richard Schwartz2
ğŸ“„ RÃ©sumÃ©         : This article examines the application of two single-document sentence compression techniques to the problem of multi-document summarizationâ€”a â€œparse-and-trimâ€ approach and a statistical noisy-channel approach. We introduce the Multi-Candidate Reduction (MCR) framework for multi-document summarization, in which many compressed candidates are generated for each source sentence. These candidates are then selected for inclusion in the final summary based on a combination of static and dynamic features. Evaluations demonstrate that sentence compression is a valuable component of a larger multi-document summarization framework.
ğŸ“Š Lignes totales : 1368
âœ‚ï¸ Lignes rÃ©sumÃ©  : 7
ğŸ”  Longueur texte : 635 caractÃ¨res
â± Temps analyse  : 0 ms

==============================
ğŸ—‚ Fichier        : compression_phrases_Prog-Linear-jair.txt
ğŸ“ Titre          : Journal of Artificial Intelligence Research 31 (2008) 399-429    Submitted 09/07; published 03/08 Global Inference for Sentence Compression An Integer Linear Programming Approach
ğŸ“„ RÃ©sumÃ©         : Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models.
ğŸ“Š Lignes totales : 1644
âœ‚ï¸ Lignes rÃ©sumÃ©  : 7
ğŸ”  Longueur texte : 547 caractÃ¨res
â± Temps analyse  : 0 ms

==============================
ğŸ—‚ Fichier        : hybrid_approach.txt
ğŸ“ Titre          : Sentence Compression for Automated Subtitling: A Hybrid Approach Vincent Vandeghinste and Yi Pan Centre for Computational Linguistics
ğŸ“„ RÃ©sumÃ©         : . When subtitling, only when a sentence needs to be reIn this paper a sentence compression tool is deduced, and the amount of reduction is known, the scribed. We describe how an input sentence gets sentence is sent to the sentence compression tool. analysed by using a.o. a tagger, a shallow parser So the sentence compression tool is a module of an and a subordinate clause detector, and how, based automated subtitling tool. The output of the senon this analysis, several compressed versions of this tence compression tool needs to be processed acsentence are generated, each with an associated escording to the subtitling guidelines like (Dewulf and timated probability. These probabilities were estiSaerens, 2000), in order to be in the correct lay-out mated from a parallel transcript/subtitle corpus. To which makes it usable for actual subtitling. Manuavoid ungrammatical sentences, the tool also makes ally post-editing the subtitles will still be required, use of a number of rules. The evaluation was done as for some sentences no automatic compression is on three different pronunciation speeds, averaging generated. sentence reduction rates of 40% to 17%. The numIn real subtitling it often occurs that the sentences ber of reasonable reductions ranges between 32.9% are not compressed, but to keep the subtitles synand 51%, depending on the average estimated prochronized with the speech, some sentences are ennunciation speed. tirely removed. In section 2 we describe the processing of a sen-
ğŸ“Š Lignes totales : 429
âœ‚ï¸ Lignes rÃ©sumÃ©  : 33
ğŸ”  Longueur texte : 1506 caractÃ¨res
â± Temps analyse  : 0 ms

==============================
ğŸ—‚ Fichier        : marcu_statistics_sentence_pass_one.txt
ğŸ“ Titre          : 
ğŸ“„ RÃ©sumÃ©         : , compressing long sentences into shorter ones, aggregating When humans produce summaries of documents, they sentences, repairing reference links, etc. do not simply extract sentences and concatenate them. Rather, they create new sentences that are grammati Our goal is also to generate coherent abstracts. Howcal, that cohere with one another, and that capture the ever, in contrast with the above work, we intend to most salient pieces of information in the original doc eventually use Abstract, Text tuples, which are widely ument. Given that large collections of text/abstract available, in order to automatically learn how to rewrite pairs are available online, it is now possible to envision Texts as coherent Abstracts. In the spirit of the work algorithms that are trained to mimic this process. In in the statistical MT community, which is focused on this paper, we focus on sentence compression, a sim sentence-to-sentence translations, we also decided to fopler version of this larger challenge. We aim to achieve cus ï¬rst on a simpler problem, that of sentence comprestwo goals simultaneously: our compressions should be sion. We chose this problem for two reasons: grammatical, and they should retain the most important pieces of information. These two goals can con â€¢ First, the problem is complex enough to require the ï¬‚ict. We devise both noisy-channel and decision-tree development of sophisticated compression models: approaches to the problem, and we evaluate results Determining what is important in a sentence and against manual compressions and a simple baseline. determining how to convey the important information grammatically, using only a few words, is just a
ğŸ“Š Lignes totales : 542
âœ‚ï¸ Lignes rÃ©sumÃ©  : 20
ğŸ”  Longueur texte : 1694 caractÃ¨res
â± Temps analyse  : 0 ms

==============================
ğŸ—‚ Fichier        : mikheev.txt
ğŸ“ Titre          : Periods, Capitalized Words, etc. Andrei Mikheevâˆ— University of Edinburgh
ğŸ“„ RÃ©sumÃ©         : 
ğŸ“Š Lignes totales : 1719
âœ‚ï¸ Lignes rÃ©sumÃ©  : 0
ğŸ”  Longueur texte : 0 caractÃ¨res
â± Temps analyse  : 1 ms

==============================
ğŸ—‚ Fichier        : probabilistic_sentence_reduction.txt
ğŸ“ Titre          : Probabilistic Sentence Reduction Using Support Vector Machines Minh Le Nguyen, Akira Shimazu, Susumu Horiguchi Bao Tu Ho and Masaru Fukushi
ğŸ“„ RÃ©sumÃ©         : (Lin 03), the best sentence reduction output for a single sentence is This paper investigates a novel application of supnot approximately best for text summarization. port vector machines (SVMs) for sentence reduction. This means that â€œlocal optimalâ€ refers to the We also propose a new probabilistic sentence reducbest reduced output for a single sentence, while tion method based on support vector machine learnthe best reduced output for the whole text is ing. Experimental results show that the proposed â€œglobal optimalâ€. Thus, it would be very valumethods outperform earlier methods in term of senable if the sentence reduction task could genertence reduction performance. ate multiple reduced outputs and select the best one using the whole text document. However,
ğŸ“Š Lignes totales : 442
âœ‚ï¸ Lignes rÃ©sumÃ©  : 17
ğŸ”  Longueur texte : 778 caractÃ¨res
â± Temps analyse  : 0 ms

==============================
ğŸ—‚ Fichier        : Stolcke_1996_Automatic_linguistic.txt
ğŸ“ Titre          : AUTOMATIC LINGUISTIC SEGMENTATION OF CONVERSATIONAL SPEECH Andreas Stolcke          Elizabeth Shriberg
ğŸ“„ RÃ©sumÃ©         : to help end-point a userâ€™s speech input. A speech indexing and reAs speech recognition moves toward more unconstrained domains trieval system (such as for transcribed broadcast audio) could prosuch as conversational speech, we encounter a need to be able to cess its data in more meaningful units if the locations of linguistic segment (or resegment) waveforms and recognizer output into linsegment boundaries were known. guistically meaningful units, such a sentences. Toward this end, we present a simple automatic segmenter of transcripts based on Our main motivation for the work reported here comes from speech N-gram language modeling. We also study the relevance of sev language modeling. Experiments at the 1995 Johns Hopkins Laneral word-level features for segmentation performance. Using only guage Modeling Workshop showed that the quality of a language word-level information, we achieve 85% recall and 70% precision model (LM) can be improved if both training and test data are segon linguistic boundary detection. mented linguistically, rather than acoustically [8]. We showed in [10] and [9] that proper modeling of filled pauses requires knowl-
ğŸ“Š Lignes totales : 258
âœ‚ï¸ Lignes rÃ©sumÃ©  : 14
ğŸ”  Longueur texte : 1162 caractÃ¨res
â± Temps analyse  : 0 ms

==============================
ğŸ—‚ Fichier        : Torres.txt
ğŸ“ Titre          : Summary Evaluation with and without References Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia VelaÌzquez-Morales
ğŸ“„ RÃ©sumÃ©         : (peer) has to be compared the evaluation of text summarization systems without with one or more reference summaries (models). DUC used human models which is used to produce system rankings. an interface called SEE to allow human judges to compare The research is carried out using a new content-based evaluation framework called F RESA to compute a variety of a peer with a model. Thus, judges give a C OVERAGE score divergences among probability distributions. We apply our to each peer produced by a system and the final system comparison framework to various well-established content-based C OVERAGE score is the average of the C OVERAGEâ€™s scores evaluation measures in text summarization such as C OVERAGE, asigned. These systemâ€™s C OVERAGE scores can then be used R ESPONSIVENESS, P YRAMIDS and ROUGE studying their to rank summarization systems. In the case of query-focused associations in various text summarization tasks including generic multi-document summarization in English and French, summarization (e.g. when the summary should answer a focus-based multi-document summarization in English and question or series of questions) a R ESPONSIVENESS score generic single-document summarization in French and Spanish. is also assigned to each summary, which indicates how Index Termsâ€”Text summarization evaluation, content-based responsive the summary is to the question(s). evaluation measures, divergences. Because manual comparison of peer summaries with model summaries is an arduous and costly process, a body of I. I NTRODUCTION research has been produced in the last decade on automatic content-based evaluation procedures. Early studies used text
ğŸ“Š Lignes totales : 522
âœ‚ï¸ Lignes rÃ©sumÃ©  : 18
ğŸ”  Longueur texte : 1669 caractÃ¨res
â± Temps analyse  : 0 ms

==============================
ğŸ—‚ Fichier        : Word2Vec.txt
ğŸ“ Titre          : Efficient Estimation of Word Representations in Vector Space Tomas Mikolov                   Kai Chen
ğŸ“„ RÃ©sumÃ©         : We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.
ğŸ“Š Lignes totales : 656
âœ‚ï¸ Lignes rÃ©sumÃ©  : 8
ğŸ”  Longueur texte : 644 caractÃ¨res
â± Temps analyse  : 0 ms

