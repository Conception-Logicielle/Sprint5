<articles>
    <article>
        <preamble>compression.txt</preamble>
        <titre>Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks∗</titre>
        <auteur>David Zajic1 , Bonnie J. Dorr1 , Jimmy Lin1 , Richard Schwartz2 1</auteur>
        <abstract>This article examines the application of two single-document sentence compression techniques to the
            problem of multi-document summarization—a “parse-and-trim” approach and a statistical noisy-channel
            approach. We introduce the Multi-Candidate Reduction (MCR) framework for multi-document summarization, in
            which many compressed candidates are generated for each source sentence. These candidates are then selected
            for inclusion in the final summary based on a combination of static and dynamic features. Evaluations
            demonstrate that sentence compression is a valuable component of a larger multi-document summarization
            framework. Keywords: headline generation, summarization, parse-and-trim, Hidden Markov Model PACS:
            Artificial intelligence, 07.05.Mh; Computer science and technology, 89.20.Ff; Spoken languages, processing
            of, 43.71.Sy 1
        </abstract>
        <introduction>This article presents an application of two different single-document sentence compression methods
            to the problem of multi-document summarization. The first, a “parse-and-trim” approach, has been implemented
            in a system called Trimmer and its extended version called Topiary. The second, an HMM-based approach, has
            been implemented in a system called HMM Hedge. These systems share the basic premise that a textual summary
            can be constructed by selecting a subset of words, in order, from the original text, with morphological
            variation allowed for some word classes. Trimmer selects subsequences of words using a
            linguistically-motivated algorithm to trim syntactic constituents from sentences until a desired length has
            been reached. In the context of ∗ Please cite as: David Zajic, Bonnie Dorr, Jimmy Lin, and Richard Schwartz.
            Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks. Information
            Processing and Management, 43(6):1549-1570, 2007. [DOI: 10.1016/j.ipm.2007.01.016] This is the pre-print
            version of a published article. Citations to and quotations from this work should reference that
            publication. If you cite this work, please check that the published form contains precisely the material to
            which you intend to refer. (Received 18 July 2006; revised 3 January 2007; accepted 8 January 2007) This
            document prepared August 31, 2007, and may have minor differences from the published version.
        </introduction>
        <corps>2007) This document prepared August 31, 2007, and may have minor differences from the published version.

            1

            Documents

            Sentence
            Filtering

            Sentences

            Sentence
            Compression
            Candidates
            Summary

            Sentence
            Selection

            Task-Specific Features
            (e.g., query)

            Figure 1: The Multi-Candidate Reduction (MCR) framework.
            very short single-document summarization, or headline generation, this approach is applied to the lead
            sentence (the first non-trivial sentence) in order to produce a short headline tens of characters long.
            HMM Hedge (Hidden Markov Model HEaDline GEnerator), on the other hand, is a statistical headline
            generation system that finds the subsequence of words most likely to be a headline for a given story.
            This approach is similar to techniques used in statistical machine translation in that the observed story
            is treated as the garbled version of an unseen headline transmitted through a noisy channel. In their
            original configurations, both Trimmer and HMM Hedge create sentence compressions that mimic the
            style of Headlinese, a form of compressed English associated with newspaper headlines (Mårdh, 1980).
            The goal of our work is to transition from a single-document summarization task with sentencelevel
            character-based length constraint to a multi-document summarization task with summary-level
            word-based length constraint. In headline generation, summaries should fill as much of the available
            space as possible, without going over the limit. Headlines that fall below the length limit miss the
            opportunity to include additional information. Headlines that exceed the length limit must be truncated,
            resulting in ungrammatical sentences and loss of information. When the length constraint is
            applied to a collection of sentences instead of a single sentence, concerns about individual sentence
            length become less important. A short sentence does not result in wasted space and it is acceptable to
            include a lengthy sentence as long as it includes important, non-redundant information.
            To transition from single-document to multi-document summarization, we examined different combinations of
            possible sentence compressions to construct the best summary. A key innovation of this
            work—tested for the first time in 2005 at the Document Understanding Conference (DUC-2005) (Zajic
            et al., 2005b) and Multilingual Summarization Evaluation (MSE-2005) (Zajic et al., 2005a)—is the
            use of multiple compressed candidates for sentences in the source documents. Sentence compression is
            used for different purposes in single- and multi-document summarization tasks. In the first case, the
            goal is to fit long sentences into the available space while preserving important information. In
            multidocument summarization, sentence compression is used to generate multiple candidates that capture
            relevant, non-redundant information. Sentence selection algorithms can then be applied to determine
            which compressed candidates provide the best combination of topic coverage and brevity.
            We introduce a framework for extractive multi-document summarization called Multi-Candidate
            Reduction (MCR), whose architecture is shown in Figure 1. Sentence compression techniques are employed in
            the intermediate stage of the processing pipeline to generate multiple compressed variants of
            source sentences. A sentence selector then builds the final summary by choosing among these candidates,
            based on features propagated from the sentence compression method, features of the candidates

            2

            themselves, and features of the present summary state. Under this framework, our previously-developed
            systems, Trimmer and HMM Hedge, are employed to generate the compressed candidates. The MCR
            architecture also includes a filtering module that chooses the source sentences from which compressed
            candidates are generated. This work does not examine the filtering process in detail, as we only employ
            very simple approaches, e.g., retain first n sentences from each document.
            Sentence compression supports extractive multi-document summarization by reducing the length
            of summary candidates while preserving their relevant content, thus allowing space for the inclusion of
            additional material. However, given the interaction of relevance and redundancy in a multi-sentence
            summary, it is unlikely that a single algorithm or scoring metric can provide the “best” compression
            of a sentence. This is the motivation for MCR, which provides several alternative candidates for each
            source sentence. Subsequent processes can then select among many alternatives when constructing the
            final summary.
            This article is organized as follows: The next section relates our approach to other existing summarization
            systems. In Section 3, we describe Trimmer, a variant of Trimmer called Topiary, and HMM
            Hedge. These systems implement two fundamentally different approaches to sentence compression.
            Section 4 describes how these tools can be applied to a multi-document summarization task. Section 5
            describes evaluations of our framework. Our systems have also been applied to several different types
            of texts; some of these applications will be briefly covered in Section 6. Finally, we propose future work
            in the area before concluding.

            2

            Related Work

            We have developed a “parse-and-trim” approach to sentence compression, implemented in a system
            called Trimmer (Dorr et al., 2003b). The system generates a headline for a news story by compressing
            the lead (or main) topic sentence according to a linguistically-motivated algorithm that operates on
            syntactic structures. Syntactic approaches to compression have been used in single-document summarization
            systems such as Cut-and-Paste (Jing and McKeown, 2000) and also in multi-document
            summarization systems such as SC (Blair-Goldensohn et al., 2004) and CLASSY (Conroy et al., 2005;
            Conroy et al., 2006). The SC system pre-processes input to remove appositives and relative clauses.
            CLASSY uses an HMM sentence selection approach combined with a conservative sentence compression
            method based on shallow parsing to detect lexical cues to trigger phrase eliminations. The approach
            taken by Trimmer is most similar to that of Jing and McKeown (2000), in which algorithms for removing
            syntactic constituents from sentences are informed by analysis of human summarizers. Trimmer
            differs from these systems in that multiple compressed candidates of each sentence are generated. The
            potential of multiple alternative compressions has also been explored by Vanderwende et al. (2006).
            Summaries may also contain lists of words or short phrases that denote important topics or concepts
            in the document. In particular, extractive topic summaries consist of keywords or key phrases that
            occur in the document. We have built Topiary, an extension to Trimmer, that constructs headlines
            combining compressed sentences with topic terms. This approach is similar to the work of Euler (2002),
            except that Euler uses topic lists to guide sentence selection and compression toward a query-specific
            summary, whereas Topiary uses topics to augment the concept coverage of a generic summary. Our
            system was demonstrated to be the top-performing single-document summarization system in the
            DUC-2004 evaluation (Zajic et al., 2004).
            In contrast to Topiary, which combines sentence compression with topic terms, others have constructed
            summaries directly from topic terms. For example, Bergler et al. (2003) choose noun phrases
            that represent the most important entities as determined by noun phrase coreference chains. Wang et
            al. (2005) propose a baseline system that constructs headlines from topic descriptors identified using
            term frequency counts; this system was reported to outperform LexTrim, their Topiary-style system.
            Zhou and Hovy (2003) construct fluent summaries from a topic list by finding phrase clusters early in
            3

            the text that contain important topic words found throughout the text.
            The task of assigning topic terms to documents is related to text categorization, in which documents
            are assigned to pre-defined categories. The categories can be labeled with topic terms, so that the
            decision to put a document in a category is equivalent to assigning that category’s label to the document.
            Assigning topic terms to documents by categorization permits the assignment of terms that do not occur
            in the document. Lewis (1999) describes a probabilistic feature-based method for assigning Reuters
            topics to news stories. Along these lines, OnTopicTM (Schwartz et al., 1997) uses an HMM to assign
            topics to a document.
            In addition to Trimmer, we have implemented HMM Hedge (Hidden Markov Model HEaDline
            GEnerator), a statistical headline generation system that finds the most likely headline for a given story.
            This approach is similar to techniques used in statistical machine translation in that the observed story
            is treated as the garbled version of an unseen headline transmitted through a noisy channel. The
            noisychannel approach has been used for a wide range of Natural Language Processing (NLP) applications
            including speech recognition (Bahl et al., 1983); machine translation (Brown et al., 1990); spelling
            correction (Mays et al., 1990); language identification (Dunning, 1994); and part-of-speech tagging
            (Cutting et al., 1992). Of those who have taken a noisy-channel approach to sentence compression
            for text summarization Banko et al. (2000), Knight and Marcu (2000), Knight and Marcu (2002),
            and Turner and Charniak (2005) have used the technique to find the most probable short string that
            generated the observed full sentence.
            Like other summarization systems based on the noisy-channel model, HMM Hedge treats the observed data (the
            story) as the result of unobserved data (headlines) that have been distorted by transmission through a noisy
            channel. The effect of the noisy channel is to add story words between the
            headline words. Our approach differs from Knight and Marcu (2000), Banko et al. (2000), and Turner
            and Charniak (2005), in that HMM Hedge does not require a corpus of paired stories and summaries.
            HMM Hedge uses distinct language models of news stories and headlines, but does not require explicit
            pairings of stories and summaries.
            To transition our single-sentence summarization techniques to the problem of multi-document summarization,
            we must consider how to select candidates for inclusion in the final summary. A common
            approach is to rank candidate sentences according to a set of features and iteratively build the summary,
            appropriately re-ranking the candidates at each step to avoid redundancy. MEAD (Radev et al.,
            2004) scores source sentences according to a linear combination of features including centroid, position,
            and first-sentence overlap. These scores are then refined to consider cross-sentence dependencies, such
            as redundancy, chronological order, and source preferences. MCR differs in that multiple variants of
            the same source sentences are available as candidates for inclusion in the final summary.
            Minimization of redundancy is an important element of a multi-document summarization system.
            Carbonell and Goldstein (1998) propose a technique called Maximal Marginal Relevance (MMR) for
            ranking documents returned by an information retrieval system so that the front of the ranked list will
            contain diversity as well as high relevance. Goldstein et al. (2000) extend MMR to multi-document
            summarization. MCR borrows the ranking approach of MMR, but uses a different set of features. Like
            MEAD, these approaches use feature weights that are optimized to maximize an automatic metric on
            training data.
            Several researchers have shown the importance of summarization in domains other than written
            news (Muresan et al., 2001; Clarke and Lapata, 2006). Within the MCR framework, we discuss the
            portability of Trimmer and HMM Hedge to a variety of different texts: written news, broadcast news
            transcriptions, email threads, and text in foreign language.

            4

            3

            Single-Sentence Compression

            Our general approach to the generation of a summary from a single document is to produce a headline
            by selecting words in order from the text of the story. Consider the following excerpt from a news story
            and corresponding headline:
            (1)

            (i)

            (ii)

            After months of debate following the Sept. 11 terrorist hijackings, the Transportation
            Department has decided that airline pilots will not be allowed to have guns in the
            cockpits.
            Pilots not allowed to have guns in cockpits.

            The bold words in (1i) form a fluent and accurate headline, as shown in (1ii).
            This basic approach has been realized in two ways. The first, Trimmer, uses a linguisticallymotivated
            algorithm to remove grammatical constituents from the lead sentence until a length threshold
            is met. Topiary is a variant of Trimmer that combines fluent text from a compressed sentence with
            topic terms to produce headlines. The second, HMM Hedge, employs a noisy-channel model to find
            the most likely headline for a given story. The remainder of this section will present Trimmer, Topiary,
            and HMM Hedge in more detail.

            3.1

            Trimmer

            Our first approach to sentence compression involves iteratively removing grammatical constituents from
            the parse tree of a sentence using linguistically-motivated rules until a length threshold has been met.
            When applied to the lead sentence, or first non-trivial sentence of a story, our algorithm generates a
            very short summary, or headline. This idea is implemented in our Trimmer system, which can leverage
            the output of any constituency parser that uses the Penn Treebank conventions. At present we use
            Charniak’s parser (Charniak, 2000).
            The insights that form the basis and justification for the Trimmer rules come from our previous
            study, which compared the relative prevalence of certain constructions in human-written summaries
            and lead sentences in stories. This study used 218 human-written summaries of 73 documents from
            the TIPSTER corpus (Harman and Liberman, 1993) dated January 1, 1989. The 218 summaries and
            the lead sentences of the 73 stories were parsed using the BBN SIFT parser (Miller et al., 2000). The
            parser produced 957 noun phrases (NP nodes in the parse trees) and 315 clauses (S nodes in the parse
            trees) for the 218 summaries. For the 73 lead sentences, the parser produced 817 noun phrases and
            316 clauses.
            At each level (sentence, clause, and noun phrase), different types of linguistic phenomena were
            counted.
            • At the sentence level, the numbers of preposed adjuncts, conjoined clauses, and conjoined verb
            phrases were counted. Children of the root S node that occur to the left of the first NP are
            considered to be preposed adjuncts. The bracketed phase in “[According to police] the crime rate
            has gone down” is a prototypical example of a preposed adjunct.
            • At the clause level, temporal expressions, trailing SBAR nodes, and trailing PP nodes were
            counted. Trailing constituents are those not designated as an argument of a verb phrase.
            • At both the sentence and clause levels, conjoined S nodes and conjoined VP nodes were counted.
            • At the NP level, determiners and relative clauses were counted.
            The counts and prevalence of the phenomena in the human-generated headlines and lead sentences
            are shown in Table 1. The results of this analysis illuminated the opportunities for trimming constituents
            and guided the development of our Trimmer rules, detailed below.
            5

            Level
            Sentence

            Clause

            Noun Phrase

            Phenomenon
            preposed adjuncts
            conjoined S
            conjoined VP
            temporal expression
            trailing PP
            trailing SBAR
            relative clause
            determiner

            Summary
            0/218
            0%
            1/218
            0.5%
            7/218
            3%
            5/315
            1.5%
            165/315 52%
            24/315
            8%
            3/957
            0.3%
            31/957
            3%

            Lead Sentence
            2/73
            2.7%
            3/73
            4%
            20/73
            27%
            77/316
            24%
            184/316 58%
            49/316
            16%
            29/817
            3.5%
            205/817 25%

            Table 1: Counts and prevalence of phenomena found in summaries and lead sentences.
            3.1.1

            Trimmer Algorithm

            Trimmer applies syntactic compression rules to a parse tree according the following algorithm:
            1. Remove temporal expressions
            2. Select Root S node
            3. Remove preposed adjuncts
            4. Remove some determiners
            5. Remove conjunctions
            6. Remove modal verbs
            7. Remove complementizer that
            8. Apply the XP over XP rule
            9. Remove PPs that do not contain named entities
            10. Remove all PPs under SBARs
            11. Remove SBARSs
            12. Backtrack to state before Step 9
            13. Remove SBARs
            14. Remove PPs that do not contain named entities
            15. Remove all PPs
            Steps 1 and 4 of the algorithm remove low-content units from the parse tree.
            Temporal expressions—although certainly not content-free—are not usually vital for summarizing
            the content of an article. Since the goal is to provide an informative headline, the identification and
            elimination of temporal expressions (Step 1) allow other more important details to remain in the
            lengthconstrained headline. The use of BBN’s IdentiFinderTM (Bikel et al., 1999) for removal of temporal
            expressions is described in Section 3.1.2.
            The determiner rule (Step 4) removes leaf nodes that are assigned the part-of-speech tag DT and
            have the surface form the, a, or an. The intuition for this rule is that the information carried by
            6

            articles is expendable in summaries, even though this makes the summaries ungrammatical for general
            English. Omitting articles is one of the most salient features of newspaper headlines. Sentences (2)
            and (3), taken from the New York Times website on September 27, 2006, illustrate this phenomenon.
            The italicized articles did not occur in the actual newspaper headlines.
            (2)

            The Gotti Case Ends With a Mistrial for the Third Time in a Year

            (3)

            A Texas Case Involving Marital Counseling Is the Latest to Test the Line Between Church and
            State

            Step 2 identifies nodes in the parse tree of a sentence that could serve as the root of a compression
            for the sentence. Such nodes will be referred to as Root S nodes. A node in a tree is a Root S node if it
            is labeled S in the parse tree and has children labeled NP and VP, in that order. The human-generated
            headlines we studied always conform to this rule. It has been adopted as a constraint in the Trimmer
            algorithm that the lowest leftmost Root S node is taken to be the root node of the headline. An
            example of this rule application is shown in (4). The boldfaced material in the parse is retained and
            the italicized material is eliminated.
            (4)

            (i)

            Input: Rebels agreed to talks with government officials, international observers said Tuesday.

            (ii)

            Parse: [S [S [NP Rebels][VP agreed to talks with government officials]], international observers said
            Tuesday.]

            (iii) Output: Rebels agreed to talks with government officials.

            When the parser produces a usable parse tree, this rule selects a valid starting point for compression.
            However, the parser sometimes produces incorrect output, as in the cases below (from DUC-2003):
            (5)

            (i)

            Parse: [S[SBAR What started as a local controversy][VP has evolved into an
            international scandal.]]

            (ii)

            Parse: [NP[NP Bangladesh][CC and][NP[NP India][VP signed a water sharing
            accord.]]]

            In (5i), an S exists, but it does not conform to the requirements of the Root S rule because it does
            not have as children an NP followed by a VP. The problem is resolved by selecting the lowest leftmost
            S, ignoring the constraints on the children. In (5ii), no S is present in the parse. This problem is
            resolved by selecting the root of the entire parse tree as the root of the headline. These parsing errors
            occur infrequently—only 6% of the sentences in the DUC-2003 evaluation data exhibit these problems,
            based on parses generated by the BBN SIFT parser.
            The motivation for removing preposed adjuncts (Step 3) is that all of the human-generated headlines
            omit what we refer to as the preamble of the sentence. Preposed adjuncts are constituents that precede
            the first NP (the subject) under the Root S chosen in Step 2; the preamble of a sentence consists of its
            preposed adjuncts. The impact of preposed adjunct removal can be seen in example (6).
            (6)

            (i)

            Input: According to a now finalized blueprint described by U.S. officials and other sources,
            the Bush administration plans to take complete, unilateral control of a post-Saddam Hussein
            Iraq.

            (ii)

            Parse: [S[PP According to a now finalized blueprint described by U.S. officials and other
            sources], [Det the] Bush administration plans to take complete, unilateral control
            of[Det a] post-Saddam Hussein Iraq.]

            7

            (iii) Output: Bush administration plans to take complete unilateral control of post-Saddam
            Hussein Iraq.
            The remaining steps of the algorithm remove linguistically peripheral material through successive
            deletions of constituents until the sentence is shorter than a length threshold. Each stage of the
            algorithm corresponds to the application of one of the rules. Trimmer first finds the pool of nodes in
            the parse to which a rule can be applied. The rule is then iteratively applied to the deepest, rightmost
            remaining node in the pool until the length threshold is reached or the pool is exhausted. After a rule
            has been applied at all possible nodes in the parse tree, the algorithm moves to the next step.
            In the case of a conjunction with two children (Step 5), one of the children will be removed. If the
            conjunction is and , the second child is removed. If the conjunction is but, the first child is removed.
            This rule is illustrated by the following examples, where the italicized text is trimmed.
            (7)

            When Sotheby’s sold a Harry S Truman signature that turned out to be a reproduction, the
            prestigious auction house apologized and bought it back .

            (8)

            President Clinton expressed sympathy after a car-bomb explosion in a Jerusalem market wounded
            24 people but said the attack should not derail the recent land-for-security deal between Israel
            and the Palestinians.

            The modal verb rule (Step 6) applies to verb phrases in which the head is a modal verb and the
            head of the child verb phrase is a form of have or be. In such cases, the modal and auxiliary verbs
            are removed. Sentences (9) and (10) show examples of this rule application. Note that although in
            Sentence (10) the omission of trimmed material changes the meaning, given a tight space constraint,
            the loss of the modality is preferable to the loss of other content information.
            (9)

            People’s palms and fingerprints may be used to diagnose schizophrenia.

            (10) Agents may have fired potentially flammable tear gas cannisters.
            The complementizer rule (Step 7) removes the word that when it occurs as a complementizer.
            Sentence (11) shows an example in which two complementizers can be removed.
            (11) Hoffman stressed that study is only preliminary and can’t prove that treatment useful.
            The XP-over-XP rule (Step 8) is a linguistic generalization that allows a single rule to cover two
            different phenomena. XP in the name of the rule is a variable that can take two values: NP and VP.
            In constructions of the form [XP [XP ...] ...], the other children of the higher XP are removed. Note
            that the child XP must be the first child of the parent XP. When XP = NP the rule removes relative
            clauses (as in Sentence (12)) and appositives (as in Sentence (13)).
            (12) Schizophrenia patients whose medication couldn’t stop the imaginary voices in their heads gained
            some relief.
            (13) A team led by Dr. Linda Brzustowicz, assistant professor of neuroscience at Rutgers University’s
            Center for Molecular and Behavioral Neuroscience in Newark, studied DNA of dozens of members
            of 22 families.
            The rules that remove prepositional phrases and subordinate clauses (Steps 9 through 15) are
            sometimes prone to removing important content. Thus, these rules are applied last, only when there
            are no other types of rules to apply. Moreover, these rules are applied with a backoff option to avoid

            8

            over-trimming the parse tree. First, the PP rule is applied (Steps 9 and 10),1 followed by the SBAR
            rule (Step 11). If the desired sentence length has not been reached, the system reverts to the parse tree
            as it was before any prepositional phrases were removed (Step 12) and applies the SBAR rule (Step
            13). If the desired length still has not been reached, the PP rule is applied again (Steps 14 and 15).
            The intuition behind this ordering is that, when removing constituents from a parse tree, it is preferable
            to remove smaller fragments before larger ones and prepositional phrases tend to be smaller than
            subordinate clauses. Thus, Trimmer first attempts to achieve the desired length by removing smaller
            constituents (PPs), but if this cannot be accomplished, the system restores the smaller constituents,
            removes a larger constituent, and then resumes the deletion of the smaller constituents. To reduce the
            risk of removing prepositional phrases that contain important information, BBN’s IdentiFinder is used
            to distinguish PPs containing temporal expressions and named entities, as described next.
            3.1.2

            Use of BBN’s IdentiFinder in Trimmer

            BBN’s IdentiFinder is used both for the elimination of temporal expressions and for conservative
            deletion of PPs containing named entities. The elimination of temporal expressions (Step 1) is a twostep
            process: (a) use IdentiFinder to mark temporal expressions; and (b) remove [PP ... [NP [X] ...]
            ...] and [NP [X]] where X is tagged as part of a temporal expression. The following examples illustrate
            the application of temporal expression removal rule:
            (14) (i)
            (ii)

            Input: The State Department on Friday lifted the ban it had imposed on foreign fliers.
            Parse: [S [NP[Det The] State Department [PP [IN on] [NP [NNP Friday]]] [VP lifted
            [Det the] ban it had imposed on foreign fliers.]]
            (iii) Output: State Department lifted ban it had imposed on foreign fliers.

            (15) (i)

            Input: An international relief agency announced Wednesday that it is withdrawing from
            North Korea.
            (ii) Parse: [S [NP [Det An] international relief agency][VP announced [NP [NNP Wednesday]] that it is
            withdrawing from North Korea.]]
            (iii) Output: International relief agency announced that it is withdrawing from North Korea.

            IdentiFinder is also used to ensure that prepositional phrases containing named entities are not
            removed during the first round of PP removal (Step 9). However, prepositional phrases containing
            named entities that are descendants of SBARs are removed before the parent SBAR is removed, since
            we should remove a smaller constituent before removing a larger constituent that subsumes it. Sentence
            (16) shows an example of a SBAR subsuming two PPs, one of which contains a named entity.
            (16) The commercial fishing restrictions in Washington will not be lifted [SBAR unless the salmon
            population increases [PP to a sustainable number] [PP in the Columbia River]].
            If the PP rule were not sensitive to named entities, the PP in the Columbia River would be the
            first prepositional phrase to be removed, because it is the lowest rightmost PP in the parse. However,
            this PP provides an important piece of information: the location of the salmon population. The rule in
            Step 9 will skip the last prepositional phrase and remove the penultimate PP to a sustainable number .
            This concludes an overview of the Trimmer rules and our syntactic sentence compression algorithm.
            Given a length limit, the system will produce a single compressed version of the target sentence.
            Multiple compressions can be generated by setting the length limit to be very small and storing the state
            of the sentence after each rule application as a compressed variant. In section 4, we will describe how
            multiple compressed candidates generated by Trimmer are used as a component of a multi-document
            summarization system.
            1

            The reason for breaking PP removal into two stages is discussed in Section 3.1.2.

            9

            3.2

            Topiary

            We have used the Trimmer approach to compression in another variant of single-sentence summarization
            called Topiary. This system combines Trimmer with a topic discovery approach (described next) to
            produce a fluent summary along with additional context.
            The Trimmer algorithm is constrained to build a headline from a single sentence. However, it is
            often the case that no single sentence contains all the important information in a story. Relevant
            information can be spread over multiple sentences, linked by anaphora or ellipsis. In addition, the
            choice of lead sentence may not be ideal and our trimming rules are imperfect.
            On the other hand, approaches that construct headlines from lists of topic terms (Lewis, 1999;
            Schwartz et al., 1997) also have limitations. For example, Unsupervised Topic Discovery (UTD)—
            described below—rarely generates any topic terms that are verbs. Thus, topic lists are good at indicating
            the general subject but rarely give any direct indication of what events took place. Intuitively,
            we need both fluent text to tell what happened and topic terms to provide context.
            3.2.1

            Topic Term Generation: UTD and OnTopic

            OnTopic (Schwartz et al., 1997) uses an HMM to assign topics to a document; topic models are derived
            from an annotated corpus. However, it is often difficult to acquire such data, especially for a new genre
            or language. UTD (Sista et al., 2002) was developed to overcome this limitation: it takes as input a
            large unannotated corpus and automatically creates a set of topic models with meaningful names.
            The UTD algorithm has several stages. First, it analyzes the corpus to find multi-word sequences
            that can be treated as single tokens. It does this using two methods. One method is a minimum
            description length criterion that detects phrases that occur frequently relative to the individual words.
            The second method uses BBN’s IdentiFinder to detect multi-word names. These names are added to
            the text as additional tokens. They are also likely to be chosen as potential topic names. In the second
            stage of UTD, we find those terms (both single-word and multi-word) with high tf.idf. Only those
            topic names that occur as high-content terms in at least four different documents are kept. The third
            stage trains topic models corresponding to these topic terms. The modified Expectation Maximization
            procedure of BBN’s OnTopic system is used to determine which words in the documents often signify
            these topic names. This produces topic models. Fourth, these topic models are used to find the most
            likely topics for each document, which is equivalent to assigning the name of the topic model to the
            document as a topic term. This often assigns topics to documents where the topic name does not occur
            in the document text.
            We found, in various experiments (Sista et al., 2002), that the topic names derived by this procedure
            were usually meaningful and that the topic assignment was about as good as when the topics were
            derived from a corpus that was annotated by people. We have also used this procedure on different
            languages and shown the same behavior. Since UTD is unsupervised, it can run equally well on a new
            language, as long as the documents can be divided into strings that approximate words.
            The topic list in (17) was generated by UTD and OnTopic for a story about the FBI investigation
            of the 1998 bombing of the U.S. embassy in Nairobi.
            (17) BIN LADEN, EMBASSY, BOMBING, POLICE OFFICIALS, PRISON, HOUSE, FIRE, KABILA
            Topiary uses UTD to generate topic terms for the collection of documents to be summarized and
            uses OnTopic to assign the topic terms to the documents. The next section will describe how topic
            terms and sentence compressions are combined to form Topiary summaries.

            10

            3.2.2

            Topiary Algorithm

            As each Trimmer rule is applied to a sentence, the resulting state of the sentence is stored as a
            compressed variant of the source sentence. Topiary selects from the variants the longest one such
            that there is room to prepend the highest scoring non-redundant topic term. Suppose the highest
            scoring topic term is “terrorism” and the length threshold is 75 characters. To make room for the topic
            “terrorism”, the length threshold is lowered by 10 characters: 9 characters for the topic and 1 character
            as a separator. Thus, Topiary chooses the longest trimmed variant under 65 characters that does not
            contain the word “terrorism”, If there is no such candidate, i.e., all the trimmed variants contain the
            word terrorism, Topiary would consider the second highest scoring topic word, “bomb”. Topiary would
            select the longest trimmed variant under 70 characters that does not contain the word “bomb”. After
            Topiary has selected a trimmed variant and prepended a topic to it, it checks to see how much unused
            space remains under the threshold. Additional topic words are added between the first topic word and
            the compressed sentence until all space is exhausted.
            This process results in a headline that contains one or more main topics about the story and a
            short sentence that says what happened concerning them. The combination is often more concise than
            a fully fluent sentence and compensates for the fact that the information content from the topic and
            the compressed sentence do not occur together in any single sentence from the source text.
            As examples, sentences (18) and (19) are the outputs of Trimmer and Topiary, respectively, for the
            same story in which UTD selected the topic terms in (17).
            (18) FBI agents this week began questioning relatives of the victims
            (19) BIN LADEN, EMBASSY, BOMBING: FBI agents this week began questioning relatives
            By combining topics and parse-and-trim compression, Topiary achieved the highest score on the
            single-document summarization task (i.e., headline generation task) in DUC-2004 (Zajic et al., 2004).

            3.3

            HMM Hedge

            Our second approach to sentence compression, implemented in HMM Hedge, treats the observed data
            (the story) as the result of unobserved data (headlines) that have been distorted by transmission
            through a noisy channel. The effect of the noisy channel is to add story words between the headline
            words. The model is biased by parameters to make the resulting headlines more like Headlinese, the
            observed language of newspaper headlines created by copy editors.
            Formally, we consider a story S to be a sequence of N words. We want to find a headline H, a
            subsequence of words from S, that maximizes the likelihood that H generated the story S, or:
            argmaxH P (H|S)
            It is difficult to directly estimate P (H|S), but this probability can be expressed in terms of other
            probabilities that are easier to compute, using Bayes’ rule:
            (H)
            P (H|S) = P (S|H)P
            P (S)

            Since the goal is to maximize this expression over H, and P (S) is constant with respect to H, the
            denominator of the above expression can be omitted. Thus we wish to find:
            argmaxH P (S|H)P (H)
            Let H be a headline consisting of words h1 , h2 , ..., hn . Let the special symbols start and end represent
            the beginning and end of a headline, respectively. P (H) can be estimated using a bigram model of
            Headlinese:
            P (H) = P (h1 |start)P (h2 |h1 )...P (end|hn )
            11

            The bigram probabilities of the words in the headline language were computed from a corpus of
            English headlines taken from 242,918 AP newswire stories in the TIPSTER corpus. The headlines
            contain 2,848,194 words from a vocabulary of 88,627 distinct words.
            Given a story S and a headline H, the action of the noisy channel is to form S by adding non-headline
            words to H. Let G be the non-headline words added by the channel to the headline: g1 , g2 , ..., gm . For
            the moment, we assume that the headline words are transmitted through the channel with probability
            1. We estimate P (S|H), the probability that the channel added non-headline words G to headline H
            to form story S. This is accomplished using a unigram model of newspaper stories that we will refer
            to as the general language, in contrast to the headline language. Let Pgl (g) be the probability of
            non-headline word g in the general language and Pch (h) = 1 be the probability that headline word h
            is transmitted through the channel as story word h.
            P (S|H) = Pgl (g1 )Pgl (g2 )...Pgl (gm )Pch (h1 )Pch (h2 )...Pch (hn )
            = Pgl (g1 )Pgl (g2 )...Pgl (gm )
            The unigram probabilities of the words in the general language were computed from 242,918 English
            AP news stories in the TIPSTER corpus. The stories contain 135,150,288 words from a vocabulary of
            428,633 distinct words.
            The process by which the noisy channel generates a story from a headline can be represented by a
            Hidden Markov Model (HMM) (Baum, 1972). An HMM is a weighted finite-state automaton in which
            each state probabilistically emits a string. The simplest HMM for generating headlines consists of two
            states: an H state that emits words that occur in the headline and a G state that emits all the other
            words in the story.
            Since we use a bigram model of headlines, each state that emits headline words must “remember”
            the previously emitted headline word. If we did not constrain headline words to actually occur in the
            story, we would need an H state for each word in the headline vocabulary. However, because headline
            words are chosen from the story words, it is sufficient to have an H state for each story word. For any
            story, the HMM consists of a start state S, an end state E, an H state for each word in the story, a
            corresponding G state for each H state, and a state Gstart that emits words that occur before the first
            headline word in the story. An H state can emit only the word it represents. The corresponding G
            state remembers which word was emitted by its H state and can emit any word in the story language.
            A headline corresponds to a path through the HMM from S to E that emits all the words in the story
            in the correct order. In practice the HMM is constructed with states for only the first N words of the
            story, where N is a constant (60), or N is the number of words in the first sentence.2
            In example (1i), given earlier, the H states will emit the words in bold (pilots, not, allowed, to,
            have, guns, in, cockpits) and the G states will emit all the other words. The HMM will transition
            between the H and G states as needed to generate the words of the story. In the current example,
            the model will have states Start, Gstart , End, and 28 H states with 28 corresponding G states.3 The
            headline given in example (1ii) corresponds to the following sequence of states: Start, Gstart 17 times,
            Hpilots , Gpilots , Hnot , Gnot , Hallowed , Hto , Hhave , Hguns , Hin , Gin , Hcockpits , End. This path is
            not the
            only one that could generate the story in (1i). Other possibilities are:
            (20) (i)
            (ii)

            Transportation Department decided airline pilots not to have guns.
            Months of the terrorist has to have cockpits.

            2

            Limiting consideration of headline words to the early part of the story is justified in Dorr et al. (2003a)
            where it
            was shown that more than half of the headline words are chosen from the first sentence of the story. Other
            methods for
            selecting the window of story words are possible and will be explored in future research.
            3
            The subscript of a G state indicates the H state it is associated with, not the story word it emits. In the
            example,
            Gpilots emits story word will , Gnot emits story word be, and Gin emits story word the.

            12

            Although (20i) and (20ii) are possible headlines for (1i), the conditional probability of (20ii) given (1i)
            will be lower than the conditional probability of (20i) given (1i).
            The Viterbi algorithm (Viterbi, 1967) is used to select the most likely headline for a given story.
            We use length constraints to find the most likely headlines consisting of W words, where W ranges
            from 5 to 15. Multiple backpointers are used so that we can find the n most likely headlines at each
            length.
            HMM Hedge is enhanced by three additional decoding parameters to help the system choose outputs
            that best mimic actual headlines: a position bias, a clump bias, and a gap bias. The incorporation
            of these biases changes the score produced by the decoder from a probability to a relative desirability
            score. The three parameters were motivated by analysis of system output and their values were set by
            trial and error. A logical extension to this work would be to learn the best setting of these biases, e.g.,
            through Expectation Maximization.
            The position bias favors headlines that include words near the front of the story. This reflects our
            observations of human-constructed headlines, in which headline words tend to appear near the front
            of the story. The initial position bias p is a positive number less than one. The story word in the
            nth position is assigned a position bias of log(pn ). When an H state emits a story word, the position
            bias is added to the desirability score. Thus, words near the front of the story carry less of a position
            bias than words farther along. Note that this generalization often does not hold in the case of human
            interest and sports stories, which may start with a hook to get the reader’s attention, rather than a
            topic sentence.
            We also observed that human-constructed headlines tend to contain contiguous blocks of story
            words. Example (1ii), given earlier, illustrates this with the string “allowed to have guns”. The
            string bias is used to favor “clumpiness”. i.e., the tendency to generate headlines composed of clumps
            of contiguous story words. The log of the clump bias is added to the desirability score with each
            transition from an H state to its associated G state. With high clump biases, the system will favor
            headlines consisting of fewer but larger clumps of contiguous story words.
            The gap bias is used to disfavor headline “gappiness”, i.e., large gaps of non-headline words in the
            story between clumps of headline words. Although humans are capable of constructing fluent headlines
            by selecting widely spaced words, we observed that HMM Hedge was more likely to combine unrelated
            material by doing this. At each transition from a G state to an H state, corresponding to the end
            of a sequence of non-headline words in the story, a gap bias is applied that increases with the size of
            the gap between the current headline and the last headline word to be emitted. This can also be seen
            as a penalty for spending too much time in one G state. With high gap biases, the system will favor
            headlines with few large gaps.
            One characteristic difference between newspaper headline text and newspaper story text is that
            headlines tend to be in present tense while story sentences tend to be in the past tense. Past tense
            verbs occur more rarely in the headline language than in the general language. HMM Hedge mimics this
            aspect of Headlinese by allowing morphological variation between headline verbs and the corresponding
            story verbs. Morphological variation for verbs is achieved by creating an H state for each available
            variant of a story verb. These H states still emit the story verb but they are labeled with the variant.
            HMM Hedge can generate a headline in which proposes is the unobserved headline word that emits the
            observed story word proposed , even though proposes does not occur in the story.
            (21) (i)
            (ii)

            A group has proposed awarding $1 million in every general election to one randomly chosen
            voter.
            Group proposes awarding $1 million to randomly chosen voter.

            Finally, we constrain each headline to contain at least one verb. That is to say, we ignore headlines
            that do not contain at least one verb, no matter how desirable the decoding is.
            13

            Although we have described an application of HMM Hedge to blocks of story words without reference
            to sentence boundaries, it is also possible to use HMM as a single sentence compressor by limiting the
            block to the words in a single sentence.
            Also, as we will see shortly, multiple alternative compressions of a sentence may be generated with
            HMM Hedge. The Viterbi algorithm is capable of discovering n-best compressions of a window of
            story words and can be constrained to consider only paths that include a specific number of H states,
            corresponding to compressions that contain a specific number of words. We use HMM Hedge to generate
            55 compressions for each sentence by computing the five best headlines at each length, from 5 to 15
            words. In Section 4 we will describe how HMM Hedge is used as a component of a multi-document
            summarization system.

            4

            Multi-Document Summarization

            The sentence compression tools we developed for single-document summarization have been incorporated into
            our Multi-Candidate Reduction framework for multi-document summarization. MCR
            produces a textual summary from a collection of relevant documents in three steps. First, sentences
            are selected from the source documents for compression. The most important information occurs near
            the front of the stories, so we select the first five sentences of each document for compression. Second,
            multiple compressed versions of each sentence are produced using Trimmer or HMM Hedge to create a
            pool of candidates for inclusion in the summary. Finally, a sentence selector constructs the summary
            by iteratively choosing from the pool of candidates based on a linear combination of features until the
            summary reaches a desired length.
            At present, weights for the features are determined by manually optimizing on a set of training
            data to maximize the Rouge-2 recall score (Lin and Hovy, 2003), using Rouge version 1.5.5. A
            typical summarization task might call for the system to generate a 250-word summary from a couple
            of dozen news stories. These summaries may be query-focused, in the sense that the summaries should
            be responsive to a particular information need, or they may be generic, in that a broad overview of the
            documents is desired.
            Our sentence selector adopts certain aspects of Maximal Marginal Relevance (MMR) (Carbonell
            and Goldstein, 1998), an approach that attempts to balance relevance and anti-redundancy. In MCR’s
            selection module, the highest scoring sentence from the pool of eligible candidates is chosen for inclusion
            in the summary. Features that contribute to a candidate’s score can be divided into two types: dynamic
            and static. When a candidate is chosen, all other compressed variants of that sentence are eliminated.
            After a candidate is added to the summary, the dynamic features are re-computed, and the candidates
            are re-ranked. Candidates are added to the summary until the desired length is achieved. The ordering
            of candidates in the summary is the same as the order in which they were selected for inclusion. The
            final sentence of the summary is truncated if it causes the summary to exceed the length limit.

            4.1

            Static Features

            Static features are calculated before sentence selection begins and do not change during the process of
            summary construction:
            • Position. The zero-based position of the sentence in the document.
            • Sentence Relevance. The relevance score of the sentence to the query.
            • Document Relevance. The relevance score of the document to the query.
            • Sentence Centrality. The centrality score of the sentence to the topic cluster.
            14

            • Document Centrality. The centrality score of the document to the topic cluster.
            • Scores from the Compression Modules:
            – Trim rule application counts. For Trimmer-based MCR, the number of Trimmer rule instances applied to
            produce the candidate.
            – Negative Log Desirability. For HMM-based MCR, the relative desirability score of the
            candidate.
            We use the Uniform Retrieval Architecture (URA), University of Maryland’s software infrastructure
            for information retrieval tasks, to compute relevance and centrality scores for each compressed candidate.
            There are four such scores: the relevance score between a compressed sentence and the query,
            the relevance score between the document containing the compressed sentence and the query, the centrality
            score between a compressed sentence and the topic cluster, and the centrality score between
            the document containing the compressed sentence and the topic cluster. We define the topic cluster
            to be the entire collection of documents relevant to this particular summarization task. Centrality is
            a concept that quantifies how similar a piece of text is to all other texts that discuss the same general
            topic. We assume that sentences having higher term overlap with the query and sources more “central”
            to the topic cluster are preferred for inclusion in the final summary.
            The relevance score between a compressed sentence and the query is an idf-weighted count of
            overlapping terms (number of terms shared by the two text segments). Inverse document frequency
            (idf), a commonly-used measure in the information retrieval literature, roughly captures term salience.
            The idf of a term t is defined by log(N/ct ), where N is the total number of documents in a particular
            corpus and ct is the number of documents containing term t; these statistics were calculated from one
            year’s worth of LA Times articles. Weighting term overlap by inverse document frequency captures
            the intuition that matching certain terms is more important than matching others.
            Lucene, a freely-available off-the-shelf information retrieval system, is used to compute the three
            other scores. The relevance score between the document containing the compressed sentence and
            the query is computed using Lucene’s built-in similarity function. The centrality score between the
            compressed sentence and the topic cluster is the mean of the similarity between the sentence and each
            document comprising the cluster (once again, as computed by Lucene’s built-in similarity function).
            The document-cluster centrality score is also computed in much the same way, by taking the mean
            of the similarity of the particular document with every other document in the cluster. In order to
            obtain an accurate distribution of term frequencies to facilitate the similarity calculation, we indexed
            all relevant documents (i.e., the topic cluster) along with a comparable corpus (one year of the LA
            Times)—this additional text essentially serves as a background model for non-relevant documents.
            Some features are derived from the sentence compression modules used to generate candidates. For
            Trimmer, the rule application count feature of a candidate is the number of rules that were applied to
            a source sentence to produce the candidate. The rules are not presumed to be equally effective, so the
            rule application counts are broken down by rule type. For HMM Hedge, we use the relative desirability
            score calculated by the decoder, expressed as a negative log.
            The features discussed in this section are assigned to the candidates before summary generation
            begins and remain fixed throughout the process of summary sentence selection. The next section
            discusses how candidate features are assigned new values as summary geneneration proceeds.

            4.2

            Dynamic Features

            Dynamic features change during the process of sentence selection to reflect changes in the state of the
            summary as sentences are added.4 The dynamic features are:
            4
            At present the dynamic features are properties of the candidates, calculated with respect to the current
            summary
            state. There are no features directly relating to the amount of space left in the summary, so there is no
            mechanism that

            15

            • Redundancy. A measure of how similar the sentence is to the current summary.
            • Sentence-from-doc. The number of sentences already selected from the sentence’s document.
            The intuition behind our redundancy measure is that candidates containing words that occur much
            more frequently in the current state of the summary than they do in general English are redundant
            to the summary. We imagine that sentences in the summary are generated by the underlying word
            distribution of the summary rather than the distribution of words in the general language. If a sentence
            appears to have been generated by the summary rather than by the general language, we take it to
            be redundant to the summary. Suppose we have a summary about earthquakes. The presence in a
            candidate of words like earthquake, seismic, and Richter Scale, which have a high likelihood in the
            summary, will make us think that the candidate is redundant to the summary.
            To estimate the extent to which a candidate is more likely to have been generated by a summary
            than by the general language, we consider the probabilities of the words in the candidate. We estimate
            that the probability that a word w occurs in a candidate generated by the summary is
            P (w) = λP (w|D) + (1 − λ)P (w|C)
            where D is the summary, C is the general language corpus5 , λ is a parameter estimating the probability
            that the word was generated by the summary and (1−λ) is the probability that the word was generated
            by the general language. We have set λ = 0.3, as a general estimate of the portion of words in a text
            that are specific to the text’s topic. We estimate the probabilities by counting the words6 in the current
            summary and the general language corpus:
            P (w|D) =

            count of w in D
            size of D

            P (w|C) =

            count of w in C
            size of C

            We take the probability of a sentence to be the product of the probabilities of its words, so we calculate
            the probability that a sentence was generated by the summary, i.e. our redundancy metric, as:
            Y
            Redundancy(S) =
            λP (s|D) + (1 − λ)P (s|C)
            s∈S

            For ease of computation, we actually use log probabilities:
            X
            log(λP (s|D) + (1 − λ)P (s|C))
            s∈S

            Redundancy is a dynamic feature because the word distribution of the current summary changes with
            every iteration of the sentence selector.

            4.3

            Examples of System Output

            We applied our MCR framework to test data from the DUC-2006 evaluation (Dang and Harman, 2006).
            Given a topic description and a set of 25 documents related to the topic (drawn from AP newswire,
            the New York Times, and the Xinhua News Agency English Service), the system’s task was to create
            would affect the distribution of compressed candidates over the iterations of the sentence selector. This
            issue will be
            addressed as future work in Section 7.
            5
            The documents in the set being summarized are used to estimate the general language model.
            6
            Actually, preprocessing for redundancy includes stopword removal and applying the Porter Stemmer (Porter,
            1980).

            16

            Title: Native American Reservation System—pros and cons
            Narrative Description: Discuss conditions on American Indian reservations or among Native
            American communities. Include the benefits and drawbacks of the reservation system. Include
            legal privileges and problems.
            Figure 2: Topic D0601A from the DUC-2006 multi-document summarization task.
            a 250-word summary that addressed the information need expressed in the topic. One of the topic
            descriptions is shown in Figure 2. The 25 documents in the document set have an average size of 1170
            words, so a 250-word summary represents a compression ratio of 0.86%.
            Figures 3, 4 and 5 show examples of MCR output using Trimmer compression, HMM Hedge compression, or no
            compression. For readability, we use ◦ as a sentence delimiter; this is not part of the
            actual system output. The sentences compressed by Trimmer mimic Headlinese by omitting determiners and
            auxiliary verbs. For example, the first sentence in Figure 3 is a compression of the following
            source sentence:
            Seeking to get a more accurate count of the country’s American Indian population, the
            Census Bureau is turning to tribal leaders and residents on reservations to help overcome
            long-standing feelings of wariness or anger toward the federal government.
            Three determiners and a form of be have been removed from the source sentence in the compression
            that appears in the summary. The removal of this material makes the sentence appear more like a
            headline.
            In comparison with Trimmer compressions, HMM compressions are generally less readable and
            more likely to be misleading. Consider the final sentence in Figure 4.
            (22) main purpose of reservation to pay American Indians by poverty proposals
            This is a compression of the following source sentence:
            (23) But the main purpose of the visit—the first to a reservation by a president since Franklin
            Roosevelt—was simply to pay attention to American Indians, who are so raked by grinding
            poverty that Clinton’s own advisers suggested he come up with special proposals geared specifically to the
            Indians’ plight.
            Because HMM Hedge uses a bigram model of Headlines, it is unable to capture sentence-level
            grammaticality. The same limitation makes it difficult to prevent misleading or incorrect compressions.
            For example, the third sentence from the end of Figure 4 seems to say that a court legalized gambling
            on Indian reservations:
            (24) Supreme Court allows legalized gambling Indian reservations
            However, it is a compression of the following source sentence:
            (25) Only Monday, the California Supreme Court overturned a ballot measure that would have allowed
            expansion of legalized gambling on Indian reservations.
            Nevertheless, we can see from the examples that sentence compression allows a summary to include
            more material from other sources. This increases the topic coverage of system output.
            17

            Seeking to get more accurate count of country’s American Indian population, Census Bureau turning to
            tribal leaders and residents on reservations to help overcome long-standing feelings. ◦ American Indian
            reservations would get infusion. ◦ Smith and thousands seeking help for substance abuse at American
            Indian Community House, largest of handful of Native American cultural institutions. ◦ Clinton going
            to Pine Ridge Reservation for visit with Oglala Sioux nation and to participate in conference on Native
            American homeownership and economic development. ◦ Said Glen Revere, nutritionist with Indian Health
            Services on 2.8 million-acre Wind River Reservation, about 100 miles east of Jackson, Wyo. “Then we
            came up with idea for this community garden, and it been bigger than we ever expected.” ◦ Road leading
            into Shinnecock Indian reservation is not welcoming one But main purpose of visit – first to reservation
            by president since Franklin Roosevelt – was simply to pay attention to American Indians, who raked by
            grinding poverty Clinton’s own advisers suggested he come up with special proposals geared specifically to
            Indians’ plight. ◦ “This highlights what going on out there, since beginning of reservation system,” said
            Sidney Harring, professor at City University of New York School of Law and expert on Indian crime and
            criminal law. ◦ American Indians are victims. ◦ President Clinton turned attention to arguably poorest,
            most forgotten ◦ U.S. citizens: American Indians. ◦ When American Indians began embracing gambling,
            Hualapai tribe moved quickly to open casino. ◦ members of Northern Arapaho Tribe on Wind River
            Reservation started seven-acre community garden with donated land, seeds and

            Figure 3: MCR Summary for DUC-2006 Topic D0601A, using Trimmer for sentence compression.

            David Rocchio deputy legal counsel to Vermont Gov. Howard Dean who has been involved in discussions
            on Indian gambling through the National Governors Association said that the concern that governors have
            is not with the benefit casinos bring to tribes ◦ Native Americans living on reservations that maintain 50
            percent or more unemployment are exempt from the national five year family limit on welfare benefits ◦
            Smith and thousands like her are seeking help for their substance abuse at the American Indian Community
            House the largest of a handful of Native American cultural institutions in the New York area ◦ Juvenile
            crime is one strand in the web of social problems facing urban and reservation Indian communities the
            report said ◦ Soldierwolf’s family represents the problems that plague many of the 1.3 million American
            Indians who live on reservations of whom 49 percent are unemployed ◦ Powless said the Onondaga people
            want to work with the community outside the reservation to improve the economy of the region perhaps
            creating tourism destinations that might include Indian culture or setting up a free trade zone at unused
            manufacturing sites ◦ As Indian communities across the nation struggle with short funds and a long list
            of problems they are watching the Navajo Nation’s legal battle with the federal government ◦ recognize
            Indians not only Native Americans as Americans ◦ go on reservation system Harring Indian ◦ Supreme
            Court allows legalized gambling Indian reservations ◦ American Indian reservations tribal colleges rise
            faster
            than ◦ main purpose of reservation to pay American Indians by poverty proposals

            Figure 4: MCR Summary for DUC-2006 Topic D0601A, using HMM Hedge for sentence compression

            18

            Seeking to get a more accurate count of the country’s American Indian population, the Census Bureau is
            turning to tribal leaders and residents on reservations to help overcome long-standing feelings of wariness
            or anger toward the federal government. ◦ American Indian reservations would get an infusion of $1.2
            billion in federal money for education, health care and law enforcement under President Clinton’s proposed
            2001 budget ◦ Smith and thousands like her are seeking help for their substance abuse at the American
            Indian Community House, the largest of a handful of Native American cultural institutions in the New
            York area. ◦ Clinton was going to the Pine Ridge Reservation for a visit with the Oglala Sioux nation
            and to participate in a conference on Native American homeownership and economic development. ◦ said
            Glen Revere, a nutritionist with the Indian Health Services on the 2.8 million-acre Wind River Reservation,
            about 100 miles east of Jackson, Wyo. “Then we came up with the idea for this community garden, and
            it’s been bigger than we ever expected in so many ways.” ◦ The road leading into the Shinnecock Indian
            reservation is not a welcoming one ◦ But the main purpose of the visit – the first to a reservation by a
            president since Franklin Roosevelt – was simply to pay attention to American Indians, who are so raked by
            grinding poverty that Clinton’s own advisers suggested he come up with special proposals geared specifically
            to the Indians’ plight. ◦ “This highlights what has been going on out there for 130 years,

            Figure 5: MCR Summary for DUC-2006 Topic D0601A, with no sentence compression

            19

            R1 Recall

            R1 Precision

            R1 F

            R2 Recall

            R2 Precision

            R2 F

            HMM
            Sentence
            0.23552
            (0.230140.24082)
            0.21896
            (0.213860.22384)
            0.22496
            (0.219830.22978)
            0.06838
            (0.065460.07155)
            0.06287
            (0.060170.06576)
            0.06488
            (0.062090.06785)

            HMM
            60 Block
            0.21381
            (0.209120.21827)
            0.18882
            (0.184440.19301)
            0.19966
            (0.195050.20391)
            0.06133
            (0.058480.06414)
            0.05351
            (0.050970.05588)
            0.05686
            (0.054200.05942)

            Trimmer

            Topiary

            0.21014
            (0.204360.21594)
            0.20183
            (0.196270.20722)
            0.20179
            (0.196120.20718)
            0.06337
            (0.060300.06677)
            0.06230
            (0.058870.06617)
            0.06079
            (0.057880.06401)

            0.25143
            (0.246320.25663)
            0.23038
            (0.225670.23522)
            0.23848
            (0.233730.24328)
            0.06637
            (0.063450.06958)
            0.06024
            (0.057470.06326)
            0.06252
            (0.059760.06561)

            Table 2: Rouge scores and 95% confidence intervals for 624 documents from DUC-2003 test set.

            5

            System Evaluations

            We tested four single-document summarization systems on the DUC-2003 Task 1 test set:
            • HMM Hedge using the first sentence of each document (HMM Sentence)
            • HMM Hedge using the first 60 words of each document (HMM 60 block)
            • Trimmer
            • Topiary
            Task 1 from DUC-2003 was to construct generic 75-byte summaries for 624 documents drawn from AP
            Newswire and the New York Times. The average size of the documents was 3,997 bytes, so a 75-byte
            summary represents a compression ratio of 1.9%.
            An automatic summarization evaluation tool, Rouge (Lin and Hovy, 2003), was used to evaluate
            the results. The system parameters were optimized by hand to maximize the Rouge-1 recall on
            a comparable training corpus, 500 AP Newswire and New York Times articles from the DUC-2004
            single-document short summary test data.
            The Rouge results are shown in Table 2. Results show that HMM Hedge 60 scored significantly
            lower than most other systems and that Topiary scored higher than all other systems for all R1
            measures. In addition, HMM Hedge Sentence scored significantly higher than Trimmer for the R1
            measures.
            We also evaluated Trimmer and HMM Hedge as components in our Multi-Candidate Reduction
            framework, along with a baseline that uses the same sentence selector but does not use sentence
            compression. All three systems considered the first five sentences of each document and used the
            sentence selection algorithm presented in Section 4. The feature weights were manually optimized to
            maximize Rouge-2 recall on a comparable training corpus, 1,593 Financial Times and Los Angeles
            Times articles grouped into 50 topics from the DUC-2005 query-focused multi-document summarization
            20

            R1 Recall

            R2 Recall

            Trimmer

            HMM Hedge

            0.29391
            (0.285600.30247)
            0.06718
            (0.063320.07111)

            0.27311
            (0.265540.28008)
            0.06251
            (0.058730.06620)

            No
            Compression
            0.27576
            (0.267720.28430)
            0.06126
            (0.057670.06519)

            Table 3: Rouge scores and 95% confidence intervals for 50 DUC-2006 test topics, comparing three
            MCR variants.

            MCR Score
            Higher
            Not Different
            Range
            Lower

            Rouge-2
            0.0805
            1
            23
            0.0678-0.0899
            11

            Rouge-SU4
            0.1360
            1
            24
            0.1238-0.1475
            10

            BE-HM
            0.0413
            0
            27
            0.0318-0.0508
            8

            Table 4: Official DUC-2006 Automatic Metrics for our MCR submission (System 32).

            test data. The systems were used to generate query-focused, 250-word summaries using the DUC-2006
            test data, described in Section 4.3.
            The systems were evaluated using Rouge, configured to omit stopwords from the calculation.7
            Results are shown in Table 3. MCR using Trimmer compressions scored significantly higher than
            MCR using HMM Hedge compressions and the baseline for Rouge-1, but there was not a significant
            difference among the three systems for Rouge-2.
            Finally, the University of Maryland and BBN submitted a version of MCR to the official DUC2006 evaluation.
            This version used Trimmer as the source of sentence compressions. Results show
            that use of sentence compression hurt the system on human evaluation of grammaticality. This is not
            surprising, since Trimmer aims to produce compressions that are grammatical in Headlinese, rather
            than standard English. Our MCR run scored significantly lower than 23 systems on NIST’s human
            evaluation of grammaticality. However, the system did not score significantly lower than any other
            system on NIST’s human evaluation of content responsiveness. A second NIST evaluation of content
            responsiveness asked evaluators to take readability into consideration. In this evaluation, MCR scored
            significantly lower than only two systems. The evaluators recognized that Trimmer compressions are
            not grammatical in standard English; yet, the content coverage was not significantly different from the
            best automatic systems and only two systems were found to be significantly more readable.
            NIST computed three “official” automatic evaluation metrics for DUC-2006: Rouge-2, RougeSU4 and BE-HM. Table
            4 shows the official scores of the submitted MCR system for these three
            metrics, along with numbers of systems that scored significantly higher, significantly lower, or were not
            significantly different from our MCR run. Also shown is the range of scores for the systems that were
            not significantly different from MCR. These results show that the performance of our MCR run was
            comparable to most other systems submitted to DUC-2006.
            7
            This is a change in the Rouge configuration from the official DUC-2006 evaluation. We note that the removal
            of
            non-essential stopwords (typical of Headlinese) is an important component of Trimmer-based sentence
            compression. For
            internal system comparisons, we configure Rouge in a way that will allow us to detect system differences
            relevant to our
            research focus. For reporting of official Rouge results on submitted systems we use the community’s accepted
            Rouge
            configurations.

            21

            The evaluation in Table 3 suggests that Trimmer sentence compression is preferable to HMM
            Hedge sentence compression for generation of English summaries of collections of document in English.
            However, HMM Hedge may prove to have value with noisier data, as we discuss in the next section.
            Nevertheless, sentence compression appears to be a valuable component of our framework for multidocument
            summarization, thus validating the ideas behind Multi-Candidate Reduction.

            6

            Applications to Different Types of Texts

            We have applied the MCR framework to summarizing different types of texts. In this section we briefly
            touch on genre-specific issues that are the subject of ongoing work. Trimmer, Topiary, and HMM Hedge
            were designed for summarization of written news. In this genre, the lead sentence is almost always
            the first non-trivial sentence of the document. More sophisticated methods for finding lead sentences
            did not outperform the baseline of simply selecting the first sentence for AP wire “hard” news stories.
            However, some types of articles, such as sports stories, opinion pieces, and movie reviews often do
            not have informative lead sentences and will require additional work in finding the best sentence for
            compression.
            MCR has also been applied to summarizing transcripts of broadcast news—another input form
            where lead sentences are often not informative. The conventions of broadcast news introduce categories
            of story-initial light content sentences, such as “I’m Dan Rather” or “We have an update on the story
            we’ve been following”. These present challenges for the filtering stage of our MCR framework.
            Such texts are additionally complicated by a range of problems not encountered in written news:
            noise introduced by automatic speech recognizers or other faulty transcription, issues associated with
            sentence boundary detection and story boundary detection. If word error rate is high, parser failures
            can prevent Trimmer from producing useful output. In this context, HMM Hedge becomes more
            attractive, since our language models are more resilient to noisy input.
            We have performed an initial evaluation of Trimmer, Topiary, and a baseline consisting of the
            first 75 characters of a document, on the task of creating 75-character headlines for broadcast news
            transcriptions (Zajic, 2007). The corpus for this task consisted of 560 broadcast news stories from
            ABC, CNN, NBC, Public Radio International, and Voice of America. We used Rouge-1 recall to
            evaluate the summaries and found that both systems scored higher than the baseline and that Topiary
            scored higher than Trimmer. However there were no significant differences among the systems.
            Another application of our framework is the summarization of email threads—collections of emails
            that share a common topic or were written as responses to each other. This task can essentially be
            treated as a multi-document summarization problem, albeit email thread structure introduces some
            constraints with respect to the ordering of summary sentences. Noisy data is inherent in this problem
            and pre-processing to remove quoted text, attachments, and headers is crucial. We have found that
            metadata, such as the name of the sender of each included extract help make email summaries easier
            to read.
            We performed an initial evaluation of HMM Hedge and Trimmer as the source of sentence compressions for an
            email thread summarization system based on the MCR framework (Zajic, 2007). The
            corpus for this task consisted of 10 manually constructed email threads from the Enron Corpus (Klimt
            and Yang, 2004). We used Rouge-1 and Rouge-2 recall with jackknifing to compare the automatic
            systems and the human summarizers. We did not observe a significant difference between the two systems, but
            we found that the task of summarizing email threads was extremely difficult for the humans
            (one summarizer scored significantly worse than the automatic systems). This application of MCR to
            email thread summarization is an initial effort. The difficulty of the task for the humans suggests that
            the community needs to develop a clearer understanding of what makes a good email thread summary
            and to explore practical uses for them.

            22

            Finally, Trimmer and HMM Hedge have been applied to Hindi-English cross-language summarization. In this
            case, Trimmer was applied to the output of machine translation. We adapted HMM
            Hedge to cross-lingual summarization by using the mechanism developed for morphological variation
            to represent translation probabilities from Hindi story words to English headline words. For more
            details, see Dorr et al. (2003a).

            7

            Future Work

            Future work on text summarization under the Multi-Candidate Reduction framework will focus on the
            three main components of the architecture: sentence filtering, sentence compression, and candidate
            selection.
            For single document summarization, the simple technique of selecting the first non-trivial sentence
            of a document for compression remains the best approach. However, for human interest stories or sports
            articles, this approach is less effective. In broadcast news transcripts, the first sentence often does not
            contain important information. Currently, filtering for multi-document summarization also relies on
            the assumption that important information tends to appear near the front of documents—the first five
            sentences of each document are retained to generate compressed candidates. An interesting area of
            future work is to explore other approaches to filtering, such as using query relevance and document
            centrality, to move beyond the baseline of selecting the first n sentences. For HMM Hedge, these
            methods can be used to determine the optimal blocks of text on which to apply the decoder.
            Currently, Trimmer produces multiple compressions by applying rules in a fixed order; the state of
            the compressed sentence after each rule application becomes a candidate. A richer pool of candidates
            can be produced by modifying Trimmer rules to operate in order-independent combinations, rather
            than a fixed sequence. We believe that the sentence selector can produce better summaries if it has
            larger pools of candidates to choose from. Naturally, different sentence compressions are not the only
            techniques for enriching the candidate pool—other possibilities include merging sentences and resolving
            anaphora. Topiary will also be enhanced by using multiple combinations of compressions and topic
            terms in the context of headline generation.
            We also plan to enrich the candidate selector by taking into account more features of the current
            summary state. Possibilities include sentence selector iteration count and remaining summary space, as
            well as feature weights that change during the progress of summary generation. These extensions will
            allow us to study the distribution of compressed and uncompressed sentences across sentence selector
            iterations. System output can potentially be improved by finer-grained control of this distribution.
            These features might also help avoid the current problem in which the final sentence is truncated due
            to length restrictions (e.g., by selecting a final sentence of more appropriate length).
            Proper setting of parameters is another important area for future work. Systematic optimization
            of parameter values in HMM Hedge and the sentence selector could lead to significant improvements
            in output quality. A logical extension to this work would be to learn the best parameter settings, e.g.,
            through Expectation Maximization.
            At present, MCR focuses exclusively on summary content selection and does not take sentence
            ordering into consideration when constructing the summary. Naturally, high-quality summaries should
            read fluently in addition to having relevant content. Recent work in this area that can be applied
            to MCR includes includes Conroy et al. (2006), Barzilay et al. (2002), Okazaki et al. (2004), Lapata (2003),
            and Dorr and Gaasterland (this special issue 2007). Within the MCR architecture, fluency
            considerations can be balanced with other important factors such as relevance and anti-redundancy
            through appropriate feature weighting.

            23

            8
        </corps>
        <conclusion>Conclusion

            This work presents Multi-Candidate Reduction, a general architecture for multi-document summarization. The
            framework integrates successful single-document compression techniques that we have
            previously developed. MCR is motivated by the insight that multiple candidate compressions of source
            sentences should be made available to subsequent processing modules, which may have access to more
            information for summary construction. This is implemented in a dynamic feature-based sentence selector that
            iteratively builds a summary from compressed variants. Evaluations show that sentence
            compression plays an important role in multi-document summarization and that our MCR framework
            is both flexible and extensible.

            Acknowledgments
            This work has been supported, in part, under the GALE program of the Defense Advanced Research
            Projects Agency, Contract No. HR0011-06-2-0001, the TIDES program of the Defense Advanced
            Research Projects Agency, BBNT Contract No. 9500006806, and the University of Maryland Joint
            Institute for Knowledge Discovery. Any opinions, findings, conclusions or recommendations expressed
            in this paper are those of the authors and do not necessarily reflect the views of DARPA. The first
            author would like to thank Naomi for proofreading, support, and encouragement. The second author
            would like to thank Steve, Carissa, and Ryan for their energy enablement. The third author would like
            to thank Esther and Kiri for their kind support.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>L. Bahl, F. Jelinek, and R. Mercer. 1983. A maximum likelihood approach to continuous speech
            recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190.
            M. Banko, V. Mittal, and M. Witbrock. 2000. Headline generation based on statistical translation.
            In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL
            2000), pages 318–325, Hong Kong.
            R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multidocument
            news summarization. Journal of Artificial Intelligence Research, 17:35–55.
            L. Baum. 1972. An inequality and associated maximization technique in statistical estimation of
            probabilistic functions of a Markov process. Inequalities, 3:1–8.
            S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz. 2003. Using knowledge-poor coreference
            resolution for text summarization. In Proceedings of the HLT-NAACL 2003 Text Summarization
            Workshop and Document Understanding Conference (DUC 2003), pages 85–92, Edmonton, Alberta.
            D. Bikel, R. Schwartz, and R. Weischedel. 1999. An algorithm that learns what’s in a name. Machine
            Learning, 34(1/3):211–231.
            S. Blair-Goldensohn, D. Evans, V. Hatzivassiloglou, K. McKeown, A. Nenkova, R. Passonneau,
            B. Schiffman, A. Schlaikjer, A. Siddharthan, and S. Siegelman. 2004. Columbia University
            at DUC 2004. In Proceedings of the 2004 Document Understanding Conference (DUC 2004) at
            HLT/NAACL 2004, pages 23–30, Boston, Massachusetts.

            24

            P. Brown, J. Cocke, S. Pietra, V. Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin. 1990. A
            statistical approach to machine translation. Computational Linguistics, 16(2):79–85.
            J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering
            documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR
            Conference on Research and Development in Information Retrieval (SIGIR 1998), pages 335–336,
            Melbourne, Australia.
            Eugene Charniak. 2000. A Maximum-Entropy-Inspired Parser. In Proceedings of the First Meeting
            of the North American Chapter of the Association for Computational Linguistics (NAACL 2000),
            pages 132–139, Seattle, Washington.
            J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains,
            training requirements and evaluation measures. In Proceedings of the 21st International Conference on
            Computational Linguistics and 44th Annual Meeting of the Association for Computational
            Linguistics (COLING/ACL 2006), pages 377–384, Sydney, Australia.
            J. Conroy, J. Schlesinger, and J. Goldstein. 2005. CLASSY query-based multi-document summarization. In
            Proceedings of the 2005 Document Understanding Conference (DUC-2005) at NLT/EMNLP
            2005, Vancouver, Canada.
            J. Conroy, J. Schlesinger, D. O’Leary, and J. Goldstein. 2006. Back to basics: CLASSY 2006. In
            Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006,
            New York, New York.
            D. Cutting, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the
            Third Conference on Applied Natural Language Processing, Trento, Italy.
            Hoa Dang and Donna Harman. 2006. Proceedings of the 2006 Document Understanding Conference
            (DUC 2006) at HLT/NAACL 2006.
            B. Dorr and T. Gaasterland. this special issue, 2007. Exploiting aspectual features and connecting
            words for summarization-inspired temporal-relation extraction. Information Processing and Management.
            B. Dorr, D. Zajic, and R. Schwartz. 2003a. Cross-language headline generation for Hindi. ACM
            Transactions on Asian Language Information Processing (TALIP), 2(3):270–289.
            B. Dorr, D. Zajic, and R. Schwartz. 2003b. Hedge Trimmer: A parse-and-trim approach to headline
            generation. In Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and Document
            Understanding Conference (DUC 2003), pages 1–8, Edmonton, Alberta.
            T. Dunning. 1994. Statistical identification of language. Technical Report MCCS 94-273, New Mexico
            State University.
            T. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of 13th
            International Workshop on Database and Expert Systems Applications (DEXA 2002), pages 215–
            222, Aix-en-Provence, France.
            J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. 2000. Multi-document summarization by
            sentence extraction. In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summarization, pages 40–48.
            D. Harman and M. Liberman. 1993. TIPSTER Complete. Linguistic Data Consortium (LDC),
            Philadelphia.
            25

            H. Jing and K. McKeown. 2000. Cut and paste based text summarization. In Proceedings of the First
            Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL
            2000), pages 178–185, Seattle, Washington.
            B. Klimt and Y. Yang. 2004. Introducing the Enron Corpus. In Proceedings of the First Conference
            on Email and Anti-Spam (CEAS), Mountain View, California.
            K. Knight and D. Marcu. 2000. Statistics-based summarization—step one: Sentence compression. In
            Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI-2000), Austin,
            Texas.
            K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach
            to sentence compression. Artificial Intelligence, 139(1):91–107.
            M. Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings
            of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages
            545–552, Barcelona, Spain.
            David Dolan Lewis. 1999. An evaluation of phrasal and clustered representations on a text categorization
            task. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research
            and Development in Information Retrieval (SIGIR 1992), pages 37–50, Copenhagen, Denmark.
            C.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In
            Proceedings of the 2003 Human Language Technology Conference and the North American
            Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2003),
            pages 71–78, Edmonton, Alberta.
            I. Mårdh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo.
            E. Mays, F. Damerau, and R. Mercer. 1990. Context-based spelling correction. In Proceedings of IBM
            Natural Language ITL, pages 517–522, Paris, France.
            S. Miller, L. Ramshaw, H. Fox, and R. Weischedel. 2000. A novel use of statistical parsing to extract
            information from text. In Proceedings of the First Meeting of the North American Chapter of the
            Association for Computational Linguistics (NAACL 2000), pages 226–233, Seattle, Washington.
            S. Muresan, E. Tzoukermann, and J. Klavans. 2001. Combining linguistic and machine learning
            techniques for email. In Proceedings of the ACL/EACL 2001 Workshop on Computational Natural
            Language Learning (ConLL), pages 290–297, Toulouse, France.
            N. Okazaki, Y. Matsuo, and M. Ishizuka. 2004. Improving chronological sentence ordering by precedence
            relation. In Proceedings of the 20th International Conference on Computational Linguistics
            (COLING 2004), pages 750–756, Geneva, Switzerland.
            M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.
            D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer, A. Çelebi, S. Dimitrov, E. Drabek, A. Hakim,
            W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion, S. Teufel, M. Topper, A. Winkel, and Z. Zhang.
            2004. MEAD—a platform for multidocument multilingual text summarization. In Proceedings of
            the 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon,
            Portugal.

            26

            R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood model
            for topic classification of broadcast news. In Proceedings of the Fifth European Speech Communication
            Association Conference on Speech Communication and Technology (Eurospeech-97), Rhodes,
            Greece.
            S. Sista, R. Schwartz, T. Leek, and J. Makhoul. 2002. An algorithm for unsupervised topic discovery
            from broadcast news stories. In Proceedings of the 2002 Human Language Technology Conference
            (HLT), pages 99–103, San Diego, California.
            J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression.
            In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL
            2005), pages 290–297, Ann Arbor, Michigan.
            L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft Research at DUC2006: Task-focused
            summarization with sentence simplification and lexical expansion. In Proceedings of the 2006 Document
            Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York, New York.
            A. Viterbi. 1967. Error bounds for convolution codes and an asymptotically optimal decoding algorithm. IEEE
            Transactions on Information Theory, 13:260–269.
            R. Wang, N. Stokes, W. Doran, E. Newman, J. Carthy, and J. Dunnion. 2005. Comparing Topiarystyle approaches
            to headline generation. In Lecture Notes in Computer Science: Advances in
            Information Retrieval: 27th European Conference on IR Research (ECIR 2005), volume 3408,
            Santiago de Compostela, Spain. Springer Berlin / Heidelberg.
            D. Zajic, B. Dorr, and R. Schwartz. 2004. BBN/UMD at DUC-2004: Topiary. In Proceedings of
            the 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL 2004, pages 112–119,
            Boston, Massachusetts.
            D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2005a. UMD/BBN at MSE2005. In Proceedings of the
            MSE2005 Track of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for
            MT and/or Summarization, Ann Arbor, Michigan.
            D. Zajic, B. Dorr, R. Schwartz, C. Monz, and J. Lin. 2005b. A sentence-trimming approach to
            multi-document summarization. In Proceedings of the 2005 Document Understanding Conference
            (DUC-2005) at NLT/EMNLP 2005, pages 151–158, Vancouver, Canada.
            D. Zajic. 2007. Multiple Alternative Sentence Compressions (MASC) as a Tool for Automatic Summarization
            Tasks. Ph.D. thesis, University of Maryland, College Park.
            L. Zhou and E. Hovy. 2003. Headline summarization at ISI. In Proceedings of the HLT-NAACL
            2003 Text Summarization Workshop and Document Understanding Conference (DUC 2003), pages
            174–178, Edmonton, Alberta.

            27
        </biblio>
    </article>
    <article>
        <preamble>compression_phrases_Prog-Linear-jair.txt</preamble>
        <titre>Global Inference for Sentence Compression An Integer Linear Programming Approach</titre>
        <auteur>James Clarke jclarke@ed.ac.uk</auteur>
        <abstract>Sentence compression holds promise for many applications ranging from summarization to subtitle
            generation. Our work views sentence compression as an optimization problem and uses integer linear
            programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated
            constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend
            these models with novel global constraints. Experimental results on written and spoken texts demonstrate
            improvements over state-of-the-art models.
        </abstract>
        <introduction>The computational treatment of sentence compression has recently attracted much attention in the
            literature. The task can be viewed as producing a summary of a single sentence that retains the most
            important information and remains grammatical (Jing, 2000). A sentence compression mechanism would greatly
            benefit a wide range of applications. For example, in summarization, it could improve the conciseness of the
            generated summaries (Jing, 2000; Lin, 2003; Zajic, Door, Lin, & Schwartz, 2007). Other examples include
            compressing text to be displayed on small screens such as mobile phones or PDAs (Corston-Oliver, 2001),
            subtitle generation from spoken transcripts (Vandeghinste & Pan, 2004), and producing audio scanning devices
            for the blind (Grefenstette, 1998). Sentence compression is commonly expressed as a word deletion problem:
            given an input source sentence of words x = x1 , x2 , . . . , xn , the aim is to produce a target
            compression by removing any subset of these words (Knight & Marcu, 2002). The compression problem has been
            extensively studied across different modeling paradigms, both supervised and unsupervised. Supervised models
            are typically trained on a parallel corpus of source sentences and target compressions and come in many
            flavors. Generative models aim to model the probability of a target compression given the source sentence
            either directly (Galley & McKeown, 2007) or indirectly using the noisy-channel model (Knight & Marcu, 2002;
            Turner & Charniak, 2005), whereas discriminative formulations attempt to minimize error rate on a training
            set. These include decision-tree learning (Knight & Marcu, 2002), maximum entropy (Riezler, King, Crouch, &
            Zaenen, 2003), support vector machines (Nguyen, Shimazu, Horiguchi, Ho, & Fukushi, 2004), and large-margin
            learning (McDonald, 2006). c 2008 AI Access Foundation. All rights reserved.
        </introduction>
        <corps>c 2008 AI Access Foundation. All rights reserved.

            Clarke & Lapata

            Unsupervised methods dispense with the parallel corpus and generate compressions either
            using rules (Turner & Charniak, 2005) or a language model (Hori & Furui, 2004).
            Despite differences in formulation, all these approaches model the compression process
            using local information. For instance, in order to decide which words to drop, they exploit
            information about adjacent words or constituents. Local models can do a good job at
            producing grammatical compressions, however they are somewhat limited in scope since
            they cannot incorporate global constraints on the compression output. Such constraints
            consider the sentence as a whole instead of isolated linguistic units (words or constituents).
            To give a concrete example we may want to ensure that each target compression has a verb,
            provided that the source had one in the first place. Or that verbal arguments are present in
            the compression. Or that pronouns are retained. Such constraints are fairly intuitive and
            can be used to instill not only linguistic but also task specific information into the model.
            For instance, an application which compresses text to be displayed on small screens would
            presumably have a higher compression rate than a system generating subtitles from spoken
            text. A global constraint could force the former system to generate compressions with a
            fixed rate or a fixed number of words.
            Existing approaches do not model global properties of the compression problem for a
            good reason. Finding the best compression for a source sentence given the space of all
            possible compressions1 (this search process is often referred to as decoding or inference)
            can become intractable for too many constraints and overly long sentences. Typically, the
            decoding problem is solved efficiently using dynamic programming often in conjunction
            with heuristics that reduce the search space (e.g., Turner & Charniak, 2005). Dynamic
            programming guarantees we will find the global optimum provided the principle of optimality holds. This
            principle states that given the current state, the optimal decision for each
            of the remaining stages does not depend on previously reached stages or previously made
            decisions (Winston & Venkataramanan, 2003). However, we know this to be false in the
            case of sentence compression. For example, if we have included modifiers to the left of a
            noun in a compression then we should probably include the noun too or if we include a verb
            we should also include its arguments. With a dynamic programming approach we cannot
            easily guarantee such constraints hold.
            In this paper we propose a novel framework for sentence compression that incorporates
            constraints on the compression output and allows us to find an optimal solution. Our
            formulation uses integer linear programming (ILP), a general-purpose exact framework for
            NP-hard problems. Specifically, we show how previously proposed models can be recast
            as integer linear programs. We extend these models with constraints which we express as
            linear inequalities. Decoding in this framework amounts to finding the best solution given
            a linear (scoring) function and a set of linear constraints that can be either global or local.
            Although ILP has been previously used for sequence labeling tasks (Roth & Yih, 2004;
            Punyakanok, Roth, Yih, & Zimak, 2004), its application to natural language generation
            is less widespread. We present three compression models within the ILP framework, each
            representative of an unsupervised (Knight & Marcu, 2002), semi-supervised (Hori & Furui,
            2004), and fully supervised modeling approach (McDonald, 2006). We propose a small
            number of constraints ensuring that the compressions are structurally and semantically
            1. There are 2n possible compressions where n is the number of words in a sentence.

            400

            Global Inference for Sentence Compression

            valid and experimentally evaluate their impact on the compression task. In all cases, we
            show that the added constraints yield performance improvements.
            The remainder of this paper is organized as follows. Section 2 provides an overview
            of related work. In Section 3 we present the ILP framework and the compression models
            we employ in our experiments. Our constraints are introduced in Section 3.5. Section 4.3
            discusses our experimental set-up and Section 5 presents our results. Discussion of future
            work concludes the paper.

            2. Related Work
            In this paper we develop several ILP-based compression models. Before presenting these
            models, we briefly summarize previous work addressing sentence compression with an emphasis on data-driven
            approaches. Next, we describe how ILP techniques have been used
            in the past to solve other inference problems in natural language processing (NLP).
            2.1 Sentence Compression
            Jing (2000) was perhaps the first to tackle the sentence compression problem. Her approach
            uses multiple knowledge sources to determine which phrases in a sentence to remove. Central
            to her system is a grammar checking module that specifies which sentential constituents
            are grammatically obligatory and should therefore be present in the compression. This
            is achieved using simple rules and a large-scale lexicon. Other knowledge sources include
            WordNet and corpus evidence gathered from a parallel corpus of source-target sentence
            pairs. A phrase is removed only if it is not grammatically obligatory, not the focus of the
            local context and has a reasonable deletion probability (estimated from a parallel corpus).
            In contrast to Jing (2000), the bulk of the research on sentence compression relies exclusively on corpus
            data for modeling the compression process without recourse to extensive knowledge sources (e.g., WordNet). A
            large number of approaches are based on the
            noisy-channel model (Knight & Marcu, 2002). These approaches consist of a language
            model P (y) (whose role is to guarantee that compression output is grammatical), a channel
            model P (x|y) (capturing the probability that the source sentence x is an expansion of the
            target compression y), and a decoder (which searches for the compression y that maximizes
            P (y)P (x|y)). The channel model is acquired from a parsed version of a parallel corpus; it
            is essentially a stochastic synchronous context-free grammar (Aho & Ullman, 1969) whose
            rule probabilities are estimated using maximum likelihood. Modifications of this model are
            presented by Turner and Charniak (2005) and Galley and McKeown (2007) with improved
            results.
            In discriminative models (Knight & Marcu, 2002; Riezler et al., 2003; McDonald, 2006;
            Nguyen et al., 2004) sentences are represented by a rich feature space (also induced from
            parse trees) and the goal is to learn which words or word spans should be deleted in a given
            context. For instance, in Knight and Marcu’s (2002) decision-tree model, compression is
            performed deterministically through a tree rewriting process inspired by the shift-reduce
            parsing paradigm. Nguyen et al. (2004) render this model probabilistic through the use
            of support vector machines. McDonald (2006) formalizes sentence compression in a largemargin learning
            framework without making reference to shift-reduce parsing. In his model
            compression is a classification task: pairs of words from the source sentence are classified
            401

            Clarke & Lapata

            as being adjacent or not in the target compression. A large number of features are defined
            over words, parts-of-speech, phrase structure trees and dependencies. These features are
            gathered over adjacent words in the compression and the words in-between which were
            dropped (see Section 3.4.3 for a more detailed account).
            While most compression models have been developed with written text in mind, Hori
            and Furui (2004) propose a model for automatically transcribed spoken text. Their model
            generates compressions through word deletion without using parallel data or syntactic information in any
            way. Assuming a fixed compression rate, it searches for the compression
            with the highest score using a dynamic programming algorithm. The scoring function consists of a language
            model responsible for producing grammatical output, a significance score
            indicating whether a word is topical or not, and a score representing the speech recognizer’s
            confidence in transcribing a given word correctly.
            2.2 Integer Linear Programming in NLP
            ILPs are constrained optimization problems where both the objective function and the
            constraints are linear equations with integer variables (see Section 3.1 for more details). ILP
            techniques have been recently applied to several NLP tasks, including relation extraction
            (Roth & Yih, 2004), semantic role labeling (Punyakanok et al., 2004), the generation of
            route directions (Marciniak & Strube, 2005), temporal link analysis (Bramsen, Deshpande,
            Lee, & Barzilay, 2006), set partitioning (Barzilay & Lapata, 2006), syntactic parsing (Riedel
            & Clarke, 2006), and coreference resolution (Denis & Baldridge, 2007).
            Most of these approaches combine a local classifier with an inference procedure based
            on ILP. The classifier proposes possible answers which are assessed in the presence of global
            constraints. ILP is used to make a final decision that is consistent with the constraints
            and likely according to the classifier. For example, the semantic role labeling task involves
            identifying the verb-argument structure for a given sentence. Punyakanok et al. (2004) first
            use SNOW, a multi-class classifier2 (Roth, 1998), to identify and label candidate arguments.
            They observe that the labels assigned to arguments in a sentence often contradict each other.
            To resolve these conflicts they propose global constraints (e.g., each argument should be
            instantiated once for a given verb, every verb should have at least one argument) and use
            ILP to reclassify the output of SNOW.
            Dras (1999) develops a document paraphrasing model using ILP. The key premise of
            his work is that in some cases one may want to rewrite a document so as to conform to
            some global constraints such as length, readability, or style. The proposed model has three
            ingredients: a set of sentence-level paraphrases for rewriting the text, a set of global constraints, and an
            objective function which quantifies the effect incurred by the paraphrases.
            Under this formulation, ILP can be used to select which paraphrases to apply so that the
            global constraints are satisfied. Paraphrase generation falls outside the scope of the ILP
            model – sentence rewrite operations are mainly syntactic and provided by a module based
            on synchronous tree adjoining grammar (S-TAG, Shieber & Schabes, 1990). Unfortunately,
            only a proof-of-concept is presented; implementation and evaluation of this module are left
            to future work.
            2. SNOW’s learning algorithm is a variation of the Winnow update rule.

            402

            Global Inference for Sentence Compression

            Our work models sentence compression as an optimization problem. We show how previously proposed models can
            be reformulated in the context of integer linear programming
            which allows us to easily incorporate constraints during the decoding process. Our constraints are
            linguistically and semantically motivated and are designed to bring less local
            syntactic knowledge into the model and help preserve the meaning of the source sentence.
            Previous work has identified several important features for the compression task (Knight
            & Marcu, 2002; McDonald, 2006); however, the use of global constraints is novel to our
            knowledge. Although sentence compression has not been explicitly formulated in terms of
            optimization, previous approaches rely on some optimization procedure for generating the
            best compression. The decoding process in the noisy-channel model searches for the best
            compression given the source and channel models. However, the compression found is usually sub-optimal as
            heuristics are used to reduce the search space or is only locally optimal
            due to the search method employed. For example, in the work of Turner and Charniak
            (2005) the decoder first searches for the best combination of rules to apply. As it traverses
            the list of compression rules, it removes sentences outside the 100 best compressions (according to the
            channel model). This list is eventually truncated to 25 compressions. In
            other models (Hori & Furui, 2004; McDonald, 2006) the compression score is maximized
            using dynamic programming which however can yield suboptimal results (see the discussion
            in Section 1).
            Contrary to most other NLP work using ILP (a notable exception is Roth & Yih, 2005),
            we do not view compression generation as a two stage process where learning and inference
            are carried out sequentially (i.e., first a local classifier hypothesizes a list of possible answers and
            then the best answer is selected using global constraints). Our models integrate
            learning with inference in a unified framework where decoding takes place in the presence
            of all available constraints, both local and global. Moreover, we investigate the influence
            of our constraint set across models and learning paradigms. Previous work typically formulates constraints
            for a single model (e.g., the SNOW classifier) and learning paradigm
            (e.g., supervised). We therefore assess how the constraint-based framework advocated in
            this article influences the performance of expressive models (which require large amounts of
            parallel data) and non-expressive ones (which use very little parallel data or none at all). In
            other words, we are able to pose and answer the following question: what kinds of models
            benefit most from constraint-based inference?
            Our work is close in spirit but rather different in content to Dras (1999). We concentrate
            on compression, a specific paraphrase type, and apply our models on the sentence-level. Our
            constraints thus do not affect the document as a whole but individual sentences. Furthermore, compression
            generation is an integral part of our ILP models, whereas Dras assumes
            that paraphrases are generated by a separate process.

            3. Framework
            In this section we present the details of the proposed framework for sentence compression.
            As mentioned earlier, our work models sentence compression directly as an optimization
            problem. There are 2n possible compressions for each source sentence and while many
            of these will be unreasonable, it is unlikely that only one compression will be satisfactory (Knight &
            Marcu, 2002). Ideally, we require a function that captures the operations
            403

            Clarke & Lapata

            (or rules) that can be performed on a sentence to create a compression while at the same
            time factoring how desirable each operation makes the resulting compression. We can then
            perform a search over all possible compressions and select the best one, as determined by
            how desirable it is. A wide range of models can be expressed under this framework. The
            prerequisites for implementing these are fairly low, we only require that the decoding process be expressed
            as a linear function with a set of linear constraints. In practice, many
            models rely on a Markov assumption for factorization which is usually solved with a dynamic
            programming-based decoding process. Such algorithms can be formulated as integer
            linear programs with little effort.
            We first give a brief introduction into integer linear programming, an extension of linear
            programming for readers unfamiliar with mathematical programming. Our compression
            models are next described in Section 3.4 and constraints in Section 3.5.
            3.1 Linear Programming
            Linear programming (LP) problems are optimization problems with constraints. They
            consist of three parts:
            • Decision variables. These are variables under our control which we wish to assign
            optimal values to.
            • A linear function (the objective function). This is the function we wish to minimize or
            maximize. This function is influences by the values assigned to the decision variables.
            • Constraints. Most problems will only allow the decision variables to take certain
            values. These restrictions are the constraints.
            These terms are best demonstrated with a simple example taken from Winston and
            Venkataramanan (2003). Imagine a manufacturer of tables and chairs which we shall call
            the Telfa Corporation. To produce a table, 1 hour of labor and 9 square board feet of wood
            is required. Chairs require 1 hour of labor and 5 square board feet of wood. Telfa have
            6 hours of labor and 45 square board feet of wood available. The profit made from each
            table is 8 GBP and 5 GBP for chairs. We wish to determine the number of tables and
            chairs that should be manufactured to maximize Telfa’s profit.
            First, we must determine the decision variables. In our case we define:
            x1 = number of tables manufactured
            x2 = number of chairs manufactured
            Our objective function is the value we wish to maximize, namely the profit.
            Profit = 8x1 + 5x2
            There are two constraints in this problem: we must not exceed 6 hours of labor and no
            more than 45 square board feet of wood must be used. Also, we cannot create a negative
            amount of chairs or tables:
            404

            Global Inference for Sentence Compression

            Labor constraint
            x 1 + x2
            Wood constraint
            9x1 + 5x2
            Variable constraints
            x1
            x2

            ≤ 6
            ≤ 45
            ≥ 0
            ≥ 0

            Once the decision variables, objective function and constraints have been determined we
            can express the LP model:
            max z = 8x1 + 5x2 (Objective function)
            subject to (s.t.)
            x1 + x2
            9x1 + 5x2
            x1
            x2

            ≤ 6 (Labor constraint)
            ≤ 45 (Wood constraint)
            ≥ 0
            ≥ 0

            Two of the most basic concepts involved in solving LP problems are the feasibility region
            and optimal solution. The optimal solution is one in which all constraints are satisfied
            and the objective function is minimized or maximized. A specification of the value for
            each decision variable is referred to as a point. The feasibility region for a LP is a region
            consisting of the set of all points that satisfy all the LP’s constraints. The optimal solution
            lies within this feasibility region, it is the point with the minimum or maximum objective
            function value.
            A set of points satisfying a single linear inequality is a half-space. The feasibility region
            is defined by a the intersection of m half-spaces (for m linear inequalities) and forms a
            polyhedron. Our Telfa example forms a polyhedral set (a polyhedral convex set) from
            the intersection of our four constraints. Figure 1a shows the feasible region for the Telfa
            example. To find the optimal solution we graph a line (or hyperplane) on which all points
            have the same objective function value. In maximization problems it is called the isoprofit
            line and in minimization problems the isocost line. One isoprofit line is represented by the
            dashed black line in Figure 1a. Once we have one isoprofit line we can find all other isoprofit
            lines by moving parallel to the original isoprofit line.
            The extreme points of the polyhedral set are defined as the intersections of the lines
            that form the boundaries of the polyhedral set (points A B C and D in Figure 1a). It can
            be shown that any LP that has an optimal solution, has an extreme point that is globally
            optimal. This reduces the search space of the optimization problem to finding the extreme
            point with the highest or lowest value. The simplex algorithm (Dantzig, 1963) solves LPs
            by exploring the extreme points of a polyhedral set. Specifically, it moves from one extreme
            point to an adjacent extreme point (extreme points that lie on the same line segment) until
            an optimal extreme point is found. Although the simplex algorithm has an exponential
            worst-case complexity, in practice the algorithm is very efficient.
            15
            9
            The optimal solution for the Telfa example is z = 165
            4 , x1 = 4 , x2 = 4 . Thus, to
            achieve a maximum profit of 41.25 GBP they must build 3.75 tables and 2.25 chairs. This
            is obviously impossible as we would not expect people to buy fractions of tables and chairs.
            Here, we want to be able to constrain the problem such that the decision variables can only
            take integer values. This can be done with Integer Linear Programming.
            405

            Clarke & Lapata

            a.

            b.

            10

            10

            9

            9

            = LP’s feasible region

            9x1 + 5x2 = 45

            9x1+ 5x2 = 45

            8

            8

            7

            7

            6 B

            6

            x2 5

            x2 5

            4

            4

            = IP feasible point
            = IP relaxation’s feasible region

            3

            3

            Optimal LP solution

            Optimal LP solution
            2

            C

            2

            x 1 + x2 = 6

            1
            0

            A

            0

            1

            2

            3

            x1

            4

            D
            5

            6

            x 1 + x2 = 6

            11

            7

            0

            0

            1

            2

            3

            x1

            4

            5

            6

            7

            Figure 1: Feasible region for the Telfa example using linear (graph (a)) and integer linear
            (graph (b)) programming

            3.2 Integer Linear Programming
            Integer linear programming (ILP) problems are LP problems in which some or all of the
            variables are required to be non-negative integers. They are formulated in a similar manner
            to LP problems with the added constraint that all decision variables must take non-negative
            integer values.
            To formulate the Telfa problem as an ILP model we merely add the constraints that x1
            and x2 must be integer. This gives:
            max z = 8x1 + 5x2 (Objective function)
            subject to (s.t.)
            x1 + x2
            9x1 + 5x2
            x1
            x2

            ≤
            6 (Labor constraint)
            ≤
            45 (Wood constraint)
            ≥ 0; x1 integer
            ≥ 0; x2 integer

            For LP models, it can be proved that the optimal solution lies on an extreme point of
            the feasible region. In the case of integer linear programs, we only wish to consider points
            that are integer values. This is illustrated in Figure 1b for the Telfa problem. In contrast to
            linear programming, which can be solved efficiently in the worst case, integer programming
            problems are in many practical situations NP-hard (Cormen, Leiserson, & Rivest, 1992).
            406

            Global Inference for Sentence Compression

            Fortunately, ILPs are a well studied optimization problem and a number of techniques have
            been developed to find the optimal solution. Two such techniques are the cutting planes
            method (Gomory, 1960) and the branch-and-bound method (Land & Doig, 1960). We
            briefly discuss these methods here. For a more detailed treatment we refer the interested
            reader to Winston and Venkataramanan (2003) or Nemhauser and Wolsey (1988).
            The cutting planes method adds extra constraints to slice parts of the feasible region
            until it contains only integer extreme points. However, this process can be difficult or
            impossible (Nemhauser & Wolsey, 1988). The branch-and-bound method enumerates all
            points in the ILP’s feasible region but prunes those sections in the region which are known
            to be sub-optimal. It does this by relaxing the integer constraints and solving the resulting
            LP problem (known as the LP relaxation). If the solution of the LP relaxation is integral,
            then it is the optimal solution. Otherwise, the resulting solution provides an upper bound
            on the solution for the ILP. The algorithm proceeds by creating two new sub-problems based
            on the non-integer solution for one variable at a time. These are solved and the process
            repeats until the optimal integer solution is found.
            Using the branch-and-bound method, we find that the optimal solution to the Telfa
            problem is z = 40, x1 = 5, x2 = 0; thus, to achieve a maximum profit of 40 GBP, Telfa
            must manufacture 5 tables and 0 chairs. This is a relatively simple problem, which could be
            solved merely by inspection. Most ILP problems will involve many variables and constraints
            resulting in a feasible region with a large number of integer points. The branch-and-bound
            procedure can efficiently solve such ILPs in a matter of seconds and forms part of many
            commercial ILP solvers. In our experiments we use lp solve 3 , a free optimization package
            which relies on the simplex algorithm and brand-and-bound methods for solving ILPs.
            Note that under special circumstances other solving methods may be applicable. For
            example, implicit enumeration can be used to solve ILPs where all the variables are binary
            (also known as pure 0−1 problems). Implicit enumeration is similar to the branch-andbound method, it
            systematically evaluates all possible solutions, without however explicitly
            solving a (potentially) large number of LPs derived from the relaxation. This removes
            much of the computational complexity involved in determining if a sub-problem is infeasible. Furthermore,
            for a class of ILP problems known as minimum cost network flow
            problems (MCNFP), the LP relaxation always yields an integral solution. These problems
            can therefore be treated as LP problems.
            In general, a model will yield an optimal solution in which all variables are integers if
            the constraint matrix has a property known as total unimodularity. A matrix A is totally
            unimodular if every square sub-matrix of A has its determinant equal to 0, +1 or −1.
            It is the case that the more the constraint matrix looks totally unimodular, the easier
            the problem will be to solve by branch-and-bound methods. In practice it is good to
            formulate ILPs where as many variables as possible have coefficients of 0, +1 or −1 in the
            constraints (Winston & Venkataramanan, 2003).
            3.3 Constraints and Logical Conditions
            Although integer variables in ILP problems may take arbitrary values, these are frequently
            are restricted to 0 and 1. Binary variables (0−1 variables) are particularly useful for rep3. The software
            is available from http://lpsolve.sourceforge.net/.

            407

            Clarke & Lapata

            Condition
            Implication
            Iff
            Or
            Xor
            And
            Not

            Statement
            if a then b
            a if and only if b
            a or b or c
            a xor b xor c
            a and b
            not a

            Constraint
            b−a≥0
            a−b=0
            a+b+c≥1
            a+b+c=1
            a = 1; b = 1
            1−a=1

            Table 1: How to represent logical conditions using binary variables and constraints in ILP.

            resenting a variety of logical conditions within the ILP framework through the use of constraints. Table 1
            lists several logical conditions and their equivalent constraints.
            We can also express transitivity, i.e., “c if and only if a and b”. Although it is often thought that
            transitivity can only be expressed as a polynomial expression of binary
            variables (i.e., ab = c), it is possible to replace the latter by the following linear inequalities
            (Williams, 1999):

            (1 − c) + a ≥ 1
            (1 − c) + b ≥ 1
            c + (1 − a) + (1 − b) ≥ 1
            This can be easily extended to model indicator variables representing whether a set of binary
            variables can take certain values.
            3.4 Compression Models
            In this section we describe three compression models which we reformulate as integer linear
            programs. Our first model is a simple language model which has been used as a baseline in
            previous research (Knight & Marcu, 2002). Our second model is based on the work of Hori
            and Furui (2004); it combines a language model with a corpus-based significance scoring
            function (we omit here the confidence score derived from the speech recognizer since our
            models are applied to text only). This model requires a small amount of parallel data to
            learn weights for the language model and the significance score.
            Our third model is fully supervised, it uses a discriminative large-margin framework
            (McDonald, 2006), and is trained trained on a larger parallel corpus. We chose this model
            instead of the more popular noisy-channel or decision-tree models, for two reasons, a practical one and a
            theoretical one. First, McDonald’s (2006) model delivers performance superior
            to the decision-tree model (which in turn performs comparably to the noisy-channel). Second, the noisy
            channel is not an entirely appropriate model for sentence compression. It
            uses a language model trained on uncompressed sentences even though it represents the
            probability of compressed sentences. As a result, the model will consider compressed sentences less likely
            than uncompressed ones (a further discussion is provided by Turner &
            Charniak, 2005).
            408

            Global Inference for Sentence Compression

            3.4.1 Language Model
            A language model is perhaps the simplest model that springs to mind. It does not require
            a parallel corpus (although a relatively large monolingual corpus is necessary for training),
            and will naturally prefer short sentences to longer ones. Furthermore, a language model can
            be used to drop words that are either infrequent or unseen in the training corpus. Knight
            and Marcu (2002) use a bigram language model as a baseline against their noisy-channel
            and decision-tree models.
            Let x = x1 , x2 , . . . , xn denote a source sentence for which we wish to generate a target
            compression. We introduce a decision variable for each word in the source and constrain it
            to be binary; a value of 0 represents a word being dropped, whereas a value of 1 includes
            the word in the target compression. Let:
            δi =

            (

            1 if xi is in the compression
            ∀i ∈ [1 . . . n]
            0 otherwise

            If we were using a unigram language model, our objective function would maximize the
            overall sum of the decision variables (i.e., words) multiplied by their unigram probabilities
            (all probabilities throughout this paper are log-transformed):
            max

            n
            X

            δi · P (xi )

            (1)

            i=1

            Thus, if a word is selected, its corresponding δi is given a value of 1, and its probability
            P (xi ) according to the language model will be counted in our total score.
            A unigram language model will probably generate many ungrammatical compressions.
            We therefore use a more context-aware model in our objective function, namely a trigram
            model. Dynamic programming would be typically used to decode a language model by
            traversing the sentence in a left-to-right manner. Such an algorithm is efficient and provides
            all the context required for a conventional language model. However, it can be difficult
            or impossible to incorporate global constraints into such a model as decisions on word
            inclusion cannot extend beyond a three word window. By formulating the decoding process
            for a trigram language model as an integer linear program we are able to take into account
            constraints that affect the compressed sentence more globally. This process is a much more
            involved task than in the unigram case where there is no context, instead we must now
            make decisions based on word sequences rather than isolated words. We first create some
            additional decision variables:
            αi =

            (

            βij =

            
            
             1

            γijk =

            1 if xi starts the compression
            ∀i ∈ [1 . . . n]
            0 otherwise

            if sequence xi , xj ends
            the compression
            ∀i ∈ [0 . . . n − 1]
            
             0 otherwise
            ∀j ∈ [i + 1 . . . n]

            
            
             1

            if sequence xi , xj , xk ∀i ∈ [0 . . . n − 2]
            is in the compression ∀j ∈ [i + 1 . . . n − 1]
            
             0 otherwise
            ∀k ∈ [j + 1 . . . n]
            409

            Clarke & Lapata

            Our objective function is given in Equation (2). This is the sum of all possible trigrams
            that can occur in all compressions of the source sentence where x0 represents the ‘start’
            token and xi is the ith word in sentence x. Equation (3) constrains the decision variables
            to be binary.
            max z =

            n
            X

            αi · P (xi |start)
            i=1
            n−2
            n
            X n−1
            X X

            +

            γijk · P (xk |xi , xj )

            i=1 j=i+1 k=j+1

            +

            n−1
            X

            n
            X

            βij · P (end|xi , xj )

            (2)

            i=0 j=i+1

            subject to:

            δi , αi , βij , γijk = 0 or 1

            (3)

            The objective function in (2) allows any combination of trigrams to be selected. This
            means that invalid trigram sequences (e.g., two or more trigrams containing the ‘end’ token)
            could appear in the target compression. We avoid this situation by introducing sequential
            constraints (on the decision variables δi , γijk , αi , and βij ) that restrict the set of allowable
            trigram combinations.
            Constraint 1

            Exactly one word can begin a sentence.
            n
            X

            αi = 1

            (4)

            i=1

            Constraint 2 If a word is included in the sentence it must either start the sentence or be
            preceded by two other words or one other word and the ‘start’ token x0 .
            δk − αk −

            k−2
            X k−1
            X

            γijk = 0

            (5)

            i=0 j=1

            ∀k : k ∈ [1 . . . n]
            Constraint 3 If a word is included in the sentence it must either be preceded by one
            word and followed by another or it must be preceded by one word and end the sentence.
            δj −

            j−1
            X

            n
            X

            γijk −

            i=0 k=j+1

            j−1
            X

            βij = 0

            (6)

            i=0

            ∀j : j ∈ [1 . . . n]

            Constraint 4 If a word is in the sentence it must be followed by two words or followed
            by one word and then the end of the sentence or it must be preceded by one word and end
            the sentence.
            δi −

            n−1
            X

            n
            X

            j=i+1 k=j+1

            γijk −

            n
            X

            j=i+1

            410

            βij −

            i−1
            X

            βhi = 0

            h=0

            ∀i : i ∈ [1 . . . n]

            (7)

            Global Inference for Sentence Compression

            Constraint 5

            Exactly one word pair can end the sentence.
            n−1
            X

            n
            X

            βij = 1

            (8)

            i=0 j=i+1

            The sequential constraints described above ensure that the second order factorization (for
            trigrams) holds and are different from our compression-specific constraints which are presented in Section
            3.5.
            Unless normalized by sentence length, a language model will naturally prefer one-word
            output. This normalization is however non-linear and cannot be incorporated into our ILP
            formulation. Instead, we impose a constraint on the length of the compressed sentence.
            Equation (9) below forces the compression to contain at least b tokens.
            n
            X

            δi ≥ b

            (9)

            i=1

            Alternatively, we could force the compression to be exactly b tokens (by substituting the
            inequality with an equality in (9)) or to be less than b tokens (by replacing ≥ with ≤).4
            The constraint in (9) is language model-specific and is not used elsewhere.
            3.4.2 Significance Model
            The language model just described has no notion of which content words to include in the
            compression and thus prefers words it has seen before. But words or constituents will be of
            different relative importance in different documents or even sentences.
            Inspired by Hori and Furui (2004), we add to our objective function (see Equation (2))
            a significance score designed to highlight important content words. In Hori and Furui’s
            original formulation each word is weighted by a score similar to un-normalized tf ∗ idf . The
            significance score is not applied indiscriminately to all words in a sentence but solely to
            topic-related words, namely nouns and verbs. Our score differs in one respect. It combines
            document-level with sentence-level significance. So in addition to tf ∗ idf , each word is
            weighted by its level of embedding in the syntactic tree.
            Intuitively, in a sentence with multiply nested clauses, more deeply embedded clauses
            tend to carry more semantic content. This is illustrated in Figure 2 which depicts the
            clause embedding for the sentence “Mr Field has said he will resign if he is not reselected,
            a move which could divide the party nationally”. Here, the most important information is
            conveyed by clauses S3 (he will resign) and S4 (if he is not reselected) which are embedded.
            Accordingly, we should give more weight to words found in these clauses than in the main
            clause (S1 in Figure 2). A simple way to enforce this is to give clauses weight proportional
            to the level of embedding. Our modified significance score becomes:
            I(xi ) =

            Fa
            l
            · fi log
            N
            Fi

            (10)

            where xi is a topic word, fi and Fi are the frequency of xi in the document and corpus
            respectively, Fa is the sum of all topic words in the corpus, l is the number of clause
            4. Compression rate can be also limited to a range by including two inequality constraints.

            411

            Clarke & Lapata

            S1
            S2
            Mr Field has said
            S3
            he will resign
            S4
            if he is not reselected
            , a move
            SBAR
            which could divide the party nationally

            Figure 2: The clause embedding of the sentence “Mr Field has said he will resign if he is
            not reselected, a move which could divide the party nationally”; nested boxes
            correspond to nested clauses.

            constituents above xi , and N is the deepest level of clause embedding. Fa and Fi are
            estimated from a large document collection, fi is document-specific, whereas Nl is sentencespecific. So, in
            Figure 2 the term Nl is 1.0 (4/4) for clause S4 , 0.75 (3/4) for clause S3 , and
            so on. Individual words inherit their weight from their clauses.
            The modified objective function with the significance score is given below:
            max z =

            n
            X

            δi · λI(xi ) +

            i=1
            n−2
            X n−1
            X

            +

            n
            X

            αi · P (xi |start)

            i=1

            n
            X

            γijk · P (xk |xi , xj )

            i=1 j=i+1 k=j+1

            +

            n−1
            X

            n
            X

            βij · P (end|xi , xj )

            (11)

            i=0 j=i+1

            We also add a weighting factor (λ) to the objective, in order to counterbalance the importance of the
            language model and the significance score. The weight is tuned on a small
            parallel corpus. The sequential constraints from Equations (4)–(8) are again used to ensure
            that the trigrams are combined in a valid way.
            3.4.3 Discriminative Model
            As a fully supervised model, we used the discriminative model presented by McDonald
            (2006). This model uses a large-margin learning framework coupled with a feature set
            defined on compression bigrams and syntactic structure.
            Let x = x1 , . . . , xn denote a source sentence with a target compression y = y1 , . . . , ym
            where each yj occurs in x. The function L(yi ) ∈ {1 . . . n} maps word yi in the target com412

            Global Inference for Sentence Compression

            pression to the index of the word in the source sentence, x. We also include the constraint
            that L(yi )< L(yi+1 ) which forces each word in x to occur at most once in the compression
            y. Let the score of a compression y for a sentence x be:
            (12)

            s(x, y)

            This score is factored using a first-order Markov assumption on the words in the target
            compression to give:
            s(x, y) =

            |y|
            X

            s(x, L(yj−1 ), L(yj ))

            (13)

            j=2

            The score function is defined to be the dot product between a high dimensional feature
            representation and a corresponding weight vector:
            s(x, y) =

            |y|
            X

            w · f (x, L(yj−1 ), L(yj ))

            (14)

            j=2

            Decoding in this model amounts to finding the combination of bigrams that maximizes
            the scoring function in (14). McDonald (2006) uses a dynamic programming approach
            where the maximum score is found in a left-to-right manner. The algorithm is an extension
            of Viterbi for the case in which scores factor over dynamic sub-strings (Sarawagi & Cohen,
            2004; McDonald, Crammer, & Pereira, 2005a). This allows back-pointers to be used to
            reconstruct the highest scoring compression as well as the k-best compressions.
            Again this is similar to the trigram language model decoding process (see Section 3.4.1),
            except that here a bigram model is used. Consequently, the ILP formulation is slightly
            simpler than that of the trigram language model. Let:
            δi =

            (

            1 if xi is in the compression
            (1 ≤ i ≤ n)
            0 otherwise

            We then introduce some more decision variables:
            αi =
            βi =
            γij =

            (

            (

            (

            1 if xi starts the compression
            ∀i ∈ [1 . . . n]
            0 otherwise

            1 if word xi ends the compression
            0 otherwise
            ∀i ∈ [1 . . . n]

            1 if sequence xi , xj is in the compression ∀i ∈ [1 . . . n − 1]
            0 otherwise
            ∀j ∈ [i + 1 . . . n]

            The discriminative model can be now expressed as:
            max z =

            n
            X

            αi · s(x, 0, i)
            i=1
            n−1
            n
            X X

            γij · s(x, i, j)

            +

            +

            i=1 j=i+1
            n
            X

            βi · s(x, i, n + 1)

            i=1

            413

            (15)

            Clarke & Lapata

            Constraint 1

            Exactly one word can begin a sentence.
            n
            X

            αi = 1

            (16)

            i=1

            Constraint 2 If a word is included in the sentence it must either start the compression
            or follow another word.

            δj − αj −

            j
            X

            γij = 0

            (17)

            i=1

            ∀j : j ∈ [1 . . . n]
            Constraint 3 If a word is included in the sentence it must be either followed by another
            word or end the sentence.

            δi −

            n
            X

            γij − βi = 0

            (18)

            j=i+1

            ∀i : i ∈ [1 . . . n]

            Constraint 4

            Exactly one word can end a sentence.
            n
            X

            βi = 1

            (19)

            i=1

            Again, the sequential constraints in Equations (16)–(19) are necessary to ensure that the
            resulting combination of bigrams are valid.
            The current formulation provides a single optimal compression given the model. However, McDonald’s (2006)
            dynamic programming algorithm is capable of returning the k-best
            compressions; this is useful for their learning algorithm described later. In order to produce
            k-best compressions, we must rerun the ILP with extra constraints which forbid previous
            solutions. In other words, we first formulate the ILP as above, solve it, add its solution to
            the k-best list, and then create a set of constraints that forbid the configuration of δi decision
            variables which form the current solution. The procedure is repeated until k compressions
            are found.
            The computation of the compression score crucially relies on the dot product between
            a high dimensional feature representation and a corresponding weight vector (see Equation (14)). McDonald
            (2006) employs a rich feature set defined over adjacent words and
            individual parts-of-speech, dropped words and phrases from the source sentence, and dependency structures
            (also of the source sentence). These features are designed to mimic the
            information presented in the previous noisy-channel and decision-tree models of Knight and
            Marcu (2002). Features over adjacent words are used as a proxy to the language model of
            the noisy channel. Unlike other models, which treat the parses as gold standard, McDonald
            uses the dependency information as another form of evidence. Faced with parses that are
            noisy the learning algorithm can reduce the weighting given to those features if they prove
            414

            Global Inference for Sentence Compression

            poor discriminators on the training data. Thus, the model should be much more robust
            and portable across different domains and training corpora.
            The weight vector, w is learned using the Margin Infused Relaxed Algorithm (MIRA,
            Crammer & Singer, 2003) a discriminative large-margin online learning technique (McDonald, Crammer, &
            Pereira, 2005b). This algorithm learns by compressing each sentence and
            comparing the result with the gold standard. The weights are updated so that the score of
            the correct compression (the gold standard) is greater than the score of all other compressions by a margin
            proportional to their loss. The loss function is the number of words falsely
            retained or dropped in the incorrect compression relative to the gold standard. A source
            sentence will have exponentially many compressions and thus exponentially many margin
            constraints. To render learning computationally tractable, McDonald et al. (2005b) create
            constraints only on the k compressions that currently have the highest score, bestk (x; w).
            3.5 Constraints
            We are now ready to describe our compression-specific constraints. The models presented
            in the previous sections contain only sequential constraints and are thus equivalent to their
            original formulation. Our constraints are linguistically and semantically motivated in a
            similar fashion to the grammar checking component of Jing (2000). However, they do
            not rely on any additional knowledge sources (such as a grammar lexicon or WordNet)
            beyond the parse and grammatical relations of the source sentence. We obtain these from
            RASP (Briscoe & Carroll, 2002), a domain-independent, robust parsing system for English.
            However, any other parser with broadly similar output (e.g., Lin, 2001) could also serve our
            purposes. Our constraints revolve around modification, argument structure, and discourse
            related factors.
            Modifier Constraints Modifier constraints ensure that relationships between head words
            and their modifiers remain grammatical in the compression:
            δi − δj ≥ 0

            (20)

            ∀i, j : xj ∈ xi ’s ncmods
            δi − δj ≥ 0

            (21)

            ∀i, j : xj ∈ xi ’s detmods
            Equation (20) guarantees that if we include a non-clausal modifier5 (ncmod) in the compression (such as an
            adjective or a noun) then the head of the modifier must also be included;
            this is repeated for determiners (detmod) in (21). In Table 2 we illustrate how these constraints disallow
            the deletion of certain words (starred sentences denote compressions that
            would not be possible given our constraints). For example, if the modifier word Pasok from
            sentence (1a) is in the compression, then its head Party will also included (see (1b)).
            We also want to ensure that the meaning of the source sentence is preserved in the
            compression, particularly in the face of negation. Equation (22) implements this by forcing
            not in the compression when the head is included (see sentence (2b) in Table 2). A similar
            constraint is added for possessive modifiers (e.g., his, our), including genitives (e.g., John’s
            5. Clausal modifiers (cmod) are adjuncts modifying entire clauses. In the example “he ate the cake because
            he was hungry”, the because-clause is a modifier of the sentence “he ate the cake”.

            415

            Clarke & Lapata

            1a.
            1b.
            2a.
            2b.
            2c.
            3a.
            3b.
            3c.
            3d.
            3e.
            3f.

            He became a power player in Greek Politics in 1974, when he founded the
            socialist Pasok Party.
            *He became a power player in Greek Politics in 1974, when he founded the
            Pasok.
            We took these troubled youth who don’t have fathers, and brought them into
            a room to Dads who don’t have their children.
            *We took these troubled youth who do have fathers, and brought them into a
            room to Dads who do have their children.
            *We took these troubled youth who don’t have fathers, and brought them into
            a room to Dads who don’t have children.
            The chain stretched from Uganda to Grenada and Nicaragua, since the 1970s.
            *Stretched from Uganda to Grenada and Nicaragua, since the 1970s.
            *The chain from Uganda to Grenada and Nicaragua, since the 1970s.
            *The chain stretched Uganda to Grenada and Nicaragua, since the 1970s.
            *The chain stretched from to Grenada and Nicaragua, since the 1970s.
            *The chain stretched from Uganda to Grenada Nicaragua, since the 1970s.
            Table 2: Examples of compressions disallowed by our set of constraints.

            gift), as shown in Equation (23). An example of the possessive constraint is given in
            sentence (2c) in Table 2.
            δi − δj = 0

            (22)

            ∀i, j : xj ∈ xi ’s ncmods ∧ xj = not
            δi − δj = 0

            (23)

            ∀i, j : xj ∈ xi ’s possessive mods
            Argument Structure Constraints We also define a few intuitive constraints that take
            the overall sentence structure into account. The first constraint (Equation (24)) ensures
            that if a verb is present in the compression then so are its arguments, and if any of the
            arguments are included in the compression then the verb must also be included. We thus
            force the program to make the same decision on the verb, its subject, and object (see
            sentence (3b) in Table 2).
            δi − δj = 0

            (24)

            ∀i, j : xj ∈ subject/object of verb xi
            Our second constraint forces the compression to contain at least one verb provided the
            source sentence contains one as well:
            X

            δi ≥ 1

            (25)

            i:xi ∈verbs

            The constraint entails that it is not possible to drop the main verb stretched from sentence (3a) (see also
            sentence (3c) in Table 2).
            416

            Global Inference for Sentence Compression

            Other sentential constraints include Equations (26) and (27) which apply to prepositional phrases and
            subordinate clauses. These constraints force the introducing term
            (i.e., the preposition, or subordinator) to be included in the compression if any word from
            within the syntactic constituent is also included. By subordinator we mean wh-words
            (e.g., who, which, how, where), the word that, and subordinating conjunctions (e.g., after,
            although, because). The reverse is also true, i.e., if the introducing term is included, at
            least one other word from the syntactic constituent should also be included.
            δi − δj ≥ 0

            (26)

            ∀i, j : xj ∈ PP/SUB
            ∧xi starts PP/SUB
            X

            δi − δj ≥ 0

            (27)

            i:xi ∈PP/SUB

            ∀j : xj starts PP/SUB
            As an example consider sentence (3d) from Table 2. Here, we cannot drop the preposition
            from if Uganda is in the compression. Conversely, we must include from if Uganda is in the
            compression (see sentence (3e)).
            We also wish to handle coordination. If two head words are conjoined in the source
            sentence, then if they are included in the compression the coordinating conjunction must
            also be included:
            (1 − δi ) + δj ≥ 1

            (28)

            (1 − δi ) + δk ≥ 1

            (29)

            δi + (1 − δj ) + (1 − δk ) ≥ 1

            (30)

            ∀i, j, k : xj ∧ xk conjoined by xi
            Consider sentence (3f) from Table 2. If both Uganda and Nicaragua are present in the
            compression, then we must include the conjunction and.
            Finally, Equation (31) disallows anything within brackets in the source sentence from
            being included in the compression. This is a somewhat superficial attempt at excluding
            parenthetical and potentially unimportant material from the compression.
            δi = 0

            (31)

            ∀i : xi ∈ bracketed words (inc parentheses)
            Discourse Constraints Our discourse constraint concerns personal pronouns. Specifically, Equation (32)
            forces personal pronouns to be included in the compression. The
            constraint is admittedly more important for generating coherent documents (as opposed to
            individual sentences). It nevertheless has some impact on sentence-level compressions, in
            particular when verbal arguments are missed by the parser. When these are pronominal,
            constraint (32) will result in more grammatical output since some of the argument structure
            of the source sentence will be preserved in the compression.
            δi = 1
            ∀i : xi ∈ personal pronouns
            417

            (32)

            Clarke & Lapata

            We should note that some of the constraints described above would be captured by
            models that learn synchronous deletion rules from a corpus. For example, the noisy-channel
            model of Knight and Marcu (2002) learns not to drop the head when the latter is modified
            by an adjective or a noun, since the transformations DT NN → DT or AJD NN → ADJ are
            almost never seen in the data. Similarly, the coordination constraint (Equations (28)–(30))
            would be enforced using Turner and Charniak’s (2005) special rules — they enhance their
            parallel grammar with rules modeling more structurally complicated deletions than those
            attested in their corpus. In designing our constraints we aimed at capturing appropriate
            deletions for many possible models, including those that do not rely on a training corpus
            or do not have an explicit notion of a parallel grammar (e.g., McDonald, 2006). The
            modification constraints would presumably be redundant for the noisy-channel model, which
            could otherwise benefit from more specialized constraints, e.g., targeting sparse rules or
            noisy parse trees, however we leave this to future work.
            Another feature of the modeling framework presented here is that deletions (or nondeletions) are treated as
            unconditional decisions. For example, we require not to drop the
            noun in adjective-noun sequences if the adjective is not deleted as well. We also require to
            always include a verb in the compression if the source sentence has one. These hardwired decisions could in
            some cases prevent valid compressions from being considered. For instance,
            it is not possible to compress the sentence “this is not appropriate behavior” to “this is
            not appropriate” or“Bob loves Mary and John loves Susan” to “Bob loves Mary and John
            Susan”. Admittedly we lose some expressive power, yet we ensure that the compressions
            will be broadly grammatically, even for unsupervised or semi-supervised models. Furthermore, in practice we
            find that our models consistently outperform non-constraint-based
            alternatives, without extensive constraint engineering.
            3.6 Solving the ILP
            As we mentioned earlier (Section 3.1), solving ILPs is NP-hard. In cases where the coefficient matrix is
            unimodular, it can be shown that the optimal solution to the linear
            program is integral. Although the coefficient matrix in our problems is not unimodular, we
            obtained integral solutions for all sentences we experimented with (approximately 3,000,
            see Section 4.1 for details). We conjecture that this is due to the fact that all of our variables have 0,
            +1 or −1 coefficients in the constraints and therefore our constraint matrix
            shares many properties of a unimodular matrix. We generate and solve an ILP for every
            sentence we wish to compress. Solve times are less than a second per sentence (including
            input-output overheads) for all models presented here.

            4. Experimental Set-up
            Our evaluation experiments were motivated by three questions: (1) Do the constraintbased compression models
            deliver performance gains over non-constraint-based ones? We
            expect better compressions for the model variants which incorporate compression-specific
            constraints. (2) Are there differences among constraint-based models? Here, we would like
            to investigate how much modeling power is gained by the addition of the constraints. For
            example, it may be the case that a state-of-the-art model like McDonald’s (2006) does not
            benefit much from the addition of constraints. And that their effect is much bigger for less
            418

            Global Inference for Sentence Compression

            sophisticated models. (3) How do the models reported in this paper port across domains?
            In particular, we are interested in assessing whether the models and proposed constraints
            are general and robust enough to produce good compressions for both written and spoken
            texts.
            We next describe the data sets on which our models were trained and tested (Section 4.1),
            explain how model parameters were estimated (Section 4.2) and present our evaluation setup
            (Section 4.3). We discuss our results in Section 5.
            4.1 Corpora
            Our intent was to assess the performance of the models just described on written and spoken
            text. The appeal of written text is understandable since most summarization work today
            focuses on this domain. Speech data not only provides a natural test-bed for compression
            applications (e.g., subtitle generation) but also poses additional challenges. Spoken utterances can be
            ungrammatical, incomplete, and often contain artefacts such as false starts,
            interjections, hesitations, and disfluencies. Rather than focusing on spontaneous speech
            which is abundant in these artefacts, we conduct our study on the less ambitious domain
            of broadcast news transcripts. This lies in-between the extremes of written text and spontaneous speech as
            it has been scripted beforehand and is usually read off on autocue.
            Previous work on sentence compression has almost exclusively used the Ziff-Davis corpus
            for training and testing purposes. This corpus originates from a collection of news articles
            on computer products. It was created automatically by matching sentences that occur in
            an article with sentences that occur in an abstract (Knight & Marcu, 2002). The abstract
            sentences had to contain a subset of the source sentence’s words and the word order had
            to remain the same. In earlier work (Clarke & Lapata, 2006) we have argued that the
            Ziff-Davis corpus is not ideal for studying compression for several reasons. First, we showed
            that human-authored compressions differ substantially from the Ziff-Davis which tends to
            be more aggressively compressed. Second, humans are more likely to drop individual words
            than lengthy constituents. Third, the test portion of the Ziff-Davis contains solely 32 sentences. This is
            an extremely small data set to reveal any statistically significant differences
            among systems. In fact, previous studies relied almost exclusively on human judgments for
            assessing the well-formedness of the compressed output, and significance tests are reported
            for by-subjects analyses only.
            We thus focused in the present study on manually created corpora. Specifically, we
            asked annotators to perform sentence compression by removing tokens on a sentence-bysentence basis.
            Annotators were free to remove any words they deemed superfluous provided
            their deletions: (a) preserved the most important information in the source sentence, and
            (b) ensured the compressed sentence remained grammatical. If they wished, they could leave
            a sentence uncompressed by marking it as inappropriate for compression. They were not
            allowed to delete whole sentences even if they believed they contained no information content
            with respect to the story as this would blur the task with abstracting. Following these
            guidelines, our annotators produced compressions of 82 newspaper articles (1,433 sentences)
            from the British National Corpus (BNC) and the American News Text corpus (henceforth
            written corpus) and 50 stories (1,370 sentences) from the HUB-4 1996 English Broadcast
            News corpus (henceforth spoken corpus). The written corpus contains articles from The LA
            419

            Clarke & Lapata

            Times, Washington Post, Independent, The Guardian and Daily Telegraph. The spoken
            corpus contains broadcast news from a variety of networks (CNN, ABC, CSPAN and NPR)
            which have been manually transcribed and segmented at the story and sentence level. Both
            corpora have been split into training, development and testing sets6 randomly on article
            boundaries (with each set containing full stories) and are publicly available from http:
            //homepages.inf.ed.ac.uk/s0460084/data/.
            4.2 Parameter Estimation
            In this work we present three compression models ranging from unsupervised to semisupervised, and fully
            supervised. The unsupervised model simply relies on a trigram language model for driving compression (see
            Section 3.4.1). This was estimated from 25 million tokens of the North American corpus using the
            CMU-Cambridge Language Modeling
            Toolkit (Clarkson & Rosenfeld, 1997) with a vocabulary size of 50,000 tokens and GoodTuring discounting. To
            discourage one-word output we force the ILP to generate compressions whose length is no less than 40% of the
            source sentence (see the constraint in (9)).
            The semi-supervised model is the weighted combination of a word-based significance score
            with a language model (see Section 3.4.2). The significance score was calculated using
            25 million tokens from the American News Text corpus. We optimized its weight (see
            Equation (11)) on a small subset of the training data (three documents in each case) using Powell’s method
            (Press, Teukolsky, Vetterling, & Flannery, 1992) and a loss function
            based on the F-score of the grammatical relations found in the gold standard compression
            and the system’s best compression (see Section 4.3 for details). The optimal weight was
            approximately 1.8 for the written corpus and 2.2 for the spoken corpus.
            McDonald’s (2006) supervised model was trained on the written and spoken training
            sets. Our implementation used the same feature sets as McDonald, the only difference
            being that our phrase structure and dependency features were extracted from the output of
            Roark’s (2001) parser. McDonald uses Charniak’s (2000) parser which performs comparably.
            The model was learnt using k-best compressions. On the development data, we found that
            k = 10 provided the best performance.
            4.3 Evaluation
            Previous studies have relied almost exclusively on human judgments for assessing the wellformedness of
            automatically derived compressions. These are typically rated by naive subjects on two dimensions,
            grammaticality and importance (Knight & Marcu, 2002). Although
            automatic evaluation measures have been proposed (Riezler et al., 2003; Bangalore, Rambow, & Whittaker,
            2000) their use is less widespread, we suspect due to the small size of
            the test portion of the Ziff-Davis corpus which is commonly used in compression work.
            We evaluate the output of our models in two ways. First, we present results using
            an automatic evaluation measure put forward by Riezler et al. (2003). They compare
            the grammatical relations found in the system compressions against those found in a gold
            standard. This allows us to measure the semantic aspects of summarization quality in terms
            of grammatical-functional information and can be quantified using F-score. Furthermore,
            6. The splits are 908/63/462 sentences for the written corpus and 882/78/410 sentences for the spoken
            corpus.

            420

            Global Inference for Sentence Compression

            in Clarke and Lapata (2006) we show that relations-based F-score correlates reliably with
            human judgments on compression output. Since our test corpora are larger than ZiffDavis (by more than a
            factor of ten), differences among systems can be highlighted using
            significance testing.
            Our implementation of the F-score measure used the grammatical relations annotations
            provided by RASP (Briscoe & Carroll, 2002). This parser is particularly appropriate for the
            compression task since it provides parses for both full sentences and sentence fragments and
            is generally robust enough to analyze semi-grammatical sentences. We calculated F-score
            over all the relations provided by RASP (e.g., subject, direct/indirect object, modifier; 15
            in total).
            In line with previous work we also evaluate our models by eliciting human judgments.
            Following the work of Knight and Marcu (2002), we conducted two separate experiments.
            In the first experiment participants were presented with a source sentence and its target
            compression and asked to rate how well the compression preserved the most important
            information from the source sentence. In the second experiment, they were asked to rate
            the grammaticality of the compressed outputs. In both cases they used a five point rating
            scale where a high number indicates better performance. We randomly selected 21 sentences
            from the test portion of each corpus. These sentences were compressed automatically by
            the three models presented in this paper with and without constraints. We also included
            gold standard compressions. Our materials thus consisted of 294 (21 × 2 × 7) sourcetarget sentences. A Latin
            square design ensured that subjects did not see two different
            compressions of the same sentence. We collected ratings from 42 unpaid volunteers, all self
            reported native English speakers. Both studies were conducted over the Internet using a
            custom build web interface. Examples of our experimental items are given in Table 3.

            5. Results
            Let us first discuss our results when compression output is evaluated in terms of F-score.
            Tables 4 and 5 illustrate the performance of our models on the written and spoken corpora,
            respectively. We also present the compression rate7 for each system. In all cases the
            constraint-based models (+Constr) yield better F-scores than the non-constrained ones.
            The difference is starker for the semi-supervised model (Sig). The constraints bring an
            improvement of 17.2% on the written corpus and 18.3% on the spoken corpus. We further
            examined whether performance differences among models are statistically significant, using
            the Wilcoxon test. On the written corpus all constraint models significantly outperform the
            models without constraints. The same tendency is observed on the spoken corpus except for
            the model of McDonald (2006) which performs comparably with and without constraints.
            We also wanted to establish which is the best constraint model. On both corpora we
            find that the language model performs worst, whereas the significance model and McDonald
            perform comparably (i.e., the F-score differences are not statistically significant). To get
            a feeling for the difficulty of the task, we calculated how much our annotators agreed in
            their compression output. The inter-annotator agreement (F-score) on the written corpus
            was 65.8% and on the spoken corpus 73.4%. The agreement is higher on spoken texts since
            they consists of many short utterances (e.g., Okay, That’s it for now, Good night) that can
            7. The term refers to the percentage of words retained from the source sentence in the compression.

            421

            Clarke & Lapata

            Source

            The aim is to give councils some control over the future growth of second
            homes.
            Gold
            The aim is to give councils control over the growth of homes.
            LM
            The aim is to the future.
            LM+Constr The aim is to give councils control.
            Sig
            The aim is to give councils control over the future growth of homes.
            Sig+Constr The aim is to give councils control over the future growth of homes.
            McD
            The aim is to give councils.
            McD+Constr The aim is to give councils some control over the growth of homes.
            Source
            The Clinton administration recently unveiled a new means to encourage
            brownfields redevelopment in the form of a tax incentive proposal.
            Gold
            The Clinton administration unveiled a new means to encourage brownfields redevelopment in a tax incentive
            proposal.
            LM
            The Clinton administration in the form of tax.
            LM+Constr The Clinton administration unveiled a means to encourage redevelopment in the form.
            Sig
            The Clinton administration unveiled a encourage brownfields redevelopment form tax proposal.
            Sig+Constr The Clinton administration unveiled a means to encourage brownfields
            redevelopment in the form of tax proposal.
            McD
            The Clinton unveiled a means to encourage brownfields redevelopment
            in a tax incentive proposal.
            McD+Constr The Clinton administration unveiled a means to encourage brownfields
            redevelopment in the form of a incentive proposal.
            Table 3: Example compressions produced by our systems (Source: source sentence, Gold:
            gold-standard compression, LM: language model compression, LM+Constr: language model compression with
            constraints, Sig: significance model, Sig+Constr:
            significance model with constraints, McD: McDonald’s (2006) compression model,
            McD+Constr: McDonald’s (2006) compression model with constraints).

            be compressed only very little or not all. Note that there is a marked difference between the
            automatic and human compressions. Our best performing systems are inferior to human
            output by more than 20 F-score percentage points.
            Differences between the automatic systems and the human output are also observed
            with respect to the compression rate. As can be seen the language model compresses most
            aggressively, whereas the significance model and McDonald tend to be more conservative
            and closer to the gold standard. Interestingly, the constraints do not necessarily increase
            the compression rate. The latter increases for the significance model but decreases for
            the language model and remains relatively constant for McDonald. It is straightforward to
            impose the same compression rate for all constraint-based models (e.g., by forcing the model
            P
            to retain b tokens ni=1 δi = b). However, we refrained from doing this since we wanted our
            422

            Global Inference for Sentence Compression

            Models
            LM
            Sig
            McD
            LM+Constr
            Sig+Constr
            McD+Constr
            Gold

            CompR
            46.2
            60.6
            60.1
            41.2
            72.0
            63.7
            70.3

            F-score
            18.4
            23.3
            36.0
            28.2∗
            40.5∗†
            40.8∗†
            —

            Table 4: Results on the written corpus; compression rate (CompR) and grammatical relation F-score (F-score);
            ∗ : +Constr model is significantly different from model
            without constraints; † : significantly different from LM+Constr.
            Models
            LM
            Sig
            McD
            LM+Constr
            Sig+Constr
            McD+Constr
            Gold

            CompR
            52.0
            60.9
            68.6
            49.5
            78.4
            68.5
            76.1

            F-score
            25.4
            30.4
            47.6
            34.8∗
            48.7∗†
            50.1†
            —

            Table 5: Results on the spoken corpus; compression rate (CompR) and grammatical relation F-score (F-score);
            ∗ : +Constr model is significantly different from without
            constraints; † : significantly different from LM+Constr.

            models to regulate the compression rate for each sentence individually according to its
            specific information content and structure.
            We next consider the results of our human study which assesses in more detail the quality
            of the generated compressions on two dimensions, namely grammaticality and information
            content. F-score conflates these two dimensions and therefore in theory could unduly reward
            a system that produces perfectly grammatical output without any information loss. Tables 6
            and 7 show the mean ratings8 for each system (and the gold standard) on the written and
            spoken corpora, respectively. We first performed an Analysis of Variance (Anova) to
            examine the effect of different system compressions. The Anova revealed a reliable effect
            on both grammaticality and importance for each corpus (the effect was significant by both
            subjects and items (p
            <
            0.01)).
            We next examine the impact of the constraints (+Constr in the tables). In most cases
            we observe an increase in ratings for both grammaticality and importance when a model
            is supplemented constraints. Post-hoc Tukey tests reveal that the grammaticality and
            importance ratings of the language model and significance model significantly improve with
            8. All statistical tests reported subsequently were done using the mean ratings.

            423

            Clarke & Lapata

            Models

            Grammar

            Importance

            LM
            Sig
            McD

            2.25†$
            3.05†

            1.82†$
            2.99†$
            2.84†

            LM+Constr
            Sig+Constr
            McD+Constr
            Gold

            3.47∗†
            3.76∗
            3.50†
            4.25

            2.37∗†$
            3.53∗
            3.17†
            3.98

            2.26†$

            Table 6: Results on the written text corpus; average grammaticality score (Grammar) and
            average importance score (Importance) for human judgments; ∗ : +Constr model
            is significantly different from model without constraints; † : significantly different
            from gold standard; $ : significantly different from McD+Constr.

            Models

            Grammar

            Importance

            LM
            Sig
            McD

            2.20†$
            2.29†$
            3.33†

            1.56†
            2.64†
            3.32†

            LM+Constr
            Sig+Constr
            McD+Constr
            Gold

            3.18∗†
            3.80∗†
            3.60†
            4.45

            2.49∗†$
            3.69∗†
            3.31†
            4.25

            Table 7: Results on the spoken text corpus; average grammaticality score (Grammar) and
            average importance score (Importance) for human judgments; ∗ : +Constr model
            is significantly different from model without constraints; † : significantly different
            from gold standard; $ : significantly different from McD+Constr.

            the constraints (α
            <
            0.01). In contrast, McDonald’s system sees a numerical improvement
            with the additional constraints, but this difference is not statistically significant. These
            tendencies are observed on the spoken and written corpus.
            Upon closer inspection, we can see that the constraints influence considerably the
            grammaticality of the unsupervised and semi-supervised systems. Tukey tests reveal that
            LM+Constr and Sig+Constr are as grammatical as McD+Constr. In terms of importance,
            Sig+Constr and McD+Constr are significantly better than LM+Constr (α
            <
            0.01). This
            is not surprising given that LM+Constr is a very simple model without a mechanism for
            highlighting important words in a sentence. Interestingly, Sig+Constr performs as well
            as McD+Constr in retaining the most important words, despite the fact that it requires
            minimal supervision. Although constraint-based models overall perform better than models without
            constraints, they receive lower ratings (for grammaticality and importance) in
            comparison to the gold standard. And the differences are significant in most cases.
            424

            Global Inference for Sentence Compression

            In summary, we observe that the constraints boost performance. This is more pronounced for compression
            models that are either unsupervised or use small amounts of
            parallel data. For example, a simple model like Sig yields performance comparable to
            McDonald (2006) when constraints are taken into account. This is an encouraging result
            suggesting that ILP can be used to create good compression models with relatively little
            effort (i.e., without extensive feature engineering or elaborate knowledge sources). Performance gains are
            also obtained for competitive models like McDonald’s that are fully
            supervised. But these gains are smaller, presumably because the initial model contains a
            rich feature representation consisting of syntactic information and generally does a good job
            at producing grammatical output. Finally, our improvements are consistent across corpora
            and evaluation paradigms.
        </corps>
        <conclusion>6. Conclusions
            In this paper we have presented a novel method for automatic sentence compression. A key
            aspect of our approach is the use of integer linear programming for inferring globally optimal
            compressions in the presence of linguistically motivated constraints. We have shown how
            previous formulations of sentence compression can be recast as ILPs and extended these
            models with local and global constraints ensuring that the compressed output is structurally
            and semantic well-formed. Contrary to previous work that has employed ILP solely for
            decoding, our models integrate learning with inference in a unified framework.
            Our experiments have demonstrated the advantages of the approach. Constraint-based
            models consistently bring performance gains over models without constraints. These improvements are more
            impressive for models that require little or no supervision. A case
            in point here is the significance model discussed above. The no-constraints incarnation of
            this model performs poorly and considerably worse than McDonald’s (2006) state-of-the-art
            model. The addition of constraints improves the output of this model so that its performance is
            indistinguishable from McDonald. Note that the significance model requires a
            small amount of training data (50 parallel sentences), whereas McDonald is trained on hundreds of sentences.
            It also presupposes little feature engineering, whereas McDonald utilizes
            thousands of features. Some effort is associated with framing the constraints, however these
            are created once and are applied across models and corpora. We have also observed small
            performance gains for McDonald’s system when the latter is supplemented with constraints.
            Larger improvements are possible with more sophisticated constraints, however our intent
            was to devise a set of general constraints that are not tuned to the mistakes of any specific
            system in particular.
            Future improvements are many and varied. An obvious extension concerns our constraint set. Currently our
            constraints are mostly syntactic and consider each sentence in
            isolation. By incorporating discourse constraints we could highlight words that are important at the
            document-level. Presumably words topical in a document should be retained in
            the compression. Other constraints could manipulate the compression rate. For example,
            we could encourage a higher compression rate for longer sentences. Another interesting
            direction includes the development of better objective functions for the compression task.
            The objective functions presented so far rely on first or second-order Markov assumptions.
            Alternative objectives could take into account the structural similarity between the source
            425

            Clarke & Lapata

            sentence and its target compression; or whether they share the same content which could
            be operationalized in terms of entropy.
            Beyond the task and systems presented in this paper, we believe the approach holds
            promise for other generation applications using decoding algorithms for searching the space
            of possible outcomes. Examples include sentence-level paraphrasing, headline generation,
            and summarization.

            Acknowledgments
            We are grateful to our annotators Vasilis Karaiskos, Beata Kouchnir, and Sarah Luger.
            Thanks to Jean Carletta, Frank Keller, Steve Renals, and Sebastian Riedel for helpful
            comments and suggestions and to the anonymous referees whose feedback helped to substantially improve the
            present paper. Lapata acknowledges the support of EPSRC (grant
            GR/T04540/01). A preliminary version of this work was published in the proceedings of
            ACL 2006.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations and the pushdown assembler. Journal of
            Computer and System Sciences, 3, 37–56.
            Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation metrics for generation.
            In Proceedings of the first International Conference on Natural Language Generation,
            pp. 1–8, Mitzpe Ramon, Israel.
            Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning for natural language
            generation. In Proceedings of the Human Language Technology Conference of the
            North American Chapter of the Association for Computational Linguistics, pp. 359–
            366, New York, NY, USA.
            Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs.
            In Proceedings of the 2006 Conference on Empirical Methods in Natural Language
            Processing, pp. 189–198, Sydney, Australia.
            Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation of general text. In
            Proceedings of the Third International Conference on Language Resources and Evaluation, pp. 1499–1504, Las
            Palmas, Gran Canaria.
            Charniak, E. (2000). A maximum-entropy-inspired parser. In Proceedings of the 1st North
            American Annual Meeting of the Association for Computational Linguistics, pp. 132–
            139, Seattle, WA, USA.
            Clarke, J., & Lapata, M. (2006). Models for sentence compression: A comparison across
            domains, training requirements and evaluation measures. In Proceedings of the 21st
            International Conference on Computational Linguistics and 44th Annual Meeting of
            the Association for Computational Linguistics, pp. 377–384, Sydney, Australia.
            Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using the CMU–
            Cambridge toolkit. In Proceedings of Eurospeech’97, pp. 2707–2710, Rhodes, Greece.
            426

            Global Inference for Sentence Compression

            Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction to Algorithms. The
            MIT Press.
            Corston-Oliver, S. (2001). Text Compaction for Display on Very Small Screens. In Proceedings of the Workshop
            on Automatic Summarization at the 2nd Meeting of the North
            American Chapter of the Association for Computational Linguistics, pp. 89–98, Pittsburgh, PA, USA.
            Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms for multiclass problems. Journal of
            Machine Learning Research, 3, 951–991.
            Dantzig, G. B. (1963). Linear Programming and Extensions. Princeton University Press,
            Princeton, NJ, USA.
            Denis, P., & Baldridge, J. (2007). Joint determination of anaphoricity and coreference
            resolution using integer programming. In Human Language Technologies 2007: The
            Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of
            the Main Conference, pp. 236–243, Rochester, NY.
            Dras, M. (1999). Tree Adjoining Grammar and the Reluctant Paraphrasing of Text. Ph.D.
            thesis, Macquarie University.
            Galley, M., & McKeown, K. (2007). Lexicalized markov grammars for sentence compression.
            In In Proceedings of the North American Chapter of the Association for Computational
            Linguistics, pp. 180–187, Rochester, NY, USA.
            Gomory, R. E. (1960). Solving linear programming problems in integers. In Bellman,
            R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings of Symposia in Applied
            Mathematics, Vol. 10, Providence, RI, USA.
            Grefenstette, G. (1998). Producing Intelligent Telegraphic Text Reduction to Provide an
            Audio Scanning Service for the Blind. In Hovy, E., & Radev, D. R. (Eds.), Proceedings
            of the AAAI Symposium on Intelligent Text Summarization, pp. 111–117, Stanford,
            CA, USA.
            Hori, C., & Furui, S. (2004). Speech summarization: an approach through word extraction
            and a method for evaluation. IEICE Transactions on Information and Systems, E87D (1), 15–25.
            Jing, H. (2000). Sentence reduction for automatic text summarization. In Proceedings of
            the 6th Applied Natural Language Processing Conference, pp. 310–315, Seattle,WA,
            USA.
            Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: a probabilistic
            approach to sentence compression. Artificial Intelligence, 139 (1), 91–107.
            Land, A. H., & Doig, A. G. (1960). An automatic method for solving discrete programming
            problems. Econometrica, 28, 497–520.
            Lin, C.-Y. (2003). Improving summarization performance by sentence compression — a pilot
            study. In Proceedings of the 6th International Workshop on Information Retrieval with
            Asian Languages, pp. 1–8, Sapporo, Japan.
            Lin, D. (2001). LaTaT: Language and text analysis tools. In Proceedings of the first Human
            Language Technology Conference, pp. 222–227, San Francisco, CA, USA.
            427

            Clarke & Lapata

            Marciniak, T., & Strube, M. (2005). Beyond the pipeline: Discrete optimization in NLP. In
            Proceedings of the Ninth Conference on Computational Natural Language Learning,
            pp. 136–143, Ann Arbor, MI, USA.
            McDonald, R. (2006). Discriminative sentence compression with soft syntactic constraints.
            In Proceedings of the 11th Conference of the European Chapter of the Association for
            Computational Linguistics, Trento, Italy.
            McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation with structured multilabel
            classification. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods
            in Natural Language Processing, pp.
            987–994, Vancouver, BC, Canada.
            McDonald, R., Crammer, K., & Pereira, F. (2005b). Online large-margin training of dependency parsers. In
            43rd Annual Meeting of the Association for Computational
            Linguistics, pp. 91–98, Ann Arbor, MI, USA.
            Nemhauser, G. L., & Wolsey, L. A. (1988). Integer and Combinatorial Optimization. WileyInterscience series
            in discrete mathematicals and opitmization. Wiley, New York, NY,
            USA.
            Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M. (2004). Probabilistic
            sentence reduction using support vector machines. In Proceedings of the 20th international conference on
            Computational Linguistics, pp. 743–749, Geneva, Switzerland.
            Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical
            Recipes in C: The Art of Scientific Computing. Cambridge University Press, New
            York, NY, USA.
            Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer linear programming
            inference. In Proceedings of the International Conference on
            Computational Linguistics, pp. 1346–1352, Geneva, Switzerland.
            Riedel, S., & Clarke, J. (2006). Incremental integer linear programming for non-projective
            dependency parsing. In Proceedings of the 2006 Conference on Empirical Methods in
            Natural Language Processing, pp. 129–137, Sydney, Australia.
            Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensation
            using ambiguity packing and stochastic disambiguation methods for lexical-functional
            grammar. In Human Language Technology Conference and the 3rd Meeting of the
            North American Chapter of the Association for Computational Linguistics, pp. 118–
            125, Edmonton, Canada.
            Roark, B. (2001). Probabilistic top-down parsing and language modeling. Computational
            Linguistics, 27 (2), 249–276.
            Roth, D. (1998). Learning to resolve natural language ambiguities: A unified approach. In
            In Proceedings of the 15th of the American Association for Artificial Intelligence, pp.
            806–813, Madison, WI, USA.
            Roth, D., & Yih, W. (2004). A linear programming formulation for global inference in
            natural language tasks. In Proceedings of the Annual Conference on Computational
            Natural Language Learning, pp. 1–8, Boston, MA, USA.
            428

            Global Inference for Sentence Compression

            Roth, D., & Yih, W. (2005). Integer linear programming inference for conditional random
            fields. In Proceedings of the International Conference on Machine Learning, pp. 737–
            744, Bonn.
            Sarawagi, S., & Cohen, W. W. (2004). Semi-markov conditional random fields for information extraction. In
            Advances in Neural Information Processing Systems, Vancouver,
            BC, Canada.
            Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars. In Proceedings of the 13th
            International Conference on Computational Linguistics, pp. 253–258,
            Helsinki, Finland.
            Turner, J., & Charniak, E. (2005). Supervised and unsupervised learning for sentence
            compression. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp.
            290–297, Ann Arbor, MI, USA.
            Vandeghinste, V., & Pan, Y. (2004). Sentence compression for automated subtitling: A
            hybrid approach. In Marie-Francine Moens, S. S. (Ed.), Text Summarization Branches
            Out: Proceedings of the ACL-04 Workshop, pp. 89–95, Barcelona, Spain.
            Williams, H. P. (1999). Model Building in Mathematical Programming (4th edition). Wiley.
            Winston, W. L., & Venkataramanan, M. (2003). Introduction to Mathematical Programming: Applications and
            Algorithms (4th edition). Duxbury.
            Zajic, D., Door, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentence
            compression as a tool for document summarization tasks. Information Processing
            Management Special Issue on Summarization, 43 (6), 1549–1570.

            429
        </biblio>
    </article>
    <article>
        <preamble>hybrid_approach.txt</preamble>
        <titre>Sentence Compression for Automated Subtitling: A Hybrid Approach</titre>
        <auteur>Vincent Vandeghinste and Yi Pan</auteur>
        <abstract>In this paper a sentence compression tool is described. We describe how an input sentence gets
            analysed by using a.o. a tagger, a shallow parser and a subordinate clause detector, and how, based on this
            analysis, several compressed versions of this sentence are generated, each with an associated estimated
            probability. These probabilities were estimated from a parallel transcript/subtitle corpus. To avoid
            ungrammatical sentences, the tool also makes use of a number of rules. The evaluation was done on three
            different pronunciation speeds, averaging sentence reduction rates of 40% to 17%. The number of reasonable
            reductions ranges between 32.9% and 51%, depending on the average estimated pronunciation speed.
        </abstract>
        <introduction>A sentence compression tool has been built with the purpose of automating subtitle generation for
            the deaf and hard-of-hearing. Verbatim transcriptions cannot be presented as the subtitle presentation time
            is between 690 and 780 characters per minute, which is more or less 5.5 seconds for two lines (ITC, 1997),
            (Dewulf and Saerens, 2000), while the average speech rate contains a lot more than the equivalent of 780
            characters per minute. The actual amount of compression needed depends on the speed of the speaker and on
            the amount of time available after the sentence. In documentaries, for instance, there are often large
            silent intervals between two sentences, the speech is often slower and the speaker is off-screen, so the
            available presentation time is longer. When the speaker is off-screen, the synchrony of the subtitles with
            the speech is of minor importance. When subtitling the news the speech rate is often very high so the amount
            of reduction needed to allow the synchronous presentation of subtitles and speech is much greater. The
            sentence compression rate is a parameter which can be set for each sentence. Note that the sentence
            compression tool de- scribed in this paper is not a subtitling tool. When subtitling, only when a sentence
            needs to be reduced, and the amount of reduction is known, the sentence is sent to the sentence compression
            tool. So the sentence compression tool is a module of an automated subtitling tool. The output of the
            sentence compression tool needs to be processed according to the subtitling guidelines like (Dewulf and
            Saerens, 2000), in order to be in the correct lay-out which makes it usable for actual subtitling. Manually
            post-editing the subtitles will still be required, as for some sentences no automatic compression is
            generated. In real subtitling it often occurs that the sentences are not compressed, but to keep the
            subtitles synchronized with the speech, some sentences are entirely removed. In section 2 we describe the
            processing of a sentence in the sentence compressor, from input to output. In section 3 we describe how the
            system was evaluated and the results of the evaluation. Section
        </introduction>
        <corps>evaluated and the results of the evaluation. Section
            4 contains the conclusions.

            2 From Full Sentence to Compressed
            Sentence
            The sentence compression tool is inspired by (Jing,
            2001). Although her goal is text summarization
            and not subtitling, her sentence compression system
            could serve this purpose.
            She uses multiple sources of knowledge on which
            her sentence reduction is based. She makes use of
            a corpus of sentences, aligned with human-written
            sentence reductions which is similar to the parallel
            corpus we use (Vandeghinste and Tjong Kim Sang,
            2004). She applies a syntactic parser to analyse the
            syntactic structure of the input sentences. As there
            was no syntactic parser available for Dutch (Daelemans and Strik, 2002), we created ShaRPA (Vandeghinste,
            submitted), a shallow rule-based parser
            which could give us a shallow parse tree of the
            input sentence. Jing uses several other knowledge sources, which are not used (not available for
            Dutch) or not yet used in our system (like WordNet).

            In figure 1 the processing flow of an input sentence is sketched.
            Input Sentence

            Tagger

            Abbreviator

            Numbers to Digits

            Chunker

            Subordinate Clause Detector

            Shallow Parse
            Tree

            Compressor

            Compressed
            Sentence

            Grammar
            Rules

            Removal,
            Non−removal,
            Reduction
            Database

            comes EU) and replaces the full form with its abbreviation. The database can also contain the tag
            of the abbreviated part (E.g. the tag for EU is
            N(eigen,zijd,ev,basis,stan) [E: singular non-neuter
            proper noun]).
            In a third step, all numbers which are written in
            words in the input are replaced by their form in digits. This is done for all numbers which are smaller
            than one million, both for cardinal and ordinal numerals.
            In a fourth step, the sentence is sent to ShaRPa,
            which will result in a shallow parse-tree of the sentence. The chunking accuracy for noun phrases
            (NPs) has an F-value of 94.7%, while the chunking accuracy of prepositional phrases (PPs) has an
            F-value of 95.1% (Vandeghinste, submitted).
            A last step before the actual sentence compression consists of rule-based clause-detection: Relative phrases
            (RELP), subordinate clauses (SSUB)
            and OTI-phrases (OTI is om ... te + infinitive1 ) are
            detected. The accuracy of these detections was evaluated on 30 files from the CGN component of readaloud
            books, which contained 7880 words. The
            evaluation results are presented in table 1.
            Type of S
            OTI
            RELP
            SSUB

            Word Reducer

            2.1 Sentence Analysis
            In order to apply an accurate sentence compression,
            we need a syntactic analysis of the input sentence.
            In a first step, the input sentence gets tagged for
            parts-of-speech. Before that, it needs to be transformed into a valid input format for the part-ofspeech
            tagger. The tagger we use is TnT (Brants,
            2000) , a hidden Markov trigram tagger, which was
            trained on the Spoken Dutch Corpus (CGN), Internal Release 6. The accuracy of TnT trained on CGN
            is reported to be 96.2% (Oostdijk et al., 2002).
            In a second step, the sentence is sent to the
            Abbreviator. This tool connects to a database
            of common abbreviations, which are often pronounced in full words (E.g. European Union be-

            Recall
            65.22%
            68.89%
            60.77%

            F-value
            68.18%
            69.27%
            58.74%

            Table 1: Clause Detection Accuracy

            Figure 1: Sentence Processing Flow Chart
            First we describe how the sentence is analysed
            (2.1), then we describe how the actual sentence
            compression is done (2.2), and after that we describe how words can be reduced for extra compression (2.3).
            The final part describes the selection of
            the ouput sentence (2.4).

            Precision
            71.43%
            69.66%
            56.83%

            The errors are mainly due to a wrong analysis
            of coordinating conjunctions, which is not only the
            weak point in the clause-detection module, but also
            in ShaRPa. A full parse is needed to accurately
            solve this problem.
            2.2

            Sentence Compression

            For each chunk or clause detected in the previous
            steps, the probabilities of removal, non-removal and
            reduction are estimated. This is described in more
            detail in 2.2.1.
            Besides the statistical component in the compression, there are also a number of rules in the compression
            program, which are described in more detail in 2.2.2.
            The way the statistical component and the rulebased component are combined is described in
            2.2.3.
            1

            There is no equivalent construction in English. OTI is a
            VP-selecting complementizer.

            2.2.1 Use of Statistics
            Chunk and clause removal, non-removal and reduction probabilities are estimated from the frequencies
            of removal, non-removal and reduction of certain
            types of chunks and clauses in the parallel corpus.
            The parallel corpus consists of transcripts of television programs on the one hand and the subtitles of
            these television programs on the other hand.
            A detailed description of how the parallel corpus
            was collected, and how the sentences and chunks
            were aligned is given in (Vandeghinste and Tjong
            Kim Sang, 2004).
            All sentences in the source corpus (transcripts)
            and the target corpus (subtitles) are analysed in the
            same way as described in section 2.1, and are chunk
            aligned. The chunk alignment accuracy is about
            95% (F-value).
            We estimated the removal, non-removal and reduction probabilities for the chunks of the types NP,
            PP, adjectival phrase (AP), SSUB, RELP, and OTI,
            based on their chunk removal, non-removal and reduction frequencies.
            For the tokens not belonging to either of these
            types, the removal and non-removal probabilities
            were estimated based on the part-of-speech tag for
            those words. A reduced tagset was used, as the original CGN-tagset (Van Eynde, 2004) was too finegrained and
            would lead to a multiplication of the
            number of rules which are now used in ShaRPa. The
            first step in SharPa consists of this reduction.
            For the PPs, the SSUBs and the RELPs, as well
            as for the adverbs, the chunk/tag information was
            considered as not fine-grained enough, so the estimation of the removal, non-removal and reduction
            probabilities for these types are based on the
            first word of those phrases/clauses and the reduction, removal and non-removal probabilities of such
            phrases in the parallel corpus, as the first words of
            these chunk-types are almost always the heads of
            the chunk. This allows for instance to make the
            distinction between several adverbs in one sentence,
            so they do not all have the same removal and nonremoval probabilities. A disadvantage is that this
            approach leads to sparse data concerning the less
            frequent adverbs, for which a default value (average
            over all adverbs) will be employed.
            An example : A noun phrase.
            de grootste Belgische bank
            [E: the largest Belgian bank]
            After tagging and chunking the sentence and after detecting subordinate clauses, for every nonterminal node
            in the shallow parse tree we retrieve
            the measure of removal (X), of non-removal (=) and

            of reduction2 ( ). For the terminal nodes, only the
            measures of removal and of non-removal are used.
            NP
            = 0.54
            X 0.27
             0.05

            DET
            = 0.68
            X 0.28
            de

            ADJ
            = 0.56
            X 0.35
            grootste

            ADJ
            = 0.56
            X 0.35
            Belgische

            N
            = 0.65
            X 0.26
            bank

            For every combination the probability estimate
            is calculated. So if we generate all possible compressions (including no compression), the phrase
            de grootste Belgische bank will get the
            probability estimate 
            
              . For the phrase de Belgische
            bank the probability estimate is ! "
            #$$%& ## , and so on for the
            other alternatives.
            In this way, the probability estimate of all possible alternatives is calculated.
            2.2.2 Use of Rules
            As the statistical information allows the generation
            of ungrammatical sentences, a number of rules were
            added to avoid generating such sentences. The procedure keeps the necessary tokens for each kind of
            node. The rules were built in a bootstrapping manner
            In some of these rules, this procedure is applied
            recursively. These are the rules implemented in our
            system:

            ' If a node is of type SSUB or RELP, keep the
            first word.

            ' If a node is of type S, SSUB or RELP, keep
            – the verbs. If there are prepositions which
            are particles of the verb, keep the prepositions. If there is a prepositional phrase
            which has a preposition which is in the
            complements list of the verb, keep the
            necessary tokens3 of that prepositional
            phrase.
            2

            These measures are estimated probabilities and do not need
            to add up to 1, because in the parallel training corpus, sometimes a match was detected with a chunk which
            was not a reduction of the source chunk or which was not identical to the
            source chunk: the chunk could be paraphrased, or even have
            become longer.
            3
            Recursive use of the rules

            – each token which is in the list of negative words. These words are kept to avoid
            altering the meaning of the sentence by
            dropping words which negate the meaning.
            – the necessary tokens of the te + infinitives
            (TI).
            – the conjunctions.
            – the necessary tokens of each NP.
            – the numerals.
            – the adverbially used adjectives.

            ' If a node is of type NP, keep
            – each noun.
            – each nominalised adjectival phrase.
            – each token which is in the list of negative
            words.
            – the determiners.

            2.3

            – the numerals.

            After the generation of several grammatical reductions, which are ordered according to their probability
            estimated by the product of the removal,
            non-removal and reduction probabilities of all its
            chunks, for every word in every compressed alternative of the sentence it is checked whether the word
            can be reduced.
            The words are sent to a WordSplitter-module,
            which takes a word as its input and checks if it is
            a compound by trying to split it up in two parts:
            the modifier and the head. This is done by lexicon
            lookup of both parts. If this is possible, it is checked
            whether the modifier and the head can be recompounded according to the word formation rules for
            Dutch (Booij and van Santen, 1995), (Haeseryn et
            al., 1997). This is done by sending the modifier
            and the head to a WordBuilding-module, which is
            described in more detail in (Vandeghinste, 2002).
            This is a hybrid module combining the compounding rules with statistical information about the frequency of
            compounds with the samen head, the frequency of compounds with the same modifier, and
            the number of different compounds with the same
            head.
            Only if this module allows the recomposition of
            the modifier and the head, the word can be considered to be a compound, and it can potentially be reduced to
            its head, removing the modifier.
            If the words occur in a database which contains
            a list of compounds which should not be split up,
            the word cannot be reduced. For example, the
            word voetbal [E: football] can be split up and recompounded according to the word formation rules

            – the indefinite prenominal pronouns.

            ' If a node is of type PP, keep
            – the preposition.
            – the determiners.
            – the necessary tokens of the NPs.

            ' If the node is of type adjectival phrase, keep
            – the head of the adjectival phrase.
            – the prenominal numerals.
            – each word which is in the list of negative
            words.

            ' If the node is of type OTI, keep
            – the verbs.
            – the te + infinitives.

            ' If the node is of type TI, keep the node.
            ' If the node is a time phrase4 , keep it.

            These rules are chosen because in tests on earlier
            versions of the system, using a different test set, ungrammatical output was generated. By using these
            rules the output should be grammatical, provided
            that the input sentence was analysed correctly.
            4

            2.2.3 Combining Statistics and Rules
            In the current version of the system, in a first stage
            all variations on a sentence are generated in the statistical part, and they are ranked according to their
            probability. In a second stage, all ungrammatical
            sentences are (or should be) filtered out, so the only
            sentence alternatives which remain should be grammatical ones.
            This is true, only if tagging as well as chunking
            were correct. If errors are made on these levels, the
            generation of an ungrammatical alternative is still
            possible.
            For efficiency reasons, a future version of the system should combine the rules and statistics in one
            stage, so that the statistical module only generates
            grammatically valid sentence alternatives, although
            there is no effect on correctness, as the resulting sentence alternatives would be the same if statistics
            and
            rules were better integrated.

            A time phrase, as defined in ShaRPa is used for special
            phrases, like dates, times, etc. E.g. 27 september 1998, kwart
            voor drie [E: quarter to three].

            Word Reduction

            for Dutch (voet [E: foot] and bal [E: ball]), but
            we should not replace the word voetbal with the
            word bal if we want an accurate compression, with
            the same meaning as the original sentence, as this
            would alter the meaning of the sentence too much.
            The word voetbal has (at least) two different meanings: soccer and the ball with which soccer is
            played. Reducing it to bal would only keep the second meaning. The word gevangenisstraf [E: prison
            sentence] can be split up and recompounded (gevangenis [E: prison] and straf [E: punishment]). We
            can replace the word gevangenisstraf by the word
            straf. This would still alter the meaning of the sentence, but not to the same amount as it would have
            been altered in the case of the word voetbal.
            2.4 Selection of the Compressed Sentence
            Applying all the steps described in the previous sections results in an ordered list of sentence
            alternatives, which are supposedly grammatically correct.
            When word reduction was possible, the wordreduced alternative is inserted in this list, just after
            its full-words equivalent.
            The first sentence in this list with a length smaller
            than the maximal length (depending on the available
            presentation time) is selected.
            In a future version of the system, the word reduction information can be integrated in a better way
            with the rest of the module, by combining the probability of reduction/non-reduction of a word with the
            probability of the sentence alternative. The reduction probability of a word would then play its role
            in the estimated probability of the compressed sentence alternative containing this reduced word.

            3 Evaluation
            The evaluation of a sentence compression module is
            not an easy task. The output of the system needs to
            be judged manually for its accuracy. This is a very
            time consuming task. Unlike (Jing, 2001), we do
            not compare the system results with the human sentence reductions. Jing reports a succes rate of 81.3%
            for her program, but this measure is calculated as the
            percentage of decisions on which the system agrees
            with the decisions taken by the human summarizer.
            This means that 81.3% of all system decisions are
            correct, but does not say anything about how many
            sentences are correctly reduced.
            In our evaluation we do not expect the compressor to simulate human summarizer behaviour. The
            results presented here are calculated on the sentence
            level: the amount of valid reduced sentences, being those reductions which are judged by human
            raters to be accurate reductions: grammatical sentences with (more or less) the same meaning as the

            input sentence, taking into account the meaning of
            the previous sentences on the same topic.
            3.1

            Method

            To estimate the available number of characters in a
            subtitle, it is necessary to estimate the average pronunciation time of the input sentence, provided that
            it is unknown. We estimate sentence duration by
            counting the number of syllables in a sentence and
            multiplying this with the average duration per syllable (ASD).
            The ASD for Dutch is reported to be about 177
            ms (Koopmans-van Beinum and van Donzel, 1996),
            which is the syllable speed without including pauses
            between words or sentences.
            We did some similar research on CGN using the
            ASD as a unit of analysis, while we consider both
            the situation without pauses and the situation with
            pauses. Results of this research are presented in table 2.
            ASD
            All files
            One speaker
            Read-aloud

            no pauses
            186
            185
            188

            pauses included
            237
            239
            256

            Table 2: Average Syllable Duration (ms)

            We extract the word duration from all the files
            in each component of CGN. A description of the
            components can be found in (Oostdijk et al., 2002).
            We created a syllable counter for Dutch words,
            which we evaluated on all words in the CGN lexicon. For 98.3% of all words in the lexicon, syllables
            are counted correctly. Most errors occur in very low
            frequency words or in foreign words.
            By combining word duration information and the
            number of syllables we can calculate the average
            speaking speed.
            We evaluated sentence compression in three different conditions:
            The fastest ASD in our ASD-research was 185 ms
            (one speaker, no pauses), which was used for Condition A. We consider this ASD as the maximum
            speed for Dutch.
            The slowest ASD (256 ms) was used for Condition C. We consider this ASD to be the minimum
            speed for Dutch.
            We created a testset of 100 sentences mainly focused on news broadcasts in which we use the real
            pronunciation time of each sentence in the testset
            which results in an ASD of 192ms. This ASD was

            used for Condition B, and is considered as the real
            speed for news broadcasts.
            We created a testset of 300 sentences, of which
            200 were taken from transcripts of television news,
            and 100 were taken from the ’broadcast news’ component of CGN.
            To evaluate the compressor, we estimate the duration of each sentence, by counting the number of
            syllables and multiplying that number with the ASD
            for that condition. This leads to an estimated pronunciation time. This is converted to the number of
            characters, which is available for the subtitle.
            We know the average time for subtitle presentation at the VRT (Flemish Broadcasting Coorporation) is 70
            characters in 6 seconds, which gives us
            an average of 11.67 characters per second.
            So, for example, if we have a test sentence of
            15 syllables, this gives us an estimated pronunciation time of 2.775 seconds (15 syllables
            185
            ms/syllable) in condition A. When converting this to
            the available characters, we multiply 2.775 seconds
            by 11.67 characters/second, resulting in 32 (2.775s
            11.67 ch/s = 32.4 ch) available characters.
            In condition B (considered to be real-time) for
            the part of the test-sentences coming from CGN,
            the pronunciation time was not estimated, as it was
            available in CGN.
            3.2

            Results

            The results of our experiments on the sentence compression module are presented in table 3.
            Condition
            No output (0)
            Avg Syllable speed
            (msec/syllable)
            Avg Reduction Rate
            Interrater Agreement
            Accurate Compr.
            +/- Acc. Compr.
            Reasonable Compr.

            A
            44.33%

            B
            41.67%

            C
            15.67%

            185
            39.93%
            86.2%
            4.8%
            28.1%
            32.9%

            192
            37.65%
            86.9%
            8.0%
            26.3%
            34.3%

            256
            16.93%
            91.7%
            28.9%
            22.1%
            51%

            Table 3: Sentence Compression Evaluation on the
            Sentence Level
            The sentence compressor does not generate output for all test sentences in all conditions: In those
            cases where no output was generated, the sentence
            compressor was not able to generate a sentence
            alternative which was shorter than the maximum
            number of characters available for that sentence.
            The cases where no output is generated are not considered as errors because it is often impossible, even
            for humans, to reduce a sentence by about 40%,

            without changing the content too much. The amount
            of test sentences where no output was generated
            is presented in table 3. The high percentage of
            sentences where no output was generated in conditions A and B is most probably due to the fact that
            the compression rates in these conditions are higher
            than they would be in a real life application. Condition C seems to be closer to the real life compression
            rate needed in subtitling.
            Each condition has an average reduction rate over
            the 300 test sentences. This reduction rate is based
            on the available amount of characters in the subtitle
            and the number of characters in the source sentence.
            A rater scores a compressed sentence as + when
            it is grammatically correct and semantically equivalent to the input sentence. No essential information
            should be missing. A sentence is scored as +/when it is grammatically correct, but some information is
            missing, but is clear from the context in
            which the sentence occurs. All other compressed
            sentences get scored as -.
            Each sentence is evaluated by two raters. The
            lowest score of the two raters is the score which the
            sentence gets. Interrater agreement is calculated on
            a 2 point score: if both raters score a sentence as +
            or +/- or both raters score a sentence as -, it is considered an agreed judgement. Interrater agreement
            results are presented in table 3.
            Sentence compression results are presented in table 3. We consider both the + and +/- results as
            reasonable compressions.
            The resulting percentages of reasonable compressions seem to be rather low, but one should keep
            in mind that these results are based on the sentence
            level. One little mistake in one sentence can lead
            to an inaccurate compression, although the major
            part of the decisions taken in the compression process can still be correct. This makes it very hard
            to compare our results to the results presented by
            Jing (2001), but we presented our results on sentence evaluations as it gives a clearer idea on how
            well the system would actually perform in a real life
            application.
            As we do not try to immitate human subtitling behaviour, but try to develop an equivalent approach,
            our system is not evaluated in the same way as the
            system deviced by Jing.
        </corps>
        <conclusion>4 Conclusion
            We have described a hybrid approach to sentence
            compression which seems to work in general. The
            combination of using statistics and filtering out invalid results because they are ungrammatical by using a
            set of rules is a feasible way for automated

            sentence compression.
            The way of combining the probability-estimates
            of chunk removal to get a ranking in the generated
            sentence alternatives is working reasonably well,
            but could be improved by using more fine-grained
            chunk types for data collection.
            A full syntactic analysis of the input sentence
            would lead to better results, as the current sentence
            analysis tools have one very weak point: the handling of coordinating conjunction, which leads to
            chunking errors, both in the input sentence as in the
            processing of the used parallel corpus. This leads to
            misestimations of the compression probabilities and
            creates noise in the behaviour of our system.
            Making use of semantics would most probably
            lead to better results, but a semantic lexicon and
            semantic analysis tools are not available for Dutch,
            and creating them would be out of the scope of the
            current project.
            In future research we will check the effects of
            improved word-reduction modules, as word reductions often seem to lead to inaccurate compressions. Leaving
            out the word-reduction module
            would probably lead to an even bigger amount of
            no output-cases. This will also be checked in future
            research.

            5 Acknowledgements
            Research funded by IWT (Institute for Innovation
            in Science and Technology) in the STWW program, project ATraNoS (Automatic Transcription
            and Normalisation of Speech). For more information visit http://atranos.esat.kuleuven.ac.be/.
            We would like to thank Ineke Schuurman for rating the reduced sentences.
        </conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>G. Booij and A. van Santen. 1995. Morfologie. De
            woordstructuur van het Nederlands. Amsterdam
            University Press, Amsterdam, Netherlands.
            T. Brants. 2000. TnT - A Statistical Part-of-Speech
            Tagger. Published online at http://www.coli.unisb.de/thorsten/tnt.
            W. Daelemans and H. Strik. 2002. Het Nederlands in Taal- en Spraaktechnologie: Prioriteiten
            voor Basisvoorzieningen. Technical report, Nederlandse Taalunie.
            B. Dewulf and G. Saerens. 2000. Stijlboek
            Teletekst Ondertiteling. Technical report, VRT,
            Brussel. Internal Subtitling Guidelines.
            W. Haeseryn, G. Geerts, J de Rooij, and
            M. van den Toorn. 1997. Algemene Nederlandse Spraakkunst. Martinus Nijhoff Uitgevers,
            Groningen.

            ITC.
            1997.
            Guidance on standards
            for
            subtitling.
            Technical
            report,
            ITC.
            Online at http://www.itc.org.uk/
            codes guidelines/broadcasting/tv/sub sign
            audio/subtitling stnds/.
            H. Jing. 2001. Cut-and-Paste Text Summarization.
            Ph.D. thesis, Columbia University.
            F.J. Koopmans-van Beinum and M.E. van Donzel.
            1996. Relationship Between Discourse Structure
            and Dynamic Speech Rate. In Proceedings ICSLP 1996, Philadelphia, USA.
            N. Oostdijk, W. Goedertier, F. Van Eynde, L. Boves,
            J.P. Marters, M. Moortgat, and H. Baayen. 2002.
            Experiences from the Spoken Dutch Corpus. In
            Proceedings of LREC 2002, volume I, pages 340–
            347, Paris. ELRA.
            F. Van Eynde. 2004. Part-of-speech Tagging
            en Lemmatisering. Internal manual of Corpus Gesproken Nederlands, published online at
            http://www.ccl.kuleuven.ac.be/Papers/
            POSmanual febr2004.pdf.
            V. Vandeghinste and E. Tjong Kim Sang. 2004. Using a parallel transcript/subtitle corpus for sentence
            compression. In Proceedings of LREC
            2004, Paris. ELRA.
            V. Vandeghinste. 2002. Lexicon optimization:
            Maximizing lexical coverage in speech recognition through automated compounding. In Proceedings of LREC
            2002, volume IV, pages 1270–
            1276, Paris. ELRA.
            V. Vandeghinste. submitted. ShaRPa: Shallow
            Rule-based Parsing, focused on Dutch. In Proceedings of CLIN 2003.
        </biblio>
    </article>
    <article>
        <preamble>marcu_statistics_sentence_pass_one.txt</preamble>
        <titre>Statistics-Based Summarization — Step One: Sentence Compression</titre>
        <auteur>Kevin Knight and Daniel Marcu</auteur>
        <abstract>When humans produce summaries of documents, they do not simply extract sentences and concatenate them.
            Rather, they create new sentences that are grammatical, that cohere with one another, and that capture the
            most salient pieces of information in the original document. Given that large collections of text/abstract
            pairs are available online, it is now possible to envision algorithms that are trained to mimic this
            process. In this paper, we focus on sentence compression, a simpler version of this larger challenge. We aim
            to achieve two goals simultaneously: our compressions should be grammatical, and they should retain the most
            important pieces of information. These two goals can conﬂict. We devise both noisy-channel and decision-tree
            approaches to the problem, and we evaluate results against manual compressions and a simple baseline.
        </abstract>
        <introduction>Most of the research in automatic summarization has focused on extraction, i.e., on identifying
            the most important clauses/sentences/paragraphs in texts (see (Mani & Maybury 1999) for a representative
            collection of papers). However, determining the most important textual segments is only half of what a
            summarization system needs to do because, in most cases, the simple catenation of textual segments does not
            yield coherent outputs. Recently, a number of researchers have started to address the problem of generating
            coherent summaries: McKeown et al. (1999), Barzilay et al. (1999), and Jing and McKeown (1999) in the
            context of multidocument summarization; Mani et al. (1999) in the context of revising single document
            extracts; and Witbrock and Mittal (1999) in the context of headline generation. The approach proposed by
            Witbrock and Mittal (1999) is the only one that applies a probabilistic model trained directly on Headline,
            Document pairs. However, this model has yet to scale up to generating multiple-sentence abstracts as well
            as well-formed, grammatical sentences. All other approaches employ sets of manually written or
            semi-automatically derived c 2000, American Association for Artiﬁcial InCopyright  telligence
            (www.aaai.org). All rights reserved.
        </introduction>
        <corps>telligence (www.aaai.org). All rights reserved.

            rules for deleting information that is redundant, compressing long sentences into shorter ones, aggregating
            sentences, repairing reference links, etc.
            Our goal is also to generate coherent abstracts. However, in contrast with the above work, we intend to
            eventually use Abstract, Text tuples, which are widely
            available, in order to automatically learn how to rewrite
            Texts as coherent Abstracts. In the spirit of the work
            in the statistical MT community, which is focused on
            sentence-to-sentence translations, we also decided to focus ﬁrst on a simpler problem, that of sentence
            compression. We chose this problem for two reasons:
            • First, the problem is complex enough to require the
            development of sophisticated compression models:
            Determining what is important in a sentence and
            determining how to convey the important information grammatically, using only a few words, is just a
            scaled down version of the text summarization problem. Yet, the problem is simple enough, since we do
            not have to worry yet about discourse related issues,
            such as coherence, anaphors, etc.
            • Second, an adequate solution to this problem has
            an immediate impact on several applications. For
            example, due to time and space constraints, the
            generation of TV captions often requires only the
            most important parts of sentences to be shown on a
            screen (Linke-Ellis 1999; Robert-Ribes et al. 1999).
            A good sentence compression module would therefore have an impact on the task of automatic caption
            generation. A sentence compression module
            can also be used to provide audio scanning services for the blind (Grefenstette 1998). In general, since all
            systems aimed at producing coherent abstracts implement manually written sets of
            sentence compression rules (McKeown et al. 1999;
            Mani, Gates, & Bloedorn 1999; Barzilay, McKeown,
            & Elhadad 1999), it is likely that a good sentence
            compression module would impact the overall quality
            of these systems as well. This becomes particularly
            important for text genres that use long sentences.
            In this paper, we present two approaches to the sentence compression problem. Both take as input a sequence
            of words W = w1 , w2 , . . . , wn (one sentence).

            An algorithm may drop any subset of these words. The
            words that remain (order unchanged) form a compression. There are 2n compressions to choose from—some
            are reasonable, most are not. Our ﬁrst approach develops a probabilistic noisy-channel model for sentence
            compression. The second approach develops a decisionbased, deterministic model.

            A noisy-channel model for sentence
            compression
            This section describes a probabilistic approach to the
            compression problem. In particular, we adopt the noisy
            channel framework that has been relatively successful in
            a number of other NLP applications, including speech
            recognition (Jelinek 1997), machine translation (Brown
            et al. 1993), part-of-speech tagging (Church 1988),
            transliteration (Knight & Graehl 1998), and information retrieval (Berger & Laﬀerty 1999).
            In this framework, we look at a long string and imagine that (1) it was originally a short string, and then
            (2) someone added some additional, optional text to it.
            Compression is a matter of identifying the original short
            string. It is not critical whether or not the “original”
            string is real or hypothetical. For example, in statistical
            machine translation, we look at a French string and say,
            “This was originally English, but someone added ‘noise’
            to it.” The French may or may not have been translated
            from English originally, but by removing the noise, we
            can hypothesize an English source—and thereby translate the string. In the case of compression, the noise
            consists of optional text material that pads out the core
            signal. For the larger case of text summarization, it may
            be useful to imagine a scenario in which a news editor
            composes a short document, hands it to a reporter, and
            tells the reporter to “ﬂesh it out” . . . which results in
            the article we read in the newspaper. As summarizers,
            we may not have access to the editor’s original version
            (which may or may not exist), but we can guess at it—
            which is where probabilities come in.
            As in any noisy channel application, we must solve
            three problems:
            • Source model. We must assign to every string s a
            probability P(s), which gives the chance that s is generated as an “original short string” in the above
            hypothetical process. For example, we may want P(s)
            to be very low if s is ungrammatical.
            • Channel model. We assign to every pair of strings
            s, t a probability P(t | s), which gives the chance
            that when the short string s is expanded, the result
            is the long string t. For example, if t is the same
            as s except for the extra word “not,” then we may
            want P(t | s) to be very low. The word “not” is not
            optional, additional material.
            • Decoder. When we observe a long string t, we search
            for the short string s that maximizes P(s | t). This
            is equivalent to searching for the s that maximizes
            P(s) · P (t | s).

            It is advantageous to break the problem down this
            way, as it decouples the somewhat independent goals
            of creating a short text that (1) looks grammatical,
            and (2) preserves important information. It is easier to
            build a channel model that focuses exclusively on the
            latter, without having to worry about the former. That
            is, we can specify that a certain substring may represent
            unimportant information, but we do not need to worry
            that deleting it will result in an ungrammatical structure. We leave that to the source model, which worries
            exclusively about well-formedness. In fact, we can make
            use of extensive prior work in source language modeling
            for speech recognition, machine translation, and natural language generation. The same goes for actual
            compression (“decoding” in noisy-channel jargon)—we can
            re-use generic software packages to solve problems in all
            these application domains.

            Statistical Models
            In the experiments we report here, we build very simple source and channel models. In a departure from
            the above discussion and from previous work on statistical channel models, we assign probabilities Ptree (s)
            and Pexpand tree (t | s) to trees rather than strings. In
            decoding a new string, we ﬁrst parse it into a large tree t
            (using Collins’ parser (1997)), and we then hypothesize
            and rank various small trees.
            Good source strings are ones that have both (1) a
            normal-looking parse tree, and (2) normal-looking word
            pairs. Ptree (s) is a combination of a standard probabilistic context-free grammar (PCFG) score, which is
            computed over the grammar rules that yielded the tree
            s, and a standard word-bigram score, which is computed over the leaves of the tree. For example, the
            tree s =(S (NP John) (VP (VB saw) (NP Mary))) is
            assigned a score based on these factors:
            Ptree (s) = P(TOP → S | TOP) ·
            P(S → NP VP | S) · P(NP → John | NP) ·
            P(VP → VB NP | VP) · P(VP → saw | VB) ·
            P(NP → Mary | NP) ·
            P(John | EOS) · P(saw | John) ·
            P(Mary | saw) · P(EOS | Mary)
            Our stochastic channel model performs minimal operations on a small tree s to create a larger tree t. For
            each internal node in s, we probabilistically choose an
            expansion template based on the labels of the node and
            its children. For example, when processing the S node
            in the tree above, we may wish to add a prepositional
            phrase as a third child. We do this with probability
            P(S → NP VP PP | S → NP VP). Or we may choose
            to leave it alone, with probability P(S → NP VP | S →
            NP VP). After we choose an expansion template, then
            for each new child node introduced (if any), we grow a
            new subtree rooted at that node—for example (PP (P
            in) (NP Pittsburgh)). Any particular subtree is grown
            with probability given by its PCFG factorization, as
            above (no bigrams).

            G

            G
            A

            H
            a

            C

            G
            A

            H
            D

            B

            b Q

            R

            Z

            d

            a

            e

            D

            F

            C

            D

            b

            e

            H

            K

            a

            b

            e

            c
            (t)

            Figure 1: Examples of parse trees.

            Although the modules themselves may be physically and/or
            electrically incompatible, the cable-speciﬁc jacks on them
            provide industry-standard connections.
            Cable-speciﬁc jacks provide industry-standard connections.

            Example
            In this section, we show how to tell whether one potential compression is more likely than another,
            according
            to the statistical models described above. Suppose we
            observe the tree t in Figure 1, which spans the string
            abcde. Consider the compression s1, which is shown in
            the same ﬁgure.
            We
            compute
            the
            factors
            Ptree (s1)
            and
            Pexpand tree (t | s1). Breaking this down further,
            the source PCFG and word-bigram factors, which
            describe Ptree (s1), are:
            P(TOP → G | TOP)
            P(G → H A | G)
            P(A → C D | A)

            P(H → a | H)
            P(C → b | C)
            P(D → e | D)

            P(a | EOS)
            P(b | a)

            P(e | b)
            P(EOS | e)

            The channel expansion-template factors and the channel PCFG (new tree growth) factors, which describe
            Pexpand tree (t | s1), are:
            P(G → H A | G → H A)
            P(A → C B D | A → C D)
            P(B → Q R | B)
            P(Q → Z | Q)

            All of our design goals were achieved and the delivered
            performance matches the speed of the underlying device.
            All design goals were achieved.
            Reach’s E-mail product, MailMan, is a message- management system designed initially for VINES LANs that will
            eventually be operating system-independent.
            MailMan will eventually be operating system-independent.

            (s2)

            (s1)

            The documentation is typical of Epson quality: excellent.
            Documentation is excellent.

            P(Z → c | Z)
            P(R → d | R)

            A diﬀerent compression will be scored with a diﬀerent
            set of factors. For example, consider a compression of
            t that leaves t completely untouched. In that case, the
            source costs Ptree (t) are:
            P(TOP → G | TOP)
            P(G → H A | G)

            P(H → a | H)
            P(C → b | C)

            P(a | EOS)
            P(b | a)

            P(A → C D | A)

            P(Z → c | Z)

            P(c | b)

            P(B → Q R | B)
            P(Q → Z | Q)

            P(R → d | R)
            P(D → e | D)

            P(d | c)
            P(e | d)
            P(EOS | e)

            The channel costs Pexpand tree (t | t) are:

            Ingres/Star prices start at $2,100.
            Ingres/Star prices start at $2,100.

            Figure 2: Examples from our parallel corpus.
            P(G → H A | G → H A)
            P(A → C B D | A → C B D)
            P(B → Q R | B → Q R)
            P(Q → Z | Q → Z)
            Now we can simply compare Pexpand tree (s1 |
            t) = Ptree (s1) · Pexpand tree (t | s1))/Ptree (t) versus Pexpand tree (t | t) = Ptree (t) · Pexpand tree (t
            |
            t))/Ptree (t) and select the more likely one. Note that
            Ptree (t) and all the PCFG factors can be canceled out,
            as they appear in any potential compression. Therefore,
            we need only compare compressions of the basis of the
            expansion-template probabilities and the word-bigram
            probabilities. The quantities that diﬀer between the
            two proposed compressions are boxed above. Therefore, s1 will be preferred over t if and only if:
            P(e | b) · P(A → C B D | A → C D) >
            P(b | a) · P(c | b) · P(d | c) ·
            P(A → C B D | A → C B D) ·
            P(B → Q R | B → Q R) · P(Q → Z | Q → Z)

            Training Corpus
            In order to train our system, we used the Ziﬀ-Davis
            corpus, a collection of newspaper articles announcing
            computer products. Many of the articles in the corpus
            are paired with human written abstracts. We automatically extracted from the corpus a set of 1067 sentence
            pairs. Each pair consisted of a sentence t = t1 , t2 , . . . , tn
            that occurred in the article and a possibly compressed
            version of it s = s1 , s2 , . . . , sm , which occurred in the
            human written abstract. Figure 2 shows a few sentence
            pairs extracted from the corpus.
            We decided to use such a corpus because it is consistent with two desiderata speciﬁc to summarization
            work: (i) the human-written Abstract sentences are

            grammatical; (ii) the Abstract sentences represent in a
            compressed form the salient points of the original newspaper Sentences. We decided to keep in the corpus
            uncompressed sentences as well, since we want to learn
            not only how to compress a sentence, but also when to
            do it.

            Learning Model Parameters
            We collect expansion-template probabilities from our
            parallel corpus. We ﬁrst parse both sides of the parallel
            corpus, and then we identify corresponding syntactic
            nodes. For example, the parse tree for one sentence
            may begin (S (NP . . . ) (VP . . . ) (PP . . . )) while
            the parse tree for its compressed version may begin (S
            (NP . . . ) (VP . . . )). If these two S nodes are deemed
            to correspond, then we chalk up one joint event (S →
            NP VP, S → NP VP PP); afterwards we normalize.
            Not all nodes have corresponding partners; some noncorrespondences are due to incorrect parses, while others
            are due to legitimate reformulations that are beyond
            the scope of our simple channel model. We use standard
            methods to estimate word-bigram probabilities.

            Beyond that basic level, the operations of the three products vary
            widely (1514588)
            Beyond that level, the operations of the three products vary widely
            (1430374)
            Beyond that basic level, the operations of the three products vary
            (1333437)
            Beyond that level, the operations of the three products vary
            (1249223)
            Beyond that basic level, the operations of the products vary
            (1181377)

            Decoding

            The operations of the three products vary widely (939912)

            There is a vast number of potential compressions of a
            large tree t, but we can pack them all eﬃciently into a
            shared-forest structure. For each node of t that has n
            children, we
            • generate 2n − 1 new nodes, one for each non-empty
            subset of the children, and
            • pack those nodes so that they are referred to as a
            whole.
            For example, consider the large tree t above. All compressions can be represented with the following forest:
            G→HA
            G→H
            G→A
            B→QR
            B→Q

            semantic representation into a vast number of potential
            English renderings. These renderings are packed into
            a forest, from which the most promising sentences are
            extracted using statistical scoring.
            For our purposes, the extractor selects the trees with
            the best combination of word-bigram and expansiontemplate scores. It returns a list of such trees, one for
            each possible compression length. For example, for
            the sentence Beyond that basic level, the operations of
            the three products vary, we obtain the following “best”
            compressions, with negative log-probabilities shown in
            parentheses (smaller = more likely):

            B→R
            Q→Z
            A→CBD
            A→CB
            A→CD

            A→BC
            A→C
            A→B
            A→D

            H→a
            C→b
            Z→c
            R→d
            D→e

            We can also assign an expansion-template probability
            to each node in the forest. For example, to the B →
            Q node, we can assign P(B → Q R | B → Q). If the
            observed probability from the parallel corpus is zero,
            then we assign a small ﬂoor value of 10−6 . In reality,
            we produce forests that are much slimmer, as we only
            consider compressing a node in ways that are locally
            grammatical according to the Penn Treebank—if a rule
            of the type A → C B has never been observed, then it
            will not appear in the forest.
            At this point, we want to extract a set of highscoring trees from the forest, taking into account
            both expansion-template probabilities and word-bigram
            probabilities. Fortunately, we have such a generic extractor on hand (Langkilde 2000). This extractor was
            designed for a hybrid symbolic-statistical natural language generation system called Nitrogen. In that
            application, a rule-based component converts an abstract

            The operations of the products vary widely (872066)
            The operations of the products vary (748761)
            The operations of products vary (690915)
            Operations of products vary (809158)
            The operations vary (522402)
            Operations vary (662642)

            Length Selection
            It is useful to have multiple answers to choose from, as
            one user may seek a 20% compression, while another
            seeks a 60% compression. However, for purposes of
            evaluation, we want our system to be able to select a
            single compression. If we rely on the log-probabilities
            as shown above, we will almost always choose the shortest compression. (Note above, however, how the
            threeword compression scores better than the two-word compression, as the models are not entirely happy
            removing
            the article “the”). To create a more fair competition,
            we divide the log-probability by the length of the compression, rewarding longer strings. This is commonly
            done in speech recognition.
            If we plot this normalized score against compression
            length, we usually observe a (bumpy) U-shaped curve,
            as illustrated in Figure 3. In a typical more diﬃcult
            case, a 25-word sentence may be optimally compressed
            by a 17-word version. Of course, if a user requires a
            shorter compression than that, she may select another
            region of the curve and look for a local minimum.

            A decision-based model for sentence
            compression
            In this section, we describe a decision-based, history
            model of sentence compression. As in the noisy-channel
            approach, we again assume that we are given as input

            0.10

            4

            5

            6

            7

            8

            Finally, another advantage of broadband is distance .

            Finally another advantage of broadband is distance .

            Another advantage of broadband is distance .

            Advantage of broadband is distance .

            0.15

            Another advantage is distance .

            Advantage is distance .

            Adjusted negative log-probability of best
            compression s at a particular length n
            -log P(s) P( t | s) / n

            0.20

            Stack

            9

            Compression length n

            Figure 3: Adjusted log-probabilities for top-scoring
            compressions at various lengths (lower is better).
            a parse tree t. Our goal is to “rewrite” t into a smaller
            tree s, which corresponds to a compressed version of the
            original sentence subsumed by t. Suppose we observe in
            our corpus the trees t and s2 in Figure 1. In this model,
            we ask ourselves how we may go about rewriting t into
            s2. One possible solution is to decompose the rewriting
            operation into a sequence of shift-reduce-drop actions
            that are speciﬁc to an extended shift-reduce parsing
            paradigm.
            In the model we propose, the rewriting process starts
            with an empty Stack and an Input List that contains the
            sequence of words subsumed by the large tree t. Each
            word in the input list is labeled with the name of all syntactic constituents in t that start with it (see
            Figure 4).
            At each step, the rewriting module applies an operation that is aimed at reconstructing the smaller tree s2.
            In the context of our sentence-compression module, we
            need four types of operations:
            • shift operations transfer the ﬁrst word from the input list into the stack;
            • reduce operations pop the k syntactic trees located
            at the top of the stack; combine them into a new
            tree; and push the new tree on the top of the stack.
            Reduce operations are used to derive the structure of
            the syntactic tree of the short sentence.
            • drop operations are used to delete from the input list
            subsequences of words that correspond to syntactic
            constituents. A drop x operations deletes from the

            Input List

            Input List

            Stack
            F

            G
            H
            a

            H

            A

            a

            C
            b

            H

            K

            a

            b

            B
            Q
            Z
            c

            A
            B
            Q
            Z
            c

            C
            b

            B
            Q
            Z
            c

            R
            d

            R
            d

            R
            d

            D
            e

            SHIFT;

            K

            a

            b

            ASSIGNTYPE H

            F

            STEPS 1-2
            D
            e

            SHIFT;

            H

            K

            a

            b

            F

            R
            d

            D
            e

            D
            e

            DROP B
            STEP 6
            SHIFT;
            ASSIGNTYPE D
            STEPS 7-8

            D

            ASSIGNTYPE K
            STEPS 3-4

            D
            e

            H

            B
            Q
            Z
            c

            H

            K

            a

            b

            e

            REDUCE 2 G
            STEP 9

            G
            REDUCE 2 F
            STEP 5

            F

            D

            H

            K

            a

            b

            e

            Figure 4: Example of incremental tree compression.
            input list all words that are spanned by constituent
            x in t.
            • assignType operations are used to change the label
            of trees at the top of the stack. These actions assign
            POS tags to the words in the compressed sentence,
            which may be diﬀerent from the POS tags in the
            original sentence.
            The decision-based model is more ﬂexible than the
            channel model because it enables the derivation of trees
            whose skeleton can diﬀer quite drastically from that of
            the tree given as input. For example, using the channel
            model, we are unable to obtain tree s2 from t. However,
            the four operations listed above enable us to rewrite a
            tree t into any tree s, as long as an in-order traversal of
            the leaves of s produces a sequence of words that occur
            in the same order as the words in the tree t. For example, the tree s2 can be obtained from tree t by
            following
            this sequence of actions, whose eﬀects are shown in Figure 4: shift; assignType H; shift; assignType K;
            reduce 2 F; drop B; shift; assignType D; reduce
            2 G.
            To save space, we show shift and assignType operations on the same line; however, the reader should
            understand that they correspond to two distinct actions. As one can see, the assignType K operation
            rewrites the POS tag of the word b; the reduce operations modify the skeleton of the tree given as input.
            To increase readability, the input list is shown in a format that resembles as closely as possible the
            graphical
            representation of the trees in ﬁgure 1.

            Learning the parameters of the
            decision-based model
            We associate with each conﬁguration of our shiftreduce-drop, rewriting model a learning case. The cases
            are generated automatically by a program that derives
            sequences of actions that map each of the large trees in
            our corpus into smaller trees. The rewriting procedure
            simulates a bottom-up reconstruction of the smaller
            trees.
            Overall, the 1067 pairs of long and short sentences
            yielded 46383 learning cases. Each case was labeled

            with one action name from a set of 210 possible actions: There are 37 distinct assignType actions, one
            for each POS tag. There are 63 distinct drop actions,
            one for each type of syntactic constituent that can be
            deleted during compression. There are 109 distinct reduce actions, one for each type of reduce operation
            that
            is applied during the reconstruction of the compressed
            sentence. And there is one shift operation. Given a
            tree t and an arbitrary conﬁguration of the stack and
            input list, the purpose of the decision-based classiﬁer
            is to learn what action to choose from the set of 210
            possible actions.
            To each learning example, we associated a set of 99
            features from the following two classes:
            Operational features reﬂect the number of trees
            in the stack, the input list, and the types of
            the last ﬁve operations. They also encode information that denote the syntactic category of the
            root nodes of the partial trees built up to a certain time. Examples of such features are:
            numberTreesInStack, wasPreviousOperationShift, syntacticLabelOfTreeAtTheTopOfStack, etc.
            Original-tree-speciﬁc features denote the syntactic constituents that start with the ﬁrst unit in the
            input list. Examples of such features are: inputListStartsWithA CC, inputListStartsWithA PP, etc.
            The decision-based compression module uses the
            C4.5 program (Quinlan 1993) in order to learn decision trees that specify how large syntactic trees can
            be compressed into shorter trees. A ten-fold crossvalidation evaluation of the classiﬁer yielded an accuracy
            of 87.16% (± 0.14). A majority baseline classiﬁer that chooses the action shift has an accuracy of
            28.72%.

            Employing the decision-based model
            To compress sentences, we apply the shift-reduce-drop
            model in a deterministic fashion. We parse the sentence
            to be compressed (Collins 1997) and we initialize the
            input list with the words in the sentence and the syntactic constituents that “begin” at each word, as shown
            in Figure 4. We then incrementally inquire the learned
            classiﬁer what action to perform, and we simulate the
            execution of that action. The procedure ends when the
            input list is empty and when the stack contains only
            one tree. An inorder traversal of the leaves of this tree
            produces the compressed version of the sentence given
            as input.
            Since the model is deterministic, it produces only one
            output. The advantage is that the compression is very
            fast: it takes only a few milliseconds per sentence. The
            disadvantage is that it does not produce a range of
            compressions, from which another system may subsequently choose. It is straightforward though to extend
            the model within a probabilistic framework by applying,
            for example, the techniques used by Magerman (1995).

            Evaluation
            To evaluate our compression algorithms, we randomly
            selected 32 sentence pairs from our parallel corpus,
            which we will refer to as the Test Corpus. We used the
            other 1035 sentence pairs for training. Figure 5 shows
            three sentences from the Test Corpus, together with the
            compressions produced by humans, our compression algorithms, and a baseline algorithm that produces
            compressions with highest word-bigram scores. The examples are chosen so as to reﬂect good, average, and bad
            performance cases. The ﬁrst sentence is compressed in
            the same manner by humans and our algorithms (the
            baseline algorithm chooses though not to compress this
            sentence). For the second example, the output of the
            Decision-based algorithm is grammatical, but the semantics is negatively aﬀected. The noisy-channel
            algorithm deletes only the word “break”, which aﬀects
            the correctness of the output less. In the last example,
            the noisy-channel model is again more conservative and
            decides not to drop any constituents. In constrast, the
            decision-based algorithm compresses the input substantially, but it fails to produce a grammatical output.
            We presented each original sentence in the Test Corpus to four judges, together with four compressions of
            it:
            the human generated compression, the outputs of the
            noisy-channel and decision-based algorithms, and the
            output of the baseline algorithm. The judges were told
            that all outputs were generated automatically. The order of the outputs was scrambled randomly across test
            cases.
            To avoid confounding, the judges participated in two
        </corps>
        <conclusion>Aucune conclusion trouvée.</conclusion>
        <discussion>Aucune discussion trouvée.</discussion>
        <biblio>Barzilay, R.; McKeown, K.; and Elhadad, M. 1999.
            Information fusion in the context of multi-document
            summarization. In Proceedings of the 37th Annual
            Meeting of the Association for Computational Linguistics (ACL–99), 550–557.
            Berger, A., and Laﬀerty, J. 1999. Information retrieval
            as statistical translation. In Proceedings of the 22nd
            Conference on Research and Development in Information Retrieval (SIGIR–99), 222–229.
            Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mercer, R. 1993. The mathematics of statistical machine
            translation: Parameter estimation. Computational Linguistics 19(2):263–311.
            Church, K. 1988. A stochastic parts program and noun
            phrase parser for unrestricted text. In Proceedings of
            the Second Conference on Applied Natural Language
            Processing, 136–143.
            Collins, M. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the 35th
            Annual Meeting of the Association for Computational
            Linguistics (ACL–97), 16–23.
            Grefenstette, G. 1998. Producing intelligent telegraphic text reduction to provide an audio scanning
            service for the blind. In Working Notes of the AAAI

            Spring Symposium on Intelligent Text Summarization,
            111–118.
            Jelinek, F. 1997. Statistical Methods for Speech Recognition. The MIT Press.
            Jing, H., and McKeown, K. 1999. The decomposition
            of human-written summary sentences. In Proceedings
            of the 22nd Conference on Research and Development
            in Information Retrieval (SIGIR–99).
            Knight, K., and Graehl, J. 1998. Machine transliteration. Computational Linguistics 24(4):599–612.
            Langkilde, I. 2000. Forest-based statistical sentence
            generation. In Proceedings of the 1st Annual Meeting
            of the North American Chapter of the Association for
            Computational Linguistics.
            Linke-Ellis, N. 1999. Closed captioning in America: Looking beyond compliance. In Proceedings of
            the TAO Workshop on TV Closed Captions for the
            hearing impaired people, 43–59.
            Magerman, D. 1995. Statistical decision-tree models
            for parsing. In Proceedings of the 33rd Annual Meeting
            of the Association for Computational Linguistics, 276–
            283.
            Mani, I., and Maybury, M., eds. 1999. Advances in
            Automatic Text Summarization. The MIT Press.
            Mani, I.; Gates, B.; and Bloedorn, E. 1999. Improving
            summaries by revising them. In Proceedings of the 37th
            Annual Meeting of the Association for Computational
            Linguistics, 558–565.
            McKeown, K.; Klavans, J.; Hatzivassiloglou, V.;
            Barzilay, R.; and Eskin, E. 1999. Towards multidocument summarization by reformulation: Progress and
            prospects. In Proceedings of the Sixteenth National
            Conference on Artiﬁcial Intelligence (AAAI–99).
            Quinlan, J. 1993. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann Publishers.
            Robert-Ribes, J.; Pfeiﬀer, S.; Ellison, R.; and Burnham, D. 1999. Semi-automatic captioning of TV programs,
            an Australian perspective. In Proceedings of
            the TAO Workshop on TV Closed Captions for the
            hearing impaired people, 87–100.
            Witbrock, M., and Mittal, V.
            1999.
            Ultrasummarization: A statistical approach to generating
            highly condensed non-extractive summaries. In Proceedings of the 22nd International Conference on Research
            and Development in Information Retrieval (SIGIR’99), Poster Session, 315–316.
        </biblio>
    </article>
    <article>
        <preamble>Torres.txt</preamble>
        <titre>Summary Evaluation</titre>
        <auteur>with and without References Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and
            Patricia Velázquez-Morales
        </auteur>
        <abstract>we study a new contentbased method for the evaluation of text summarization systems without human
            models which is used to produce system rankings. The research is carried out using a new content-based
            evaluation framework called F RESA to compute a variety of divergences among probability distributions. We
            apply our comparison framework to various well-established content-based evaluation measures in text
            summarization such as C OVERAGE, R ESPONSIVENESS, P YRAMIDS and ROUGE studying their associations in various
            text summarization tasks including generic multi-document summarization in English and French, focus-based
            multi-document summarization in English and generic single-document summarization in French and Spanish.
            Index Terms—Text summarization evaluation, content-based evaluation measures, divergences.
        </abstract>
        <introduction>T EXT summarization evaluation has always been a complex and controversial issue in computational
            linguistics. In the last decade, significant advances have been made in this field as well as various
            evaluation measures have been designed. Two evaluation campaigns have been led by the U.S. agence DARPA. The
            first one, SUMMAC, ran from
        </introduction>
        <corps>the U.S. agence DARPA. The first one, SUMMAC, ran from
            1996 to 1998 under the auspices of the Tipster program [1],
            and the second one, entitled DUC (Document Understanding
            Conference) [2], was the main evaluation forum from 2000
            until 2007. Nowadays, the Text Analysis Conference (TAC)
            [3] provides a forum for assessment of different information
            access technologies including text summarization.
            Evaluation in text summarization can be extrinsic or
            intrinsic [4]. In an extrinsic evaluation, the summaries are
            assessed in the context of an specific task carried out by a
            human or a machine. In an intrinsic evaluation, the summaries
            are evaluated in reference to some ideal model. SUMMAC
            was mainly extrinsic while DUC and TAC followed an
            intrinsic evaluation paradigm. In an intrinsic evaluation, an
            Manuscript received June 8, 2010. Manuscript accepted for publication July
            25, 2010.
            Juan-Manuel Torres-Moreno is with LIA/Université d’Avignon,
            France
            and
            École
            Polytechnique
            de
            Montréal,
            Canada
            (juan-manuel.torres@univ-avignon.fr).
            Eric
            SanJuan
            is
            with
            LIA/Université
            d’Avignon,
            France
            (eric.sanjuan@univ-avignon.fr).
            Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain
            (horacio.saggion@upf.edu).
            Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain;
            LIA/Université d’Avignon, France and Instituto de Ingenierı́a/UNAM, Mexico
            (iria.dacunha@upf.edu).
            Patricia
            Velázquez-Morales
            is
            with
            VM
            Labs,
            France
            (patricia velazquez@yahoo.com).

            13

            Polibits (42) 2010

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            of models and the identification, matching, and weighting of
            SCUs in both: models and peers.
            [12] evaluated the effectiveness of the Jensen-Shannon
            (J S) [13] theoretic measure in predicting systems ranks
            in two summarization tasks: query-focused and update
            summarization. They have shown that ranks produced
            by P YRAMIDS and those produced by J S measure
            correlate. However, they did not investigate the effect
            of the measure in summarization tasks such as generic
            multi-document summarization (DUC 2004 Task 2),
            biographical summarization (DUC 2004 Task 5), opinion
            summarization (TAC 2008 OS), and summarization in
            languages other than English.
            In this paper we present a series of experiments aimed at
            a better understanding of the value of the J S divergence
            for ranking summarization systems. We have carried out
            experimentation with the proposed measure and we have
            verified that in certain tasks (such as those studied by
            [12]) there is a strong correlation among P YRAMIDS,
            R ESPONSIVENESS and the J S divergence, but as we will
            show in this paper, there are datasets in which the correlation
            is not so strong. We also present experiments in Spanish
            and French showing positive correlation between the J S
            and ROUGE which is the de facto evaluation measure used
            in evaluation of non-English summarization. To the best of
            our knowledge this is the more extensive set of experiments
            interpreting the value of evaluation without human models.
            The rest of the paper is organized in the following way:
            First in Section II we introduce related work in the area of
            content-based evaluation identifying the departing point for
            our inquiry; then in Section III we explain the methodology
            adopted in our work and the tools and resources used for
            experimentation. In Section IV we present the experiments
            carried out together with the results. Section V discusses the
            results and Section VI concludes the paper and identifies future
            work.

            non-random systems, no clear conclusion was reached on the
            value of each of the studied measures.
            Nowadays, a widespread summarization evaluation
            framework is ROUGE [14], which offers a set of statistics
            that compare peer summaries with models. It counts
            co-occurrences of n-grams in peer and models to derive a
            score. There are several statistics depending on the used
            n-grams and the text processing applied to the input texts
            (e.g., lemmatization, stop-word removal).
            [15] proposed a method of evaluation based on the
            use of “distances” or divergences between two probability
            distributions (the distribution of units in the automatic
            summary and the distribution of units in the model
            summary). They studied two different Information Theoretic
            measures of divergence: the Kullback-Leibler (KL) [16] and
            Jensen-Shannon (J S) [13] divergences. KL computes the
            divergence between probability distributions P and Q in the
            following way:
            Pw
            1X
            Pw log2
            (1)
            DKL (P ||Q) =
            2 w
            Qw
            While J S divergence is defined as follows:
            1X
            2Pw
            2Qw
            DJ S (P ||Q) =
            Pw log2
            + Qw log2
            2 w
            Pw + Qw
            Pw + Qw
            (2)
            These measures can be applied to the distribution of units in
            system summaries P and reference summaries Q. The value
            obtained may be used as a score for the system summary. The
            method has been tested by [15] over the DUC 2002 corpus for
            single and multi-document summarization tasks showing good
            correlation among divergence measures and both coverage and
            ROUGE rankings.
            [12] went even further and, as in [5], they proposed to
            compare directly the distribution of words in full documents
            with the distribution of words in automatic summaries to
            derive a content-based evaluation measure. They found a
            high correlation between rankings produced using models
            and rankings produced without models. This last work is the
            departing point for our inquiry into the value of measures that
            do not rely on human models.

            II. R ELATED W ORK
            One of the first works to use content-based measures in
            text summarization evaluation is due to [5], who presented an
            evaluation framework to compare rankings of summarization
            systems produced by recall and cosine-based measures. They
            showed that there was weak correlation among rankings
            produced by recall, but that content-based measures produce
            rankings which were strongly correlated. This put forward
            the idea of using directly the full document for comparison
            purposes in text summarization evaluation. [6] presented a
            set of evaluation measures based on the notion of vocabulary
            overlap including n-gram overlap, cosine similarity, and
            longest common subsequence, and they applied them to
            multi-document summarization in English and Chinese.
            However, they did not evaluate the performance of the
            measures in different summarization tasks. [7] also compared
            various evaluation measures based on vocabulary overlap.
            Although these measures were able to separate random from

            Polibits (42) 2010

            III. M ETHODOLOGY
            The followed methodology in this paper mirrors the one
            adopted in past work (e.g. [5], [7], [12]). Given a particular
            summarization task T , p data points to be summarized
            p−1
            with input material {Ii }i=0
            (e.g. document(s), question(s),
            s−1
            topic(s)), s peer summaries {SUMi,k }k=0
            for input i, and
            m−1
            m model summaries {MODELi,j }j=0 for input i, we will
            compare rankings of the s peer summaries produced by various
            evaluation measures. Some measures that we use compare
            summaries with n of the m models:
            MEASUREM (SUMi,k , {MODELi,j }n−1
            j=0 )

            14

            (3)

            Summary Evaluation with and without References

            while other measures compare peers with all or some of the
            input material:
            MEASUREM (SUMi,k , Ii0 )

            3) Update-summarization task that consists of creating a
            summary out of a cluster of documents and a topic. Two
            sub-tasks are considered here: A) an initial summary has
            to be produced based on an initial set of documents and
            topic; B) an update summary has to be produced from
            a different (but related) cluster assuming documents
            used in A) are known. The English TAC’08 Update
            Summarization dataset is used, which consists of 48
            topics with 20 documents each – 36,911 words.
            4) Opinion summarization where systems have to analyze
            a set of blog articles and summarize the opinions
            about a target in the articles. The TAC’08 Opinion
            Summarization in English4 data set (taken from the
            Blogs06 Text Collection) is used: 25 clusters and targets
            (i.e., target entity and questions) were used – 1,167,735
            words.
            5) Generic single-document summarization in Spanish
            using the Medicina Clı́nica5 corpus, which is composed
            of 50 medical articles in Spanish, each one with its
            corresponding author abstract – 124,929 words.
            6) Generic single document summarization in French using
            the “Canadien French Sociological Articles” corpus
            from the journal Perspectives interdisciplinaires sur le
            travail et la santé (PISTES)6 . It contains 50 sociological
            articles in French, each one with its corresponding
            author abstract – 381,039 words.
            7) Generic multi-document-summarization in French using
            data from the RPM27 corpus [18], 20 different themes
            consisting of 10 articles and 4 abstracts by reference
            thematic – 185,223 words.

            (4)

            where Ii0 is some subset of input Ii . The values produced
            by the measures for each summary SUMi,k are averaged
            for each system k = 0, . . . , s − 1 and these averages are
            used to produce a ranking. Rankings are then compared
            using Spearman Rank correlation [17] which is used to
            measure the degree of association between two variables
            whose values are used to rank objects. We have chosen
            to use this correlation to compare directly results to those
            presented in [12]. Computation of correlations is done using
            the Statistics-RankCorrelation-0.12 package1 , which computes
            the rank correlation between two vectors. We also verified
            the good conformity of the results with the correlation test
            of Kendall τ calculated with the statistical software R. The
            two nonparametric tests of Spearman and Kendall do not
            really stand out as the treatment of ex-æquo. The good
            correspondence between the two tests shows that they do not
            introduce bias in our analysis. Subsequently will mention only
            the ρ of Sperman more widely used in this field.
            A. Tools
            We carry out experimentation using a new summarization
            evaluation framework: F RESA –FRamework for Evaluating
            Summaries Automatically–, which includes document-based
            summary evaluation measures based on probabilities
            distribution2 . As in the ROUGE package, F RESA supports
            different n-grams and skip n-grams probability distributions.
            The F RESA environment can be used in the evaluation of
            summaries in English, French, Spanish and Catalan, and it
            integrates filtering and lemmatization in the treatment of
            summaries and documents. It is developed in Perl and will
            be made publicly available. We also use the ROUGE package
            [10] to compute various ROUGE statistics in new datasets.

            For experimentation in the TAC and the DUC datasets we use
            directly the peer summaries produced by systems participating
            in the evaluations. For experimentation in Spanish and French
            (single and multi-document summarization) we have created
            summaries at a similar ratio to those of reference using the
            following systems:
            – ENERTEX [19], a summarizer based on a theory of
            textual energy;
            – CORTEX [20], a single-document sentence extraction
            system for Spanish and French that combines various
            statistical measures of relevance (angle between sentence
            and topic, various Hamming weights for sentences, etc.)
            and applies an optimal decision algorithm for sentence
            selection;
            – SUMMTERM [21], a terminology-based summarizer that
            is used for summarization of medical articles and
            uses specialized terminology for scoring and ranking
            sentences;
            – REG [22], summarization system based on an greedy
            algorithm;

            B. Summarization Tasks and Data Sets
            We have conducted our experimentation with the following
            summarization tasks and data sets:
            1) Generic multi-document-summarization in English
            (production of a short summary of a cluster of related
            documents) using data from DUC’043 , task 2: 50
            clusters, 10 documents each – 294,636 words.
            2) Focused-based summarization in English (production of
            a short focused multi-document summary focused on the
            question “who is X?”, where X is a person’s name) using
            data from the DUC’04 task 5: 50 clusters, 10 documents
            each plus a target person name – 284,440 words.

            4 http://www.nist.gov/tac/data/index.html

            1 http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/

            5 http://www.elsevier.es/revistas/ctl servlet? f=7032&revistaid=2

            2 F RESA

            is available at: http://lia.univavignon.fr/fileadmin/axes/TALNE/
            Ressources.html
            3 http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html

            6 http://www.pistes.uqam.ca/
            7 http://www-labs.sinequa.com/rpm2

            15

            Polibits (42) 2010

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            – J S summarizer, a summarization system that scores
            and ranks sentences according to their Jensen-Shannon
            divergence to the source document;
            – a lead-based summarization system that selects the lead
            sentences of the document;
            – a random-based summarization system that selects
            sentences at random;
            – Open Text Summarizer [23], a multi-lingual summarizer
            based on the frequency and
            – commercial systems: Word, SSSummarizer8 , Pertinence9
            and Copernic10 .

            presented here we used uni-grams, 2-grams, and the skip
            2-grams with maximum skip distance of 4 (ROUGE-1,
            ROUGE-2 and ROUGE-SU4). ROUGE is used to compare
            a peer summary to a set of model summaries in our
            framework (as indicated in equation 3).
            – Jensen-Shannon divergence formula given in Equation 2
            is implemented in our F RESA package with the following
            specification (Equation 6) for the probability distribution
            of words w.
            CT
            Pw = w
            N
            (
            S
            Cw
            if
            w
            ∈
            S
            NS
            Qw =
            (6)
            T
            Cw
            +δ
            otherwise
            N +δ∗B

            C. Evaluation Measures
            The following measures derived from human assessment of
            the content of the summaries are used in our experiments:
            – C OVERAGE is understood as the degree to which one
            peer summary conveys the same information as a model
            summary [2]. C OVERAGE was used in DUC evaluations.
            This measure is used as indicated in equation 3 using
            human references or models.
            – R ESPONSIVENESS ranks summaries in a 5-point scale
            indicating how well the summary satisfied a given
            information need [2]. It is used in focused-based
            summarization tasks. This measure is used as indicated
            in equation 4 since a human judges the summary
            with respect to a given input “user need” (e.g., a
            question). R ESPONSIVENESS was used in DUC and TAC
            evaluations.
            – P YRAMIDS [11] is a content assessment measure which
            compares content units in a peer summary to weighted
            content units in a set of model summaries. This
            measure is used as indicated in equation 3 using human
        </corps>
        <conclusion>VI. C ONCLUSIONS AND F UTURE W ORK
            This paper has presented a series of experiments in
            content-based measures that do not rely on the use of model
            summaries for comparison purposes. We have carried out
            extensive experimentation with different summarization tasks
            drawing a clearer picture of tasks where the measures could
            be applied. This paper makes the following contributions:
            – We have shown that if we are only interested in ranking
            summarization systems according to the content of their
            automatic summaries, there are tasks were models could
            be subtituted by the full document in the computation of
            the J S measure obtaining reliable rankings. However,
            we have also found that the substitution of models
            by full-documents is not always advisable. We have

            V. D ISCUSSION
            The departing point for our inquiry into text summarization
            evaluation has been recent work on the use of content-based

            17

            Polibits (42) 2010

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            TABLE II
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC’04 TASK 2
            Mesure
            ROUGE-2
            JS

            C OVERAGE
            0.79
            0.68

            p-value
            p
            <
            0.0050
            p
            <
            0.0025

            TABLE III
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN DUC’04 TASK 5
            Mesure
            ROUGE-2
            JS

            C OVERAGE
            0.78
            0.40

            p-value
            p
            <
            0.001
            p
            <
            0.050

            R ESPONSIVENESS
            0.44
            -0.18

            p-value
            p
            <
            0.05
            p
            <
            0.25

            TABLE IV
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN TAC’08 OS TASK
            Mesure
            JS

            P YRAMIDS
            -0.13

            p-value
            p
            <
            0.25

            R ESPONSIVENESS
            -0.14

            p-value
            p
            <
            0.25

            TABLE V
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clı́nica C ORPUS (S PANISH )
            Mesure
            JS
            J S2
            J S4
            J SM

            ROUGE -1
            0.56
            0.88
            0.88
            0.82

            p-value
            p
            <
            0.100
            p
            <
            0.001
            p
            <
            0.001
            p
            <
            0.005

            ROUGE -2
            0.46
            0.80
            0.80
            0.71

            ROUGE -SU4
            0.45
            0.81
            0.81
            0.71

            p-value
            p
            <
            0.200
            p
            <
            0.005
            p
            <
            0.005
            p
            <
            0.010

            a representation of the task/topic in the calculation of
            measures. To carry out these comparisons, however, we are
            dependent on the existence of references.
            F RESA will also be used in the new question-answer task
            campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/
            qa.asp) for the evaluation of long answers. This task aims
            to answer a question by extraction and agglomeration of
            sentences in Wikipedia. This kind of task corresponds
            to those for which we have found a high correlation
            among the measures J S and evaluation methods with
            human intervention. Moreover, the J S calculation will be
            among the summaries produced and a representative set of
            relevant passages from Wikipedia. F RESA will be used to
            compare three types of systems, although different tasks: the
            multi-document summarizer guided by a query, the search
            systems targeted information (focused IR) and the question
            answering systems.

            found weak correlation among different rankings in
            complex summarization tasks such as the summarization
            of biographical information and the summarization of
            opinions.
            – We have also carried out large-scale experiments in
            Spanish and French which show positive medium to
            strong correlation among system’s ranks produced by
            ROUGE and divergence measures that do not use the
            model summaries.
            – We have also presented a new framework, F RESA, for
            the computation of measures based on J S divergence.
            Following the ROUGE approach, F RESA package use
            word uni-grams, 2-grams and skip n-grams computing
            divergences. This framework will be available to the
            community for research purposes.
            Although we have made a number of contributions, this paper
            leaves many open questions than need to be addressed. In
            order to verify correlation between ROUGE and J S, in the
            short term we intend to extend our investigation to other
            languages such as Portuguese and Chinesse for which we
            have access to data and summarization technology. We also
            plan to apply F RESA to the rest of the DUC and TAC
            summarization tasks, by using several smoothing techniques.
            As a novel idea, we contemplate the possibility of adapting
            the evaluation framework for the phrase compression task
            [29], which, to our knowledge, does not have an efficient
            evaluation measure. The main idea is to calculate J S from
            an automatically-compressed sentence taking the complete
            sentence by reference. In the long term, we plan to incorporate

            Polibits (42) 2010

            p-value
            p
            <
            0.100
            p
            <
            0.002
            p
            <
            0.002
            p
            <
            0.020
        </conclusion>
        <discussion>V. D ISCUSSION
            The departing point for our inquiry into text summarization
            evaluation has been recent work on the use of content-based

            17

            Polibits (42) 2010

            Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

            TABLE II
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH C OVERAGE IN DUC’04 TASK 2
            Mesure
            ROUGE-2
            JS

            C OVERAGE
            0.79
            0.68

            p-value
            p
            <
            0.0050
            p
            <
            0.0025

            TABLE III
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN DUC’04 TASK 5
            Mesure
            ROUGE-2
            JS

            C OVERAGE
            0.78
            0.40

            p-value
            p
            <
            0.001
            p
            <
            0.050

            R ESPONSIVENESS
            0.44
            -0.18

            p-value
            p
            <
            0.05
            p
            <
            0.25

            TABLE IV
            S PEARMAN ρ OF CONTENT- BASED MEASURES IN TAC’08 OS TASK
            Mesure
            JS

            P YRAMIDS
            -0.13

            p-value
            p
            <
            0.25

            R ESPONSIVENESS
            -0.14

            p-value
            p
            <
            0.25

            TABLE V
            S PEARMAN ρ OF CONTENT- BASED MEASURES WITH ROUGE IN THE Medicina Clı́nica C ORPUS (S PANISH )
            Mesure
            JS
            J S2
            J S4
            J SM

            ROUGE -1
            0.56
            0.88
            0.88
            0.82

            p-value
            p
            <
            0.100
            p
            <
            0.001
            p
            <
            0.001
            p
            <
            0.005

            ROUGE -2
            0.46
            0.80
            0.80
            0.71

            ROUGE -SU4
            0.45
            0.81
            0.81
            0.71

            p-value
            p
            <
            0.200
            p
            <
            0.005
            p
            <
            0.005
            p
            <
            0.010

            a representation of the task/topic in the calculation of
            measures. To carry out these comparisons, however, we are
        </discussion>
        <biblio>p-value
            p
            <
            0.05
            p
            <
            0.05
            p
            <
            0.10
            p
            <
            0.05

            ROUGE -SU4
            0.741
            0.680
            0.620
            0.740

            p-value
            p
            <
            0.01
            p
            <
            0.02
            p
            <
            0.05
            p
            <
            0.01

            [18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
            “A French Human Reference Corpus for multi-documents
            summarization and sentence compression,” in LREC’10, vol. 2,
            Malta, 2010, p. In press.
            [19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy
            of Associative Memories: performants applications of Enertex algorithm
            in text summarization and topic segmentation,” in MICAI’07, 2007, pp.
            861–871.
            [20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G. Meunier,
            “Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2,
            St Malo, France, 2002, pp. 723–734.
            [21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales,
            “Automatic summarization using terminological and semantic
            resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.
            [22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton
            appliqué au résumé automatique de texte,” in JADT’10. Rome, 2010,
            p. In press.
            [23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern
            systems of automatic text summarization,” Automatic Documentation
            and Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.
            [24] C. D. Manning and H. Schütze, Foundations of Statistical Natural
            Language Processing.
            Cambridge, Massachusetts: The MIT Press,
            1999.
            [25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM,
            vol. 43, no. 6, pp. 1449–1481, 2007.
            [26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specialized
            discourse: The case of medical articles in spanish,” Terminology, vol. 13,
            no. 2, pp. 249–286, 2007.
            [27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL
            Student Research Workshop.
            Toulouse, France: Association for
            Computational Linguistics, 9-11 July 2001 2001, pp. 49–54.
            [28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:
            Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,
            Singapore, August 2009, pp. 23–30.
            [29] K. Knight and D. Marcu, “Statistics-based summarization-step one:
            Sentence compression,” in Proceedings of the National Conference on
            Artificial Intelligence. Menlo Park, CA; Cambridge, MA; London;
            AAAI Press; MIT Press; 1999, 2000, pp. 703–710.

            [1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
            B. Sundheim, “Summac: a text summarization evaluation,” Natural
            Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.
            [2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,
            no. 6, pp. 1506–1520, 2007.
            [3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,
            USA: NIST, November 17-19 2008.
            [4] K. Spärck Jones and J. Galliers, Evaluating Natural Language
            Processing Systems, An Analysis and Review, ser. Lecture Notes in
            Computer Science. Springer, 1996, vol. 1083.
            [5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of
            rankings produced by summarization evaluation measures,” in NAACL
            Workshop on Automatic Summarization, 2000, pp. 69–78.
            [6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation
            of Summaries in a Cross-lingual Environment using Content-based
            Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.
            [7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,
            D. Liu, and E. Drábek, “Evaluation challenges in large-scale document
            summarization,” in ACL’03, 2003, pp. 375–382.
            [8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method
            for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
            311–318.
            [9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
            Initiatives in Natural Language Processing. Budapest, Hungary: EACL,
            14 April 2003.
            [10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
            Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,
            M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.
            [11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
            Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.
            145–152.
            [12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
            in Summarization without Human Models,” in Empirical Methods in
            Natural Language Processing, Singapore, August 2009, pp. 306–314.
            [Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
            [13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
            Transactions on Information Theory, vol. 37, no. 145-151, 1991.
            [14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
            N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
            USA: Association for Computational Linguistics, 2003, pp. 71–78.
            [15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
            approach to automatic evaluation of summaries,” in HLT-NAACL,
            Morristown, USA, 2006, pp. 463–470.
            [16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. of
            Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.
            [17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral
            Sciences. McGraw-Hill, 1998.

            19

            Polibits (42) 2010
        </biblio>
    </article>
</articles>
